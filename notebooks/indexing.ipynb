{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ac121a313748aebdb43392f2c514be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/6.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b03dc638d6b4c7595412e98df191223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/253M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmarker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PdfConverter\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmarker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_model_dict\n\u001b[1;32m      4\u001b[0m converter \u001b[38;5;241m=\u001b[39m PdfConverter(\n\u001b[0;32m----> 5\u001b[0m     artifact_dict\u001b[38;5;241m=\u001b[39m\u001b[43mcreate_model_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m rendered \u001b[38;5;241m=\u001b[39m converter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../docs/the-pragmatic-programmer.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/rag/.venv/lib/python3.12/site-packages/marker/models.py:80\u001b[0m, in \u001b[0;36mcreate_model_dict\u001b[0;34m(device, dtype)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_model_dict\u001b[39m(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43msetup_layout_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtexify_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: setup_texify_model(device, dtype),\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecognition_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: setup_recognition_model(device, dtype),\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable_rec_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: setup_table_rec_model(device, dtype),\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetection_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: setup_detection_model(device, dtype),\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mocr_error_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: setup_ocr_error_model(device,dtype)\n\u001b[1;32m     86\u001b[0m     }\n",
      "File \u001b[0;32m~/projects/rag/.venv/lib/python3.12/site-packages/marker/models.py:66\u001b[0m, in \u001b[0;36msetup_layout_model\u001b[0;34m(device, dtype)\u001b[0m\n\u001b[1;32m     64\u001b[0m     model \u001b[38;5;241m=\u001b[39m load_layout_model(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_layout_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m model\u001b[38;5;241m.\u001b[39mprocessor \u001b[38;5;241m=\u001b[39m load_layout_processor()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/projects/rag/.venv/lib/python3.12/site-packages/surya/model/layout/model.py:18\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(checkpoint, device, dtype)\u001b[0m\n\u001b[1;32m     15\u001b[0m encoder \u001b[38;5;241m=\u001b[39m DonutSwinLayoutConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoder_config)\n\u001b[1;32m     16\u001b[0m config\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m encoder\n\u001b[0;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSuryaLayoutModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/projects/rag/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3776\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3760\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3761\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   3762\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3763\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   3764\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3774\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   3775\u001b[0m     }\n\u001b[0;32m-> 3776\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3778\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   3779\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   3780\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   3781\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/rag/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/projects/rag/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/rag/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    842\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/rag/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1009\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1007\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1009\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1020\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/projects/rag/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1543\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1541\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1543\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1552\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1553\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/projects/rag/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:452\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    450\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[1;32m    454\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/rag/.venv/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/projects/rag/.venv/lib/python3.12/site-packages/urllib3/response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1066\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1069\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/projects/rag/.venv/lib/python3.12/site-packages/urllib3/response.py:955\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 955\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/projects/rag/.venv/lib/python3.12/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    876\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 879\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/projects/rag/.venv/lib/python3.12/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "\n",
    "converter = PdfConverter(\n",
    "    artifact_dict=create_model_dict(),\n",
    ")\n",
    "\n",
    "rendered = converter(\"../docs/the-pragmatic-programmer.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'![](_page_0_Picture_0.jpeg)\\n\\n![](_page_0_Picture_4.jpeg)\\n\\n# **The Pragmatic Programmer**\\n\\n### **your journey to mastery**\\n\\n#### **by Dave Thomas, Andy Hunt**\\n\\nVersion: P3.0 (January 22, 2020)\\n\\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed with initial capital letters or in all capitals. \"The Pragmatic Programmer\" and the linking g device are trademarks of The Pragmatic Programmers, LLC.\\n\\nThe authors and publisher have taken care in the preparation of this book, but make no expressed or implied warranty of any kind and assume no responsibility for errors or omissions. No liability is assumed for incidental or consequential damages in connection with or arising out of the use of the information or programs contained herein.\\n\\nFor information about buying this title in bulk quantities, or for special sales opportunities (which may include electronic versions; custom cover designs; and content particular to your business, training goals, marketing focus, or branding interests), please contact our corporate sales department at corpsales@pearsoned.com or (800) 382-3419.\\n\\nFor government sales inquiries, please contact governmentsales@pearsoned.com. For questions about sales outside the U.S., please contact intlcs@pearson.com. Visit us on the Web: informit.com/aw\\n\\nLibrary of Congress Control Number: 2019944178\\n\\nCopyright © 2020 Pearson Education, Inc. Cover images: Mihalec/Shutterstock, Stockish/Shutterstock\\n\\nAll rights reserved. This publication is protected by copyright, and permission must be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. For information regarding permissions, request forms and the appro- priate contacts within the Pearson Education Global Rights & Permissions Department, please visit www.pearsoned.com/permissions.\\n\\nISBN-13: 978-0-13-595705-9 ISBN-10: 0-13-595705-2\\n\\n*For Juliet and Ellie, Zachary and Elizabeth, Henry and Stuart*\\n\\n# **Table of Contents**\\n\\n#### **Foreword**\\n\\n#### **Preface to the Second Edition**\\n\\nHow the Book Is Organized What\\'s in a Name? Source Code and Other Resources Send Us Feedback Second Edition Acknowledgments\\n\\n### **From the Preface to the First Edition**\\n\\nWho Should Read This Book? What Makes a Pragmatic Programmer? Individual Pragmatists, Large Teams It\\'s a Continuous Process\\n\\n### **1. A Pragmatic Philosophy**\\n\\n- **Topic 1. It\\'s Your Life**\\n- **Topic 2. The Cat Ate My Source Code**\\n- **Topic 3. Software Entropy**\\n- **Topic 4. Stone Soup and Boiled Frogs**\\n- **Topic 5. Good-Enough Software**\\n- **Topic 6. Your Knowledge Portfolio**\\n- **Topic 7. Communicate!**\\n\\n### **2. A Pragmatic Approach**\\n\\n- **Topic 8. The Essence of Good Design**\\n- **Topic 9. DRY—The Evils of Duplication**\\n- **Topic 10. Orthogonality**\\n- **Topic 11. Reversibility**\\n- **Topic 12. Tracer Bullets**\\n- **Topic 13. Prototypes and Post-it Notes**\\n- **Topic 14. Domain Languages**\\n- **Topic 15. Estimating**\\n\\n#### **3. The Basic Tools**\\n\\n- **Topic 16. The Power of Plain Text**\\n- **Topic 17. Shell Games**\\n- **Topic 18. Power Editing**\\n- **Topic 19. Version Control**\\n- **Topic 20. Debugging**\\n- **Topic 21. Text Manipulation**\\n- **Topic 22. Engineering Daybooks**\\n\\n#### **4. Pragmatic Paranoia**\\n\\n- **Topic 23. Design by Contract**\\n- **Topic 24. Dead Programs Tell No Lies**\\n- **Topic 25. Assertive Programming**\\n- **Topic 26. How to Balance Resources**\\n- **Topic 27. Don\\'t Outrun Your Headlights**\\n\\n#### **5. Bend, or Break**\\n\\n- **Topic 28. Decoupling**\\n- **Topic 29. Juggling the Real World**\\n- **Topic 30. Transforming Programming**\\n- **Topic 31. Inheritance Tax**\\n\\n#### **Topic 32. Configuration**\\n\\n#### **6. Concurrency**\\n\\n- **Topic 33. Breaking Temporal Coupling**\\n- **Topic 34. Shared State Is Incorrect State**\\n- **Topic 35. Actors and Processes**\\n- **Topic 36. Blackboards**\\n\\n### **7. While You Are Coding**\\n\\n- **Topic 37. Listen to Your Lizard Brain**\\n- **Topic 38. Programming by Coincidence**\\n- **Topic 39. Algorithm Speed**\\n- **Topic 40. Refactoring**\\n- **Topic 41. Test to Code**\\n- **Topic 42. Property-Based Testing**\\n- **Topic 43. Stay Safe Out There**\\n- **Topic 44. Naming Things**\\n\\n#### **8. Before the Project**\\n\\n- **Topic 45. The Requirements Pit**\\n- **Topic 46. Solving Impossible Puzzles**\\n- **Topic 47. Working Together**\\n- **Topic 48. The Essence of Agility**\\n\\n#### **9. Pragmatic Projects**\\n\\n- **Topic 49. Pragmatic Teams**\\n- **Topic 50. Coconuts Don\\'t Cut It**\\n- **Topic 51. Pragmatic Starter Kit**\\n- **Topic 52. Delight Your Users**\\n- **Topic 53. Pride and Prejudice**\\n- **10. Postface**\\n- **A1. Bibliography**\\n- **A2. Possible Answers to the Exercises**\\n\\nCopyright © 2020 Pearson Education, Inc.\\n\\n# **Praise for the second edition of** *The Pragmatic Programmer*\\n\\nSome say that with *The Pragmatic Programmer*, Andy and Dave captured lightning in a bottle; that it\\'s unlikely anyone will soon write a book that can move an entire industry as it did. Sometimes, though, lightning does strike twice, and this book is proof. The updated content ensures that it will stay at the top of \"best books in software development\" lists for another 20 years, right where it belongs.\\n\\n— VM (Vicky) Brasseur *Director of Open Source Strategy, Juniper Networks*\\n\\nIf you want your software to be easy to modernize and maintain, keep a copy of *The Pragmatic Programmer* close. It\\'s filled with practical advice, both technical and professional, that will serve you and your projects well for years to come.\\n\\n— Andrea Goulet *CEO, Corgibytes; Founder, LegacyCode.Rocks*\\n\\n*The Pragmatic Programmer* is the one book I can point to that completely dislodged the existing trajectory of my career in software and pointed me in the direction of success. Reading it opened my mind to the possibilities of being a craftsman, not just a cog in a big machine. One of the most significant books in my life.\\n\\n— Obie Fernandez *Author, The Rails Way*\\n\\nFirst-time readers can look forward to an enthralling induction into the modern world of software practice, a world that the first edition played a major role in shaping. Readers of the first edition will rediscover here the insights and practical wisdom that made the book so significant in the first place, expertly curated and updated, along with much that\\'s new.\\n\\n— David A. Black *Author, The Well-Grounded Rubyist*\\n\\nI have an old paper copy of the original *Pragmatic Programmer* on my bookshelf. It has been read and re-read and a long time ago it changed everything about how I approached my job as a programmer. In the new edition everything and nothing has changed: I now read it on my iPad and the code examples use modern programming languages—but the underlying concepts, ideas, and attitudes are timeless and universally applicable. Twenty years later, the book is as relevant as ever. It makes me happy to know that current and future developers will have the same opportunity to learn from Andy and Dave\\'s profound insights as I did back in the day.\\n\\n— Sandy Mamoli\\n\\n*Agile coach, author of How Self-Selection Lets People Excel*\\n\\nTwenty years ago, the first edition of *The Pragmatic Programmer* completely changed the trajectory of my career. This new edition could do the same for yours.\\n\\n— Mike Cohn\\n\\n*Author of Succeeding with Agile, Agile Estimating and Planning, and User Stories Applied*\\n\\n# **Foreword**\\n\\nI remember when Dave and Andy first tweeted about the new edition of this book. It was big news. I watched as the coding community responded with excitement. My feed buzzed with anticipation. After twenty years, *The Pragmatic Programmer* is just as relevant today as it was back then.\\n\\nIt says a lot that a book with such history had such a reaction. I had the privilege of reading an unreleased copy to write this foreword, and I understood why it created such a stir. While it\\'s a technical book, calling it that does it a disservice. Technical books often intimidate. They\\'re stuffed with big words, obscure terms, convoluted examples that, unintentionally, make you feel stupid. The more experienced the author, the easier it is to forget what it\\'s like to learn new concepts, to be a beginner.\\n\\nDespite their decades of programming experience, Dave and Andy have conquered the difficult challenge of writing with the same excitement of people who\\'ve just learned these lessons. They don\\'t talk down to you. They don\\'t assume you are an expert. They don\\'t even assume you\\'ve read the first edition. They take you as you are—programmers who just want to be better. They spend the pages of this book helping you get there, one actionable step at a time.\\n\\nTo be fair, they\\'d already done this before. The original release was full of tangible examples, new ideas, and practical tips to build your coding muscles and develop your coding brain that still apply today. But this updated edition makes two improvements on the book.\\n\\nThe first is the obvious one: it removes some of the older references, the out-of-date examples, and replaces them with fresh, modern content. You won\\'t find examples of loop invariants or build machines. Dave and Andy have taken their powerful content and made sure the lessons still come through, free of the distractions of old examples. It dusts off old ideas like DRY (don\\'t repeat yourself) and gives them a fresh coat of paint, really making them shine.\\n\\nBut the second is what makes this release truly exciting. After writing the first edition, they had the chance to reflect on what they were trying to say, what they wanted their readers to take away, and how it was being received. They got feedback on those lessons. They saw what stuck, what needed refining, what was misunderstood. In the twenty years that this book has made its way through the hands and hearts of programmers all over the world, Dave and Andy have studied this response and formulated new ideas, new concepts.\\n\\nThey\\'ve learned the importance of agency and recognized that developers have arguably more agency than most other professionals. They start this book with the simple but profound message: \"it\\'s your life.\" It reminds us of our own power in our code base, in our jobs, in our careers. It sets the tone for everything else in the book—that it\\'s more than just another technical book filled with code examples.\\n\\nWhat makes it truly stand out among the shelves of technical books is that it understands what it means to be a programmer. Programming is about trying to make the future less painful. It\\'s about making things easier for our teammates. It\\'s about getting things wrong and being able to bounce back. It\\'s about forming good habits. It\\'s about understanding your toolset. Coding is just part of the world of being a programmer, and this book explores that world.\\n\\nI spend a lot of time thinking about the coding journey. I didn\\'t grow up coding; I didn\\'t study it in college. I didn\\'t spend my teenage years\\n\\ntinkering with tech. I entered the coding world in my mid-twenties and had to learn what it meant to be a programmer. This community is very different from others I\\'d been a part of. There is a unique dedication to learning and practicality that is both refreshing and intimidating.\\n\\nFor me, it really does feel like entering a new world. A new town, at least. I had to get to know the neighbors, pick my grocery store, find the best coffee shops. It took a while to get the lay of the land, to find the most efficient routes, to avoid the streets with the heaviest traffic, to know when traffic was likely to hit. The weather is different, I needed a new wardrobe.\\n\\nThe first few weeks, even months, in a new town can be scary. Wouldn\\'t it be wonderful to have a friendly, knowledgeable neighbor who\\'d been living there a while? Who can give you a tour, show you those coffee shops? Someone who\\'d been there long enough to know the culture, understand the pulse of the town, so you not only feel at home, but become a contributing member as well? Dave and Andy are those neighbors.\\n\\nAs a relative newcomer, it\\'s easy to be overwhelmed not by the act of programming but the process of becoming a programmer. There is an entire mindset shift that needs to happen—a change in habits, behaviors, and expectations. The process of becoming a better programmer doesn\\'t just happen because you know how to code; it must be met with intention and deliberate practice. This book is a guide to becoming a better programmer efficiently.\\n\\nBut make no mistake—it doesn\\'t tell you how programming should be. It\\'s not philosophical or judgmental in that way. It tells you, plain and simple, what a Pragmatic Programmer is—how they operate, and how they approach code. They leave it up to you to decide if you want to be one. If you feel it\\'s not for you, they won\\'t hold it against you. But if you decide it is, they\\'re your friendly neighbors, there to show you the way.\\n\\n![](_page_13_Picture_0.jpeg)\\n\\n▶ *Saron Yitbarek* Founder & CEO of CodeNewbie Host of Command Line Heroes\\n\\nCopyright © 2020 Pearson Education, Inc.\\n\\n# **Preface to the Second Edition**\\n\\nBack in the 1990s, we worked with companies whose projects were having problems. We found ourselves saying the same things to each: maybe you should test that before you ship it; why does the code only build on Mary\\'s machine? Why didn\\'t anyone ask the users?\\n\\nTo save time with new clients, we started jotting down notes. And those notes became *The Pragmatic Programmer*. To our surprise the book seemed to strike a chord, and it has continued to be popular these last 20 years.\\n\\nBut 20 years is many lifetimes in terms of software. Take a developer from 1999 and drop them into a team today, and they\\'d struggle in this strange new world. But the world of the 1990s is equally foreign to today\\'s developer. The book\\'s references to things such as CORBA, CASE tools, and indexed loops were at best quaint and more likely confusing.\\n\\nAt the same time, 20 years has had no impact whatsoever on common sense. Technology may have changed, but people haven\\'t. Practices and approaches that were a good idea then remain a good idea now. Those aspects of the book aged well.\\n\\nSo when it came time to create this *20th Anniversary Edition,* we had to make a decision. We could go through and update the technologies we reference and call it a day. Or we could reexamine the assumptions behind the practices we recommended in the light of an additional two decades\\' worth of experience.\\n\\nIn the end, we did both.\\n\\nAs a result, this book is something of a *Ship of Theseus*. [1] Roughly onethird of the topics in the book are brand new. Of the rest, the majority have been rewritten, either partially or totally. Our intent was to make things clearer, more relevant, and hopefully somewhat timeless.\\n\\nWe made some difficult decisions. We dropped the *Resources* appendix, both because it would be impossible to keep up-to-date and because it\\'s easier to search for what you want. We reorganized and rewrote topics to do with concurrency, given the current abundance of parallel hardware and the dearth of good ways of dealing with it. We added content to reflect changing attitudes and environments, from the agile movement which we helped launch, to the rising acceptance of functional programming idioms and the growing need to consider privacy and security.\\n\\nInterestingly, though, there was considerably less debate between us on the content of this edition than there was when we wrote the first. We both felt that the stuff that was important was easier to identify.\\n\\nAnyway, this book is the result. Please enjoy it. Maybe adopt some new practices. Maybe decide that some of the stuff we suggest is wrong. Get involved in your craft. Give us feedback.\\n\\nBut, most important, remember to make it fun.\\n\\n## **How the Book Is Organized**\\n\\nThis book is written as a collection of short topics. Each topic is selfcontained, and addresses a particular theme. You\\'ll find numerous cross references, which help put each topic in context. Feel free to read the topics in any order—this isn\\'t a book you need to read front-to-back.\\n\\nOccasionally you\\'ll come across a box labeled *Tip nn* (such as Tip 1, *Care About Your Craft*). As well as emphasizing points in the text, we feel the tips have a life of their own—we live by them daily. You\\'ll find a summary of all the tips on a pull-out card inside the back cover.\\n\\nWe\\'ve included exercises and challenges where appropriate. Exercises normally have relatively straightforward answers, while the challenges are more open-ended. To give you an idea of our thinking, we\\'ve included our answers to the exercises in an appendix, but very few have a single *correct* solution. The challenges might form the basis of group discussions or essay work in advanced programming courses.\\n\\nThere\\'s also a short bibliography listing the books and articles we explicitly reference.\\n\\n## **What\\'s in a Name?**\\n\\nScattered throughout the book you\\'ll find various bits of jargon—either perfectly good English words that have been corrupted to mean something technical, or horrendous made-up words that have been assigned meanings by computer scientists with a grudge against the language. The first time we use each of these jargon words, we try to define it, or at least give a hint to its meaning. However, we\\'re sure that some have fallen through the cracks, and others, such as *object* and *relational database,* are in common enough usage that adding a definition would be boring. If you *do* come across a term you haven\\'t seen before, please don\\'t just skip over it. Take time to look it up, perhaps on the web, or maybe in a computer science textbook. And, if you get a chance, drop us an email and complain, so we can add a definition to the next edition.\\n\\nHaving said all this, we decided to get revenge against the computer scientists. Sometimes, there are perfectly good jargon words for concepts, words that we\\'ve decided to ignore. Why? Because the existing jargon is normally restricted to a particular problem domain, or to a particular phase of development. However, one of the basic philosophies of this book is that most of the techniques we\\'re recommending are universal: modularity applies to code, designs, documentation, and team organization, for instance. When we wanted to use the conventional jargon word in a broader context, it got confusing—we couldn\\'t seem to overcome the baggage the original term brought with it. When this happened, we contributed to the decline of the language by inventing our own terms.\\n\\n## **Source Code and Other Resources**\\n\\nMost of the code shown in this book is extracted from compilable source files, available for download from our website.[2]\\n\\nThere you\\'ll also find links to resources we find useful, along with updates to the book and news of other Pragmatic Programmer developments.\\n\\n## **Send Us Feedback**\\n\\nWe\\'d appreciate hearing from you. Email us at ppbook@pragprog.com.\\n\\n## **Second Edition Acknowledgments**\\n\\nWe have enjoyed literally thousands of interesting conversations about programming over the last 20 years, meeting people at conferences, at courses, and sometimes even on the plane. Each one of these has added to our understanding of the development process, and has contributed to the updates in this edition. Thank you all (and keep telling us when we\\'re wrong).\\n\\nThanks to the participants in the book\\'s beta process. Your questions and comments helped us explain things better.\\n\\nBefore we went beta, we shared the book with a few folks for comments. Thanks to VM (Vicky) Brasseur, Jeff Langr, and Kim Shrier for your detailed comments, and to José Valim and Nick Cuthbert for your technical reviews.\\n\\nThanks to Ron Jeffries for letting us use the Sudoku example.\\n\\nMuch gratitude to the folks at Pearson who agreed to let us create this book our way.\\n\\nA special thanks to the indispensable Janet Furlow, who masters whatever she takes on and keeps us in line.\\n\\nAnd, finally, a shout out to all the Pragmatic Programmers out there who have been making programming better for everyone for the last twenty years. Here\\'s to twenty more.\\n\\n#### **Footnotes**\\n\\n[2] https://pragprog.com/titles/tpp20\\n\\n<sup>[1]</sup> If, over the years, every component of a ship is replaced as it fails, is the resulting vessel the same ship?\\n\\nCopyright © 2020 Pearson Education, Inc.\\n\\n# **From the Preface to the First Edition**\\n\\nThis book will help you become a better programmer.\\n\\nYou could be a lone developer, a member of a large project team, or a consultant working with many clients at once. It doesn\\'t matter; this book will help you, as an individual, to do better work. This book isn\\'t theoretical —we concentrate on practical topics, on using your experience to make more informed decisions. The word *pragmatic* comes from the Latin *pragmaticus*—\"skilled in business\"—which in turn is derived from the Greek πραγματικός, meaning \"fit for use.\"\\n\\nThis is a book about doing.\\n\\nProgramming is a craft. At its simplest, it comes down to getting a computer to do what you want it to do (or what your user wants it to do). As a programmer, you are part listener, part advisor, part interpreter, and part dictator. You try to capture elusive requirements and find a way of expressing them so that a mere machine can do them justice. You try to document your work so that others can understand it, and you try to engineer your work so that others can build on it. What\\'s more, you try to do all this against the relentless ticking of the project clock. You work small miracles every day.\\n\\nIt\\'s a difficult job.\\n\\nThere are many people offering you help. Tool vendors tout the miracles their products perform. Methodology gurus promise that their techniques guarantee results. Everyone claims that their programming language is the best, and every operating system is the answer to all conceivable ills.\\n\\nOf course, none of this is true. There are no easy answers. There is no *best* solution, be it a tool, a language, or an operating system. There can only be systems that are more appropriate in a particular set of circumstances.\\n\\nThis is where pragmatism comes in. You shouldn\\'t be wedded to any particular technology, but have a broad enough background and experience base to allow you to choose good solutions in particular situations. Your background stems from an understanding of the basic principles of computer science, and your experience comes from a wide range of practical projects. Theory and practice combine to make you strong.\\n\\nYou adjust your approach to suit the current circumstances and environment. You judge the relative importance of all the factors affecting a project and use your experience to produce appropriate solutions. And you do this continuously as the work progresses. Pragmatic Programmers get the job done, and do it well.\\n\\n## **Who Should Read This Book?**\\n\\nThis book is aimed at people who want to become more effective and more productive programmers. Perhaps you feel frustrated that you don\\'t seem to be achieving your potential. Perhaps you look at colleagues who seem to be using tools to make themselves more productive than you. Maybe your current job uses older technologies, and you want to know how newer ideas can be applied to what you do.\\n\\nWe don\\'t pretend to have all (or even most) of the answers, nor are all of our ideas applicable in all situations. All we can say is that if you follow our approach, you\\'ll gain experience rapidly, your productivity will increase, and you\\'ll have a better understanding of the entire development process. And you\\'ll write better software.\\n\\n## **What Makes a Pragmatic Programmer?**\\n\\nEach developer is unique, with individual strengths and weaknesses, preferences and dislikes. Over time, each will craft their own personal environment. That environment will reflect the programmer\\'s individuality just as forcefully as his or her hobbies, clothing, or haircut. However, if you\\'re a Pragmatic Programmer, you\\'ll share many of the following characteristics:\\n\\n#### *Early adopter/fast adapter*\\n\\nYou have an instinct for technologies and techniques, and you love trying things out. When given something new, you can grasp it quickly and integrate it with the rest of your knowledge. Your confidence is born of experience.\\n\\n#### *Inquisitive*\\n\\nYou tend to ask questions. *That\\'s neat—how did you do that? Did you have problems with that library? What\\'s this quantum computing I\\'ve heard about? How are symbolic links implemented?* You are a pack rat for little facts, each of which may affect some decision years from now.\\n\\n#### *Critical thinker*\\n\\nYou rarely take things as given without first getting the facts. When colleagues say \"because that\\'s the way it\\'s done,\" or a vendor promises the solution to all your problems, you smell a challenge.\\n\\n#### *Realistic*\\n\\nYou try to understand the underlying nature of each problem you face. This realism gives you a good feel for how difficult things are, and how long things will take. Deeply understanding that a process *should* be difficult or *will* take a while to complete gives you the stamina to keep at it.\\n\\n### *Jack of all trades*\\n\\nYou try hard to be familiar with a broad range of technologies and environments, and you work to keep abreast of new developments. Although your current job may require you to be a specialist, you will always be able to move on to new areas and new challenges.\\n\\nWe\\'ve left the most basic characteristics until last. All Pragmatic Programmers share them. They\\'re basic enough to state as tips:\\n\\n## **Tip 1** Care About Your Craft\\n\\nWe feel that there is no point in developing software unless you care about doing it well.\\n\\n## **Tip 2** Think! About Your Work\\n\\nIn order to be a Pragmatic Programmer, we\\'re challenging you to think about what you\\'re doing while you\\'re doing it. This isn\\'t a one-time audit of current practices—it\\'s an ongoing critical appraisal of every decision you make, every day, and on every project. Never run on auto-pilot. Constantly be thinking, critiquing your work in real time. The old IBM corporate motto, *THINK!*, is the Pragmatic Programmer\\'s mantra.\\n\\nIf this sounds like hard work to you, then you\\'re exhibiting the *realistic* characteristic. This is going to take up some of your valuable time—time that is probably already under tremendous pressure. The reward is a more active involvement with a job you love, a feeling of mastery over an increasing range of subjects, and pleasure in a feeling of continuous improvement. Over the long term, your time investment will be repaid as you and your team become more efficient, write code that\\'s easier to maintain, and spend less time in meetings.\\n\\n## **Individual Pragmatists, Large Teams**\\n\\nSome people feel that there is no room for individuality on large teams or complex projects. \"Software is an engineering discipline,\" they say, \"that breaks down if individual team members make decisions for themselves.\"\\n\\nWe strongly disagree.\\n\\nThere *should* be engineering in software construction. However, this doesn\\'t preclude individual craftsmanship. Think about the large cathedrals built in Europe during the Middle Ages. Each took thousands of personyears of effort, spread over many decades. Lessons learned were passed down to the next set of builders, who advanced the state of structural engineering with their accomplishments. But the carpenters, stonecutters, carvers, and glass workers were all craftspeople, interpreting the engineering requirements to produce a whole that transcended the purely mechanical side of the construction. It was their belief in their individual contributions that sustained the projects: *We who cut mere stones must always be envisioning cathedrals.*\\n\\nWithin the overall structure of a project there is always room for individuality and craftsmanship. This is particularly true given the current state of software engineering. One hundred years from now, our engineering may seem as archaic as the techniques used by medieval cathedral builders seem to today\\'s civil engineers, while our craftsmanship will still be honored.\\n\\n## **It\\'s a Continuous Process**\\n\\n*A tourist visiting England\\'s Eton College asked the gardener how he got the lawns so perfect. \"That\\'s easy,\" he replied, \"You just brush off the dew every morning, mow them every other day, and roll them once a week.\"*\\n\\n*\"Is that all?\" asked the tourist. \"Absolutely,\" replied the gardener. \"Do that for 500 years and you\\'ll have a nice lawn, too.\"*\\n\\nGreat lawns need small amounts of daily care, and so do great programmers. Management consultants like to drop the word *kaizen* in conversations. \"Kaizen\" is a Japanese term that captures the concept of continuously making many small improvements. It was considered to be one of the main reasons for the dramatic gains in productivity and quality in Japanese manufacturing and was widely copied throughout the world. Kaizen applies to individuals, too. Every day, work to refine the skills you have and to add new tools to your repertoire. Unlike the Eton lawns, you\\'ll start seeing results in a matter of days. Over the years, you\\'ll be amazed at how your experience has blossomed and how your skills have grown.\\n\\nCopyright © 2020 Pearson Education, Inc.\\n\\n# Chapter 1\\n\\n# **A Pragmatic Philosophy**\\n\\nThis book is about you.\\n\\nMake no mistake, it is *your* career, and more importantly, Topic 1, *It\\'s Your Life*. You own it. You\\'re here because you know you can become a better developer and help others become better as well. You can become a *Pragmatic Programmer*.\\n\\nWhat distinguishes Pragmatic Programmers? We feel it\\'s an attitude, a style, a philosophy of approaching problems and their solutions. They think beyond the immediate problem, placing it in its larger context and seeking out the bigger picture. After all, without this larger context, how can you be pragmatic? How can you make intelligent compromises and informed decisions?\\n\\nAnother key to their success is that Pragmatic Programmers take responsibility for everything they do, which we discuss in Topic 2, *The Cat Ate My Source Code*. Being responsible, Pragmatic Programmers won\\'t sit idly by and watch their projects fall apart through neglect. In Topic 3, *Software Entropy*, we tell you how to keep your projects pristine.\\n\\nMost people find change difficult, sometimes for good reasons, sometimes because of plain old inertia. In Topic 4, *Stone Soup and Boiled Frogs*, we look at a strategy for instigating change and (in the interests of balance)\\n\\npresent the cautionary tale of an amphibian that ignored the dangers of gradual change.\\n\\nOne of the benefits of understanding the context in which you work is that it becomes easier to know just how good your software has to be. Sometimes near-perfection is the only option, but often there are trade-offs involved. We explore this in Topic 5, *Good-Enough Software*.\\n\\nOf course, you need to have a broad base of knowledge and experience to pull all of this off. Learning is a continuous and ongoing process. In Topic 6, *Your Knowledge Portfolio*, we discuss some strategies for keeping the momentum up.\\n\\nFinally, none of us works in a vacuum. We all spend a large amount of time interacting with others. Topic 7, *Communicate!* lists ways we can do this better.\\n\\nPragmatic programming stems from a philosophy of pragmatic thinking. This chapter sets the basis for that philosophy.\\n\\n## **Topic 1** It\\'s Your Life\\n\\n*I\\'m not in this world to live up to your expectations and you\\'re not in this world to live up to mine.*\\n\\n*Bruce Lee*\\n\\nIt is *your* life. You own it. You run it. You create it.\\n\\nMany developers we talk to are frustrated. Their concerns are varied. Some feel they\\'re stagnating in their job, others that technology has passed\\n\\nthem by. Folks feel they are under appreciated, or underpaid, or that their teams are toxic. Maybe they want to move to Asia, or Europe, or work from home.\\n\\nAnd the answer we give is always the same.\\n\\n```\\n\"Why can\\'t you change it?\"\\n```\\nSoftware development must appear close to the top of any list of careers where you have control. Our skills are in demand, our knowledge crosses geographic boundaries, we can work remotely. We\\'re paid well. We really can do just about anything we want.\\n\\nBut, for some reason, developers seem to resist change. They hunker down, and hope things will get better. They look on, passively, as their skills become dated and complain that their companies don\\'t train them. They look at ads for exotic locations on the bus, then step off into the chilling rain and trudge into work.\\n\\nSo here\\'s the most important tip in the book.\\n\\n**Tip 3** You Have Agency\\n\\nDoes your work environment suck? Is your job boring? Try to fix it. But don\\'t try forever. As Martin Fowler says, \"you can change your organization or change your organization.\"[3]\\n\\nIf technology seems to be passing you by, make time (in your own time) to study new stuff that looks interesting. You\\'re investing in yourself, so doing it while you\\'re off-the-clock is only reasonable.\\n\\nWant to work remotely? Have you asked? If they say no, then find someone who says yes.\\n\\nThis industry gives you a remarkable set of opportunities. Be proactive, and take them.\\n\\n## **Related Sections Include**\\n\\n- Topic 4, *Stone Soup and Boiled Frogs*\\n- Topic 6, *Your Knowledge Portfolio*\\n\\n## **Topic 2** The Cat Ate My Source Code\\n\\n*The greatest of all weaknesses is the fear of appearing weak.*\\n\\n> *J.B. Bossuet, Politics from Holy Writ, 1709*\\n\\nOne of the cornerstones of the pragmatic philosophy is the idea of taking responsibility for yourself and your actions in terms of your career advancement, your learning and education, your project, and your day-\\n\\nto-day work. Pragmatic Programmers take charge of their own career, and aren\\'t afraid to admit ignorance or error. It\\'s not the most pleasant aspect of programming, to be sure, but it will happen—even on the best of projects. Despite thorough testing, good documentation, and solid automation, things go wrong. Deliveries are late. Unforeseen technical problems come up.\\n\\nThese things happen, and we try to deal with them as professionally as we can. This means being honest and direct. We can be proud of our abilities, but we must own up to our shortcomings—our ignorance and our mistakes.\\n\\n## **Team Trust**\\n\\nAbove all, your team needs to be able to trust and rely on you—and you need to be comfortable relying on each of them as well. Trust in a team is absolutely essential for creativity and collaboration according to the research literature.[4] In a healthy environment based in trust, you can safely speak your mind, present your ideas, and rely on your team members who can in turn rely on you. Without trust, well…\\n\\nImagine a high-tech, stealth ninja team infiltrating the villain\\'s evil lair. After months of planning and delicate execution, you\\'ve made it on site. Now it\\'s your turn to set up the laser guidance grid: \"Sorry, folks, I don\\'t have the laser. The cat was playing with the red dot and I left it at home.\" That sort of breach of trust might be hard to repair.\\n\\n## **Take Responsibility**\\n\\nResponsibility is something you actively agree to. You make a commitment to ensure that something is done right, but you don\\'t necessarily have direct control over every aspect of it. In addition to doing your own personal best, you must analyze the situation for risks that are beyond your control. You have the right *not* to take on a responsibility for an impossible situation, or one in which the risks are too great, or the ethical implications too sketchy. You\\'ll have to make the call based on your own values and judgment.\\n\\nWhen you *do* accept the responsibility for an outcome, you should expect to be held accountable for it. When you make a mistake (as we all do) or an error in judgment, admit it honestly and try to offer options.\\n\\nDon\\'t blame someone or something else, or make up an excuse. Don\\'t blame all the problems on a vendor, a programming language, management, or your coworkers. Any and all of these may play a role, but it is up to *you* to provide solutions, not excuses.\\n\\nIf there was a risk that the vendor wouldn\\'t come through for you, then you should have had a contingency plan. If your mass storage melts—taking all of your source code with it—and you don\\'t have a backup, it\\'s your fault. Telling your boss \"the cat ate my source code\\'\\' just won\\'t cut it.\\n\\n- \\n## **Tip 4** Provide Options, Don\\'t Make Lame Excuses\\n\\nBefore you approach anyone to tell them why something can\\'t be done, is late, or is broken, stop and listen to yourself. Talk to the rubber duck on your monitor, or the cat. Does your excuse sound reasonable, or stupid? How\\'s it going to sound to your boss?\\n\\nRun through the conversation in your mind. What is the other person likely to say? Will they ask, \"Have you tried this…\" or \"Didn\\'t you consider that?\" How will you respond? Before you go and tell them the bad news, is there anything else you can try? Sometimes, you just *know* what they are going to say, so save them the trouble.\\n\\nInstead of excuses, provide options. Don\\'t say it can\\'t be done; explain what *can* be done to salvage the situation. Does code have to be deleted? Tell them so, and explain the value of refactoring (see Topic 40, *Refactoring* ).\\n\\nDo you need to spend time prototyping to determine the best way to proceed (see Topic 13, *Prototypes and Post-it Notes*)? Do you need to introduce better testing (see Topic 41, *Test to Code*, and *Ruthless and Continuous Testing*) or automation to prevent it from happening again?\\n\\nPerhaps you need additional resources to complete this task. Or maybe you need to spend more time with the users? Or maybe it\\'s just you: do you need to learn some technique or technology in greater depth? Would a book or a course help? Don\\'t be afraid to ask, or to admit that you need help.\\n\\nTry to flush out the lame excuses before voicing them aloud. If you must, tell your cat first. After all, if little Tiddles is going to take the blame….\\n\\n## **Related Sections Include**\\n\\n- Topic 49, *Pragmatic Teams*\\n## **Challenges**\\n\\n- How do you react when someone—such as a bank teller, an auto mechanic, or a clerk—comes to you with a lame excuse? What do you think of them and their company as a result?\\n- When you find yourself saying, \"I don\\'t know,\" be sure to follow it up with \"—but I\\'ll find out.\" It\\'s a great way to admit what you don\\'t know, but then take responsibility like a pro.\\n## **Topic 3** Software Entropy\\n\\nWhile software development is immune from almost all physical laws, the inexorable increase in *entropy* hits us hard. *Entropy* is a term from physics that refers to the amount of \"disorder\" in a system. Unfortunately, the laws of thermodynamics guarantee that the entropy in the universe tends toward a maximum. When disorder increases in software, we call it \"software rot.\" Some folks might call it by the more optimistic term, \"technical debt,\" with the implied notion that they\\'ll pay it back someday. They probably won\\'t.\\n\\nWhatever the name, though, both debt and rot can spread uncontrollably.\\n\\nThere are many factors that can contribute to software rot. The most important one seems to be the psychology, or culture, at work on a project. Even if you are a team of one, your project\\'s psychology can be a very delicate thing. Despite the best-laid plans and the best people, a project can still experience ruin and decay during its lifetime. Yet there are other projects that, despite enormous difficulties and constant setbacks, successfully fight nature\\'s tendency toward disorder and manage to come out pretty well.\\n\\nWhat makes the difference?\\n\\nIn inner cities, some buildings are beautiful and clean, while others are rotting hulks. Why? Researchers in the field of crime and urban decay discovered a fascinating trigger mechanism, one that very quickly turns a clean, intact, inhabited building into a smashed and abandoned derelict.[5]\\n\\nA broken window.\\n\\nOne broken window, left unrepaired for any substantial length of time, instills in the inhabitants of the building a sense of abandonment—a sense that the powers that be don\\'t care about the building. So another window gets broken. People start littering. Graffiti appears. Serious structural damage begins. In a relatively short span of time, the building becomes damaged beyond the owner\\'s desire to fix it, and the sense of abandonment becomes reality.\\n\\nWhy would that make a difference? Psychologists have done studies[6] that show hopelessness can be contagious. Think of the flu virus in close quarters. Ignoring a clearly broken situation reinforces the ideas that perhaps *nothing* can be fixed, that no one cares, all is doomed; all negative thoughts which can spread among team members, creating a vicious spiral.\\n\\n## **Tip 5** Don\\'t Live with Broken Windows\\n\\nDon\\'t leave \"broken windows\\'\\' (bad designs, wrong decisions, or poor code) unrepaired. Fix each one as soon as it is discovered. If there is insufficient time to fix it properly, then *board it up.* Perhaps you can comment out the offending code, or display a \"Not Implemented\" message, or substitute dummy data instead. Take *some* action to prevent further damage and to show that you\\'re on top of the situation.\\n\\nWe\\'ve seen clean, functional systems deteriorate pretty quickly once windows start breaking. There are other factors that can contribute to software rot, and we\\'ll touch on some of them elsewhere, but neglect *accelerates* the rot faster than any other factor.\\n\\nYou may be thinking that no one has the time to go around cleaning up all the broken glass of a project. If so, then you\\'d better plan on getting a dumpster, or moving to another neighborhood. Don\\'t let entropy win.\\n\\n## **First, Do No Harm**\\n\\nAndy once had an acquaintance who was obscenely rich. His house was immaculate, loaded with priceless antiques, *objets d\\'art*, and so on. One day, a tapestry that was hanging a little too close to a fireplace caught on fire. The fire department rushed in to save the day—and his house. But before they dragged their big, dirty hoses into the house, they stopped with the fire raging—to roll out a mat between the front door and the source of the fire.\\n\\nThey didn\\'t want to mess up the carpet.\\n\\nNow that sounds pretty extreme. Surely the fire department\\'s first priority is to put out the fire, collateral damage be damned. But they clearly had assessed the situation, were confident of their ability to manage the fire, and were careful not to inflict unnecessary damage to the property. That\\'s the way it must be with software: don\\'t cause collateral damage just because there\\'s a crisis of some sort. One broken window is one too many.\\n\\nOne broken window—a badly designed piece of code, a poor management decision that the team must live with for the duration of the project—is all it takes to start the decline. If you find yourself working on a project with quite a few broken windows, it\\'s all too easy to slip into the mindset of \"All the rest of this code is crap, I\\'ll just follow suit.\" It doesn\\'t matter if the project has been fine up to this point. In the original experiment leading to the \"Broken Window Theory,\" an abandoned car sat for a week untouched. But once a single window was broken, the car was stripped and turned upside down within *hours*.\\n\\nBy the same token, if you find yourself on a project where the code is pristinely beautiful—cleanly written, well designed, and elegant—you will likely take extra special care not to mess it up, just like the firefighters. Even if there\\'s a fire raging (deadline, release date, trade show demo, etc.), *you* don\\'t want to be the first one to make a mess and inflict additional damage.\\n\\nJust tell yourself, \"No broken windows.\"\\n\\n## **Related Sections Include**\\n\\n- Topic 10, *Orthogonality*\\n- Topic 40, *Refactoring*\\n- Topic 44, *Naming Things*\\n\\n## **Challenges**\\n\\n- Help strengthen your team by surveying your project neighborhood. Choose two or three broken windows and discuss with your colleagues what the problems are and what could be done to fix them.\\n- Can you tell when a window first gets broken? What is your reaction? If it was the result of someone else\\'s decision, or a management edict, what can you do about it?\\n\\n*The three soldiers returning home from war were hungry. When they saw the village ahead their spirits lifted—they were sure the villagers would give them a meal. But when they got there, they found the doors locked and the windows closed. After many years of war, the villagers were short of food, and hoarded what they had.*\\n\\n*Undeterred, the soldiers boiled a pot of water and carefully placed three stones into it. The amazed villagers came out to watch.*\\n\\n*\"This is stone soup,\" the soldiers explained. \"Is that all you put in it?\" asked the villagers. \"Absolutely—although some say it tastes even better with a few carrots…\" A villager ran off, returning in no time with a basket of carrots from his hoard.*\\n\\n*A couple of minutes later, the villagers again asked \"Is that it?\"*\\n\\n*\"Well,\" said the soldiers, \"a couple of potatoes give it body.\" Off ran another villager.*\\n\\n*Over the next hour, the soldiers listed more ingredients that would enhance the soup: beef, leeks, salt, and herbs. Each time a different villager would run off to raid their personal stores.*\\n\\n*Eventually they had produced a large pot of steaming soup. The soldiers removed the stones, and they sat down with the entire village to enjoy the first square meal any of them had eaten in months.*\\n\\nThere are a couple of morals in the stone soup story. The villagers are tricked by the soldiers, who use the villagers\\' curiosity to get food from them. But more importantly, the soldiers act as a catalyst, bringing the\\n\\nvillage together so they can jointly produce something that they couldn\\'t have done by themselves—a synergistic result. Eventually everyone wins.\\n\\nEvery now and then, you might want to emulate the soldiers.\\n\\nYou may be in a situation where you know exactly what needs doing and how to do it. The entire system just appears before your eyes—you know it\\'s right. But ask permission to tackle the whole thing and you\\'ll be met with delays and blank stares. People will form committees, budgets will need approval, and things will get complicated. Everyone will guard their own resources. Sometimes this is called \"start-up fatigue.\\'\\'\\n\\nIt\\'s time to bring out the stones. Work out what you *can* reasonably ask for. Develop it well. Once you\\'ve got it, show people, and let them marvel. Then say \"of course, it *would* be better if we added…\\'\\' Pretend it\\'s not important. Sit back and wait for them to start asking you to add the functionality you originally wanted. People find it easier to join an ongoing success. Show them a glimpse of the future and you\\'ll get them to rally around.[7]\\n\\n- **Tip 6** Be a Catalyst for Change\\n## **The Villagers\\' Side**\\n\\nOn the other hand, the stone soup story is also about gentle and gradual deception. It\\'s about focusing too tightly. The villagers think about the stones and forget about the rest of the world. We all fall for it, every day. Things just creep up on us.\\n\\nWe\\'ve all seen the symptoms. Projects slowly and inexorably get totally out of hand. Most software disasters start out too small to notice, and most project overruns happen a day at a time. Systems drift from their specifications feature by feature, while patch after patch gets added to a\\n\\npiece of code until there\\'s nothing of the original left. It\\'s often the accumulation of small things that breaks morale and teams.\\n\\n## **Tip 7** Remember the Big Picture\\n\\nWe\\'ve never tried this—honest. But \"they\" say that if you take a frog and drop it into boiling water, it will jump straight back out again. However, if you place the frog in a pan of cold water, then gradually heat it, the frog won\\'t notice the slow increase in temperature and will stay put until cooked.\\n\\nNote that the frog\\'s problem is different from the broken windows issue discussed in Topic 3, *Software Entropy*. In the Broken Window Theory, people lose the will to fight entropy because they perceive that no one else cares. The frog just doesn\\'t notice the change.\\n\\nDon\\'t be like the fabled frog. Keep an eye on the big picture. Constantly review what\\'s happening around you, not just what you personally are doing.\\n\\n## **Related Sections Include**\\n\\n- Topic 1, *It\\'s Your Life*\\n- Topic 38, *Programming by Coincidence*\\n\\n## **Challenges**\\n\\n- While reviewing a draft of the first edition, John Lakos raised the following issue: The soldiers progressively deceive the villagers, but the change they catalyze does them all good. However, by progressively deceiving the frog, you\\'re doing it harm. Can you determine whether you\\'re making stone soup or frog soup when you try to catalyze change? Is the decision subjective or objective?\\n- Quick, without looking, how many lights are in the ceiling above you? How many exits in the room? How many people? Is there anything out of context, anything that looks like it doesn\\'t belong? This is an exercise in *situational awareness*, a technique practiced by folks ranging from Boy and Girl Scouts to Navy SEALs. Get in the habit of really looking and noticing your surroundings. Then do the same for your project.\\n## **Topic 5** Good-Enough Software\\n\\n*Striving to better, oft we mar what\\'s well.*\\n\\n*Shakespeare, King Lear 1.4*\\n\\nThere\\'s an old(ish) joke about a company that places an order for 100,000 ICs with a Japanese manufacturer. Part of the specification was the defect rate: one chip in 10,000. A few weeks later the order arrived:\\n\\none large box containing thousands of ICs, and a small one containing just ten. Attached to the small box was a label that read: \"These are the faulty ones.\\'\\'\\n\\nIf only we really had this kind of control over quality. But the real world just won\\'t let us produce much that\\'s truly perfect, particularly not bug-free software. Time, technology, and temperament all conspire against us.\\n\\nHowever, this doesn\\'t have to be frustrating. As Ed Yourdon described in an article in *IEEE Software, When good-enough software is best* [You95], you can discipline yourself to write software that\\'s good enough—good enough for your users, for future maintainers, for your own peace of mind. You\\'ll find that you are more productive and your users are happier. And you may well find that your programs are actually better for their shorter incubation.\\n\\nBefore we go any further, we need to qualify what we\\'re about to say. The phrase \"good enough\\'\\' does not imply sloppy or poorly produced code. All systems must meet their users\\' requirements to be successful, and meet basic performance, privacy, and security standards. We are simply advocating that users be given an opportunity to participate in the process of deciding when what you\\'ve produced is good enough for their needs.\\n\\n## **Involve Your Users in the Trade-Off**\\n\\nNormally you\\'re writing software for other people. Often you\\'ll remember to find out what they want.[8] But do you ever ask them *how good* they want their software to be? Sometimes there\\'ll be no choice. If you\\'re working on pacemakers, an autopilot, or a low-level library that will be widely disseminated, the requirements will be more stringent and your options more limited.\\n\\nHowever, if you\\'re working on a brand-new product, you\\'ll have different constraints. The marketing people will have promises to keep, the eventual end users may have made plans based on a delivery schedule, and your company will certainly have cash-flow constraints. It would be unprofessional to ignore these users\\' requirements simply to add new features to the program, or to polish up the code just one more time. We\\'re not advocating panic: it is equally unprofessional to promise impossible time scales and to cut basic engineering corners to meet a deadline.\\n\\nThe scope and quality of the system you produce should be discussed as part of that system\\'s requirements.\\n\\n## **Tip 8** Make Quality a Requirements Issue\\n\\nOften you\\'ll be in situations where trade-offs are involved. Surprisingly, many users would rather use software with some rough edges *today* than wait a year for the shiny, bells-and-whistles version (and in fact what they will need a year from now may be completely different anyway). Many IT departments with tight budgets would agree. Great software today is often preferable to the fantasy of perfect software tomorrow. If you give your users something to play with early, their feedback will often lead you to a better eventual solution (see Topic 12, *Tracer Bullets*).\\n\\n## **Know When to Stop**\\n\\nIn some ways, programming is like painting. You start with a blank canvas and certain basic raw materials. You use a combination of science, art, and craft to determine what to do with them. You sketch out an overall shape, paint the underlying environment, then fill in the details. You constantly step back with a critical eye to view what you\\'ve done. Every now and then you\\'ll throw a canvas away and start again.\\n\\nBut artists will tell you that all the hard work is ruined if you don\\'t know when to stop. If you add layer upon layer, detail over detail, *the painting becomes lost in the paint*.\\n\\nDon\\'t spoil a perfectly good program by overembellishment and overrefinement. Move on, and let your code stand in its own right for a while. It may not be perfect. Don\\'t worry: it could never be perfect. (In Chapter 7, *While You Are Coding*, we\\'ll discuss philosophies for developing code in an imperfect world.)\\n\\n## **Related Sections Include**\\n\\n- Topic 45, *The Requirements Pit*\\n- Topic 46, *Solving Impossible Puzzles*\\n\\n## **Challenges**\\n\\n- Look at the software tools and operating systems that you use regularly. Can you find any evidence that these organizations and/or developers are comfortable shipping software they know is not perfect? As a user, would you rather (1) wait for them to get all the bugs out, (2) have complex software and accept some bugs, or (3) opt for simpler software with fewer defects?\\n- Consider the effect of modularization on the delivery of software. Will it take more or less time to get a tightly coupled monolithic block of software to the required quality compared with a system designed as\\n\\nvery loosely coupled modules or microservices? What are the advantages or disadvantages of each approach?\\n\\n- Can you think of popular software that suffers from *feature bloat*? That is, software containing far more features than you would ever use, each feature introducing more opportunity for bugs and security vulnerabilities, and making the features you *do* use harder to find and manage. Are you in danger of falling into this trap yourself?\\n## **Topic 6** Your Knowledge Portfolio\\n\\n*An investment in knowledge always pays the best interest.*\\n\\n*Benjamin Franklin*\\n\\nAh, good old Ben Franklin—never at a loss for a pithy homily. Why, if we could just be early to bed and early to rise, we\\'d be great programmers right? The early bird might get the worm, but what happens to the early\\n\\nworm?\\n\\nIn this case, though, Ben really hit the nail on the head. Your knowledge and experience are your most important day-to-day professional assets.\\n\\nUnfortunately, they\\'re *expiring assets*. [9] Your knowledge becomes out of date as new techniques, languages, and environments are developed. Changing market forces may render your experience obsolete or irrelevant. Given the ever-increasing pace of change in our technological society, this can happen pretty quickly.\\n\\nAs the value of your knowledge declines, so does your value to your company or client. We want to prevent this from ever happening.\\n\\nYour ability to learn new things is your most important strategic asset. But how do you learn *how* to learn, and how do you know *what* to learn?\\n\\n## **Your Knowledge Portfolio**\\n\\nWe like to think of all the facts programmers know about computing, the application domains they work in, and all their experience as their *knowledge portfolios.* Managing a knowledge portfolio is very similar to managing a financial portfolio:\\n\\n1. Serious investors invest regularly—as a habit.\\n\\n- 2. Diversification is the key to long-term success.\\n- 3. Smart investors balance their portfolios between conservative and high-risk, high-reward investments.\\n- 4. Investors try to buy low and sell high for maximum return.\\n- 5. Portfolios should be reviewed and rebalanced periodically.\\n\\nTo be successful in your career, you must invest in your knowledge portfolio using these same guidelines.\\n\\nThe good news is that managing this kind of investment is a skill just like any other—it can be learned. The trick is to make yourself do it initially and form a habit. Develop a routine which you follow until your brain internalizes it. At that point, you\\'ll find yourself sucking up new knowledge automatically.\\n\\n## **Building Your Portfolio**\\n\\n## *Invest regularly*\\n\\nJust as in financial investing, you must invest in your knowledge portfolio *regularly*, even if it\\'s just a small amount. The habit is as important as the sums, so plan to use a consistent time and place, away from interruptions. A few sample goals are listed in the next section.\\n\\n## *Diversify*\\n\\nThe more *different* things you know, the more valuable you are. As a baseline, you need to know the ins and outs of the particular technology you are working with currently. But don\\'t stop there. The face of computing changes rapidly—hot technology today may well be close to useless (or at least not in demand) tomorrow. The more technologies you are comfortable with, the better you will be able to adjust to change. And don\\'t forget all the *other* skills you need, including those in non-technical areas.\\n\\n### *Manage risk*\\n\\nTechnology exists along a spectrum from risky, potentially highreward to low-risk, low-reward standards. It\\'s not a good idea to invest all of your money in high-risk stocks that might collapse suddenly, nor should you invest all of it conservatively and miss out on possible opportunities. Don\\'t put all your technical eggs in one basket.\\n\\n#### *Buy low, sell high*\\n\\nLearning an emerging technology before it becomes popular can be just as hard as finding an undervalued stock, but the payoff can be just as rewarding. Learning Java back when it was first introduced and unknown may have been risky at the time, but it paid off handsomely for the early adopters when it became an industry mainstay later.\\n\\n#### *Review and rebalance*\\n\\nThis is a very dynamic industry. That hot technology you started investigating last month might be stone cold by now. Maybe you need to brush up on that database technology that you haven\\'t used in a while. Or perhaps you could be better positioned for that new job opening if you tried out that other language….\\n\\nOf all these guidelines, the most important one is the simplest to do:\\n\\n## **Tip 9** Invest Regularly in Your Knowledge Portfolio\\n\\n## **Goals**\\n\\nNow that you have some guidelines on what and when to add to your knowledge portfolio, what\\'s the best way to go about acquiring intellectual capital with which to fund your portfolio? Here are a few suggestions:\\n\\n#### *Learn at least one new language every year*\\n\\nDifferent languages solve the same problems in different ways. By learning several different approaches, you can help broaden your\\n\\nthinking and avoid getting stuck in a rut. Additionally, learning many languages is easy thanks to the wealth of freely available software.\\n\\n#### *Read a technical book each month*\\n\\nWhile there\\'s a glut of short-form essays and occasionally reliable answers on the web, for deep understanding you need long-form books. Browse the booksellers for technical books on interesting topics related to your current project.[10] Once you\\'re in the habit, read a book a month. After you\\'ve mastered the technologies you\\'re currently using, branch out and study some that *don\\'t* relate to your project.\\n\\n#### *Read nontechnical books, too*\\n\\nIt is important to remember that computers are used by *people* people whose needs you are trying to satisfy. You work with people, are employed by people, and get hacked by people. Don\\'t forget the human side of the equation, as that requires an entirely different skill set (we ironically call these *soft* skills, but they are actually quite hard to master).\\n\\n#### *Take classes*\\n\\nLook for interesting courses at a local or online college or university, or perhaps at the next nearby trade show or conference.\\n\\n#### *Participate in local user groups and meetups*\\n\\nIsolation can be deadly to your career; find out what people are working on outside of your company. Don\\'t just go and listen: actively participate.\\n\\n#### *Experiment with different environments*\\n\\nIf you\\'ve worked only in Windows, spend some time with Linux. If you\\'ve used only makefiles and an editor, try a sophisticated IDE with cutting-edge features, and vice versa.\\n\\n#### *Stay current*\\n\\nRead news and posts online on technology different from that of your current project. It\\'s a great way to find out what experiences other people are having with it, the particular jargon they use, and so on.\\n\\nIt\\'s important to continue investing. Once you feel comfortable with some new language or bit of technology, move on. Learn another one.\\n\\nIt doesn\\'t matter whether you ever use any of these technologies on a project, or even whether you put them on your resume. The process of learning will expand your thinking, opening you to new possibilities and new ways of doing things. The cross-pollination of ideas is important; try to apply the lessons you\\'ve learned to your current project. Even if your project doesn\\'t use that technology, perhaps you can borrow some ideas. Get familiar with object orientation, for instance, and you\\'ll write procedural programs differently. Understand the functional programming paradigm and you\\'ll write object-oriented code differently, and so on.\\n\\n## **Opportunities for Learning**\\n\\nSo you\\'re reading voraciously, you\\'re on top of all the latest breaking developments in your field (not an easy thing to do), and somebody asks you a question. You don\\'t have the faintest idea what the answer is, and freely admit as much.\\n\\n*Don\\'t let it stop there.* Take it as a personal challenge to find the answer. Ask around. Search the web—the scholarly parts too, not just the consumer parts.\\n\\nIf you can\\'t find the answer yourself, find out who *can*. Don\\'t let it rest. Talking to other people will help build your personal network, and you may surprise yourself by finding solutions to other, unrelated problems along the way. And that old portfolio just keeps getting bigger….\\n\\nAll of this reading and researching takes time, and time is already in short supply. So you need to plan ahead. Always have something to read in an\\n\\notherwise dead moment. Time spent waiting for doctors and dentists can be a great opportunity to catch up on your reading—but be sure to bring your own e-reader with you, or you might find yourself thumbing through a dogeared 1973 article about Papua New Guinea.\\n\\n## **Critical Thinking**\\n\\nThe last important point is to think *critically* about what you read and hear. You need to ensure that the knowledge in your portfolio is accurate and unswayed by either vendor or media hype. Beware of the zealots who insist that their dogma provides the *only* answer—it may or may not be applicable to you and your project.\\n\\nNever underestimate the power of commercialism. Just because a web search engine lists a hit first doesn\\'t mean that it\\'s the best match; the content provider can pay to get top billing. Just because a bookstore features a book prominently doesn\\'t mean it\\'s a good book, or even popular; they may have been paid to place it there.\\n\\n## **Tip 10** Critically Analyze What You Read and Hear\\n\\nCritical thinking is an entire discipline unto itself, and we encourage you to read and study all you can about it. In the meantime, here\\'s a head start with a few questions to ask and think about.\\n\\n## *Ask the \"Five Whys\"*\\n\\nA favorite consulting trick: ask \"why?\" at least five times. Ask a question, and get an answer. Dig deeper by asking \"why?\" Repeat as if you were a petulant four-year old (but a polite one). You might be able to get closer to a root cause this way.\\n\\n### *Who does this benefit?*\\n\\nIt may sound cynical, but *follow the money* can be a very helpful path to analyze. The benefits to someone else or another organization may\\n\\nbe aligned with your own, or not.\\n\\n#### *What\\'s the context?*\\n\\nEverything occurs in its own context, which is why \"one size fits all\" solutions often don\\'t. Consider an article or book touting a \"best practice.\" Good questions to consider are \"best for who?\" What are the prerequisites, what are the consequences, short and long term?\\n\\n### *When or Where would this work?*\\n\\nUnder what circumstances? Is it too late? Too early? Don\\'t stop with first-order thinking (*what will happen next*), but use second-order thinking: *what will happen after that?*\\n\\n#### *Why is this a problem?*\\n\\nIs there an underlying model? How does the underlying model work?\\n\\nUnfortunately, there are very few simple answers anymore. But with your extensive portfolio, and by applying some critical analysis to the torrent of technical articles you will read, you can understand the *complex* answers.\\n\\n## **Related Sections Include**\\n\\n- Topic 1, *It\\'s Your Life*\\n- Topic 22, *Engineering Daybooks*\\n\\n## **Challenges**\\n\\n- Start learning a new language this week. Always programmed in the same old language? Try Clojure, Elixir, Elm, F#, Go, Haskell, Python, R, ReasonML, Ruby, Rust, Scala, Swift, TypeScript, or anything else that appeals and/or looks as if you might like it.[11]\\n- Start reading a new book (but finish this one first!). If you are doing very detailed implementation and coding, read a book on design and\\n\\narchitecture. If you are doing high-level design, read a book on coding techniques.\\n\\n- Get out and talk technology with people who aren\\'t involved in your current project, or who don\\'t work for the same company. Network in your company cafeteria, or maybe seek out fellow enthusiasts at a local meetup.\\n## **Topic 7** Communicate!\\n\\n*I believe that it is better to be looked over than it is to be overlooked.*\\n\\n*Mae West, Belle of the Nineties, 1934*\\n\\nMaybe we can learn a lesson from Ms. West. It\\'s not just what you\\'ve got, but also how you package it. Having the best ideas, the finest code, or the most pragmatic thinking is ultimately sterile unless you can communicate with other\\n\\npeople. A good idea is an orphan without effective communication.\\n\\nAs developers, we have to communicate on many levels. We spend hours in meetings, listening and talking. We work with end users, trying to understand their needs. We write code, which communicates our intentions to a machine and documents our thinking for future generations of developers. We write proposals and memos requesting and justifying resources, reporting our status, and suggesting new approaches. And we work daily within our teams to advocate our ideas, modify existing practices, and suggest new ones. A large part of our day is spent communicating, so we need to do it well.\\n\\nTreat English (or whatever your native tongue may be) as just another programming language. Write natural language as you would write code: honor the DRY principle, ETC, automation, and so on. (We discuss the DRY and ETC design principles in the next chapter.)\\n\\n## **Tip 11** English is Just Another Programming Language\\n\\nWe\\'ve put together a list of additional ideas that we find useful.\\n\\n## **Know Your Audience**\\n\\nYou\\'re communicating only if you\\'re conveying what you mean to convey —just talking isn\\'t enough. To do that, you need to understand the needs, interests, and capabilities of your audience. We\\'ve all sat in meetings where a development geek glazes over the eyes of the vice president of marketing with a long monologue on the merits of some arcane technology. This isn\\'t communicating: it\\'s just talking, and it\\'s annoying.[12]\\n\\nSay you want to change your remote monitoring system to use a third-party message broker to disseminate status notifications. You can present this update in many different ways, depending on your audience. End users will appreciate that their systems can now interoperate with other services that use the broker. Your marketing department will be able to use this fact to boost sales. Development and operations managers will be happy because the care and maintenance of that part of the system is now someone else\\'s problem. Finally, developers may enjoy getting experience with new APIs, and may even be able to find new uses for the message broker. By making the appropriate pitch to each group, you\\'ll get them all excited about your project.\\n\\nAs with all forms of communication, the trick here is to gather feedback. Don\\'t just wait for questions: ask for them. Look at body language, and facial expressions. One of the Neuro Linguistic Programming presuppositions is \"The meaning of your communication is the response you get.\" Continuously improve your knowledge of your audience as you communicate.\\n\\n## **Know What You Want to Say**\\n\\nProbably the most difficult part of the more formal styles of communication used in business is working out exactly what it is you want to say. Fiction writers often plot out their books in detail before they start, but people writing technical documents are often happy to sit down at a keyboard, enter:\\n\\n### 1. Introduction\\n\\nand start typing whatever comes into their heads next.\\n\\nPlan what you want to say. Write an outline. Then ask yourself, \"Does this communicate what I want to express to my audience in a way that works for them?\" Refine it until it does.\\n\\nThis approach works for more than just documents. When you\\'re faced with an important meeting or a chat with a major client, jot down the ideas you want to communicate, and plan a couple of strategies for getting them across.\\n\\nNow that you know what your audience wants, let\\'s deliver it.\\n\\n## **Choose Your Moment**\\n\\nIt\\'s six o\\'clock on Friday afternoon, following a week when the auditors have been in. Your boss\\'s youngest is in the hospital, it\\'s pouring rain outside, and the commute home is guaranteed to be a nightmare. This probably isn\\'t a good time to ask her for a memory upgrade for your laptop.\\n\\nAs part of understanding what your audience needs to hear, you need to work out what their priorities are. Catch a manager who\\'s just been given a hard time by her boss because some source code got lost, and you\\'ll have a more receptive listener to your ideas on source code repositories. Make what you\\'re saying relevant in time, as well as in content. Sometimes all it takes is the simple question, \"Is this a good time to talk about…?\\'\\'\\n\\n## **Choose a Style**\\n\\nAdjust the style of your delivery to suit your audience. Some people want a formal \"just the facts\\'\\' briefing. Others like a long, wide-ranging chat before getting down to business. What is their skill level and experience in\\n\\nthis area? Are they experts? Newbies? Do they need hand-holding or just a quick tl;dr? If in doubt, ask.\\n\\nRemember, however, that you are half of the communication transaction. If someone says they need a paragraph describing something and you can\\'t see any way of doing it in less than several pages, tell them so. Remember, that kind of feedback is a form of communication, too.\\n\\n## **Make It Look Good**\\n\\nYour ideas are important. They deserve a good-looking vehicle to convey them to your audience.\\n\\nToo many developers (and their managers) concentrate solely on content when producing written documents. We think this is a mistake. Any chef (or watcher of the Food Network) will tell you that you can slave in the kitchen for hours only to ruin your efforts with poor presentation.\\n\\nThere is no excuse today for producing poor-looking printed documents. Modern software can produce stunning output, regardless of whether you\\'re writing using Markdown or using a word processor. You need to learn just a few basic commands. If you\\'re using a word processor, use its style sheets for consistency. (Your company may already have defined style sheets that you can use.) Learn how to set page headers and footers. Look at the sample documents included with your package to get ideas on style and layout. *Check the spelling,* first automatically and then by hand. After awl, their are spelling miss steaks that the chequer can knot ketch.\\n\\n## **Involve Your Audience**\\n\\nWe often find that the documents we produce end up being less important than the process we go through to produce them. If possible, involve your readers with early drafts of your document. Get their feedback, and pick their brains. You\\'ll build a good working relationship, and you\\'ll probably produce a better document in the process.\\n\\n## **Be a Listener**\\n\\nThere\\'s one technique that you must use if you want people to listen to you: *listen to them.* Even if this is a situation where you have all the information, even if this is a formal meeting with you standing in front of 20 suits—if you don\\'t listen to them, they won\\'t listen to you.\\n\\nEncourage people to talk by asking questions, or ask them to restate the discussion in their own words. Turn the meeting into a dialog, and you\\'ll make your point more effectively. Who knows, you might even learn something.\\n\\n## **Get Back to People**\\n\\nIf you ask someone a question, you feel they\\'re impolite if they don\\'t respond. But how often do you fail to get back to people when they send you an email or a memo asking for information or requesting some action? In the rush of everyday life, it\\'s easy to forget. Always respond to emails and voicemails, even if the response is simply \"I\\'ll get back to you later.\\'\\' Keeping people informed makes them far more forgiving of the occasional slip, and makes them feel that you haven\\'t forgotten them.\\n\\n## **Tip 12** It\\'s Both What You Say and the Way You Say It\\n\\nUnless you work in a vacuum, you need to be able to communicate. The more effective that communication, the more influential you become.\\n\\n## **Documentation**\\n\\nFinally, there\\'s the matter of communicating via documentation. Typically, developers don\\'t give much thought to documentation. At best it is an unfortunate necessity; at worst it is treated as a low-priority task in the hope that management will forget about it at the end of the project.\\n\\nPragmatic Programmers embrace documentation as an integral part of the overall development process. Writing documentation can be made easier by not duplicating effort or wasting time, and by keeping documentation close at hand—in the code itself. In fact, we want to apply *all* of our pragmatic principles to documentation as well as to code.\\n\\n## **Tip 13** Build Documentation In, Don\\'t Bolt It On\\n\\nIt\\'s easy to produce good-looking documentation from the comments in source code, and we recommend adding comments to modules and exported functions to give other developers a leg up when they come to use it.\\n\\nHowever, this doesn\\'t mean we agree with the folks who say that *every* function, data structure, type declaration, etc., needs its own comment. This kind of mechanical comment writing actually makes it more difficult to maintain code: now there are two things to update when you make a change. So restrict your non-API commenting to discussing *why* something is done, its purpose and its goal. The code already shows *how* it is done, so commenting on this is redundant—and is a violation of the DRY principle.\\n\\nCommenting source code gives you the perfect opportunity to document those elusive bits of a project that can\\'t be documented anywhere else: engineering trade-offs, why decisions were made, what other alternatives were discarded, and so on.\\n\\n## **Summary**\\n\\n- Know what you want to say.\\n- Know your audience.\\n- Choose your moment.\\n- Choose a style.\\n- Make it look good.\\n- Involve your audience.\\n- Be a listener.\\n- Get back to people.\\n- Keep code and documentation together.\\n\\n## **Related Sections Include**\\n\\n- Topic 15, *Estimating*\\n- Topic 18, *Power Editing*\\n- Topic 45, *The Requirements Pit*\\n- Topic 49, *Pragmatic Teams*\\n\\n#### **Online Communication**\\n\\nEverything we\\'ve said about communicating in writing applies equally to email, social media posts, blogs, and so on. Email in particular has evolved to the point where it is a mainstay of corporate communications; it\\'s used to discuss contracts, to settle disputes, and as evidence in court. But for some reason, people who would never send out a shabby paper document are happy to fling nasty-looking, incoherent emails around the world.\\n\\nOur tips are simple:\\n\\n- Proofread before you hit SEND .\\n- Check your spelling and look for any accidental auto-correct mishaps.\\n- Keep the format simple and clear.\\n- Keep quoting to a minimum. No one likes to receive back their own 100-line email with \"I agree\" tacked on.\\n- If you\\'re quoting other people\\'s email, be sure to attribute it, and quote it inline (rather than as an attachment). Same when quoting on social media platforms.\\n- Don\\'t flame or act like a troll unless you want it to come back and haunt you later. If you wouldn\\'t say it to someone\\'s face, don\\'t say it online.\\n- Check your list of recipients before sending. It\\'s become a cliché to criticize the boss over departmental email without realizing that the boss is on the cc list. Better yet, don\\'t criticize the boss over email.\\n\\nAs countless large corporations and politicians have discovered, email and social media posts are forever. Try to give the same attention and care to email as you would to any written memo or report.\\n\\n## **Challenges**\\n\\n- There are several good books that contain sections on communications within teams, including *The Mythical Man-Month: Essays on Software Engineering* [Bro96] and *Peopleware: Productive Projects and Teams* [DL13]. Make it a point to try to read these over the next 18 months. In addition, *Dinosaur Brains: Dealing with All Those Impossible People at Work* [BR89] discusses the emotional baggage we all bring to the work environment.\\n- The next time you have to give a presentation, or write a memo advocating some position, try working through the advice in this section before you start. Explicitly identify the audience and what you need to communicate. If appropriate, talk to your audience afterward and see how accurate your assessment of their needs was.\\n\\n#### **Footnotes**\\n\\n- [3] http://wiki.c2.com/?ChangeYourOrganization\\n- [4] See, for example, a good meta-analysis at *Trust and team performance: A meta-analysis of main effects, moderators, and covariates*, http://dx.doi.org/10.1037/apl0000110\\n- [5] See *The police and neighborhood safety* [WH82]\\n- [6] See *Contagious depression: Existence, specificity to depressed symptoms, and the role of reassurance seeking* [Joi94]\\n- [7] While doing this, you may be comforted by the line attributed to Rear Admiral Dr. Grace Hopper: \"It\\'s easier to ask forgiveness than it is to get permission.\\'\\'\\n- [8] That was supposed to be a joke!\\n- [9] An *expiring asset* is something whose value diminishes over time. Examples include a warehouse full of bananas and a ticket to a ball game.\\n- [10] We may be biased, but there\\'s a fine selection available at https://pragprog.com.\\n- [11] Never heard of any of these languages? Remember, knowledge is an expiring asset, and so is popular technology. The list of hot new and experimental languages was very different for the\\n\\nfirst edition, and is probably different again by the time you read this. All the more reason to keep learning.\\n\\n- [12] The word *annoy* comes from the Old French *enui*, which also means \"to bore.\\'\\'\\nCopyright © 2020 Pearson Education, Inc.\\n\\n# Chapter 2\\n\\n# **A Pragmatic Approach**\\n\\nThere are certain tips and tricks that apply at all levels of software development, processes that are virtually universal, and ideas that are almost axiomatic. However, these approaches are rarely documented as such; you\\'ll mostly find them written down as odd sentences in discussions of design, project management, or coding. But for your convenience, we\\'ll bring these ideas and processes together here.\\n\\nThe first and maybe most important topic gets to the heart of software development: Topic 8, *The Essence of Good Design*. Everything follows from this.\\n\\nThe next two sections, Topic 9, *DRY—The Evils of Duplication* and Topic 10, *Orthogonality*, are closely related. The first warns you not to duplicate knowledge throughout your systems, the second not to split any one piece of knowledge across multiple system components.\\n\\nAs the pace of change increases, it becomes harder and harder to keep our applications relevant. In Topic 11, *Reversibility*, we\\'ll look at some techniques that help insulate your projects from their changing environment.\\n\\nThe next two sections are also related. In Topic 12, *Tracer Bullets*, we talk about a style of development that allows you to gather requirements, test\\n\\ndesigns, and implement code at the same time. It\\'s the only way to keep up with the pace of modern life.\\n\\nTopic 13, *Prototypes and Post-it Notes* shows you how to use prototyping to test architectures, algorithms, interfaces, and ideas. In the modern world, it\\'s critical to test ideas and get feedback before you commit to them wholeheartedly.\\n\\nAs computer science slowly matures, designers are producing increasingly higher-level languages. While the compiler that accepts \"make it so\" hasn\\'t yet been invented, in Topic 14, *Domain Languages* we present some more modest suggestions that you can implement for yourself.\\n\\nFinally, we all work in a world of limited time and resources. You can survive these scarcities better (and keep your bosses or clients happier) if you get good at working out how long things will take, which we cover in Topic 15, *Estimating*.\\n\\nKeep these fundamental principles in mind during development, and you\\'ll write code that\\'s better, faster, and stronger. You can even make it look easy.\\n\\nThe world is full of gurus and pundits, all eager to pass on their hard-earned wisdom when it comes to How to Design Software. There are acronyms, lists (which seem to favor five entries), patterns, diagrams, videos, talks, and (the internet being the internet) probably a cool series on the Law of Demeter explained using interpretive dance.\\n\\nAnd we, your gentle authors, are guilty of this too. But we\\'d like to make amends by explaining something that only became apparent to us fairly recently. First, the general statement:\\n\\n![](_page_68_Picture_4.jpeg)\\n\\n**Tip 14** Good Design Is Easier to Change Than Bad Design\\n\\nA thing is well designed if it adapts to the people who use it. For code, that means it must adapt by changing. So we believe in the ETC principle: *Easier to Change*. *ETC*. That\\'s it.\\n\\nAs far as we can tell, every design principle out there is a special case of ETC.\\n\\nWhy is decoupling good? Because by isolating concerns we make each easier to change. ETC.\\n\\nWhy is the single responsibility principle useful? Because a change in requirements is mirrored by a change in just one module. ETC.\\n\\nWhy is naming important? Because good names make code easier to read, and you have to read it to change it. ETC!\\n\\n## **ETC Is a Value, Not a Rule**\\n\\nValues are things that help you make decisions: should I do this, or that? When it comes to thinking about software, ETC is a guide, helping you choose between paths. Just like all your other values, it should be floating just behind your conscious thought, subtly nudging you in the right direction.\\n\\nBut how do you make that happen? Our experience is that it requires some initial conscious reinforcement. You may need to spend a week or so deliberately asking yourself \"did the thing I just did make the overall system easier or harder to change?\" Do it when you save a file. Do it when you write a test. Do it when you fix a bug.\\n\\nThere\\'s an implicit premise in ETC. It assumes that a person can tell which of many paths will be easier to change in the future. Much of the time, common sense will be correct, and you can make an educated guess.\\n\\nSometimes, though, you won\\'t have a clue. That\\'s OK. In those cases, we think you can do two things.\\n\\nFirst, given that you\\'re not sure what form change will take, you can always fall back on the ultimate \"easy to change\" path: try to make what you write replaceable. That way, whatever happens in the future, this chunk of code won\\'t be a roadblock. It seems extreme, but actually it\\'s what you should be doing all the time, anyway. It\\'s really just thinking about keeping code decoupled and cohesive.\\n\\nSecond, treat this as a way to develop instincts. Note the situation in your engineering day book: the choices you have, and some guesses about change. Leave a tag in the source. Then, later, when this code has to change, you\\'ll be able to look back and give yourself feedback. It might help the next time you reach a similar fork in the road.\\n\\nThe rest of the sections in this chapter have specific ideas on design, but all are motivated by this one principle.\\n\\n## **Related Sections Include**\\n\\n- Topic 9, *DRY—The Evils of Duplication*\\n- Topic 10, *Orthogonality*\\n- Topic 11, *Reversibility*\\n- Topic 14, *Domain Languages*\\n- Topic 28, *Decoupling*\\n- Topic 30, *Transforming Programming*\\n- Topic 31, *Inheritance Tax*\\n\\n## **Challenges**\\n\\n- Think about a design principle you use regularly. Is it intended to make things easy-to-change?\\n- Also think about languages and programming paradigms (OO, FP, Reactive, and so on). Do any have either big positives or big negatives when it comes to helping you write ETC code? Do any have both?\\n\\nWhen coding, what can you do to eliminate the negatives and accentuate the positives?[13]\\n\\n- Many editors have support (either built-in or via extensions) to run commands when you save a file. Get your editor to popup an *ETC?* message every time you save[14] and use it as a cue to think about the code you just wrote. Is it easy to change?\\n## **Topic 9** DRY—The Evils of Duplication\\n\\nGiving a computer two contradictory pieces of knowledge was Captain James T. Kirk\\'s preferred way of disabling a marauding artificial intelligence. Unfortunately, the same principle can be effective in bringing down *your* code.\\n\\nAs programmers, we collect, organize, maintain, and harness knowledge. We document knowledge in specifications, we make it come alive in running code, and we use it to provide the checks needed during testing.\\n\\nUnfortunately, knowledge isn\\'t stable. It changes—often rapidly. Your understanding of a requirement may change following a meeting with the client. The government changes a regulation and some business logic gets outdated. Tests may show that the chosen algorithm won\\'t work. All this instability means that we spend a large part of our time in maintenance mode, reorganizing and reexpressing the knowledge in our systems.\\n\\nMost people assume that maintenance begins when an application is released, that maintenance means fixing bugs and enhancing features. We think these people are wrong. Programmers are constantly in maintenance mode. Our understanding changes day by day. New requirements arrive and existing requirements evolve as we\\'re heads-down on the project. Perhaps the environment changes. Whatever the reason, maintenance is not a discrete activity, but a routine part of the entire development process.\\n\\nWhen we perform maintenance, we have to find and change the representations of things—those capsules of knowledge embedded in the application. The problem is that it\\'s easy to duplicate knowledge in the specifications, processes, and programs that we develop, and when we do so, we invite a maintenance nightmare—one that starts well before the application ships.\\n\\nWe feel that the only way to develop software reliably, and to make our developments easier to understand and maintain, is to follow what we call the DRY principle:\\n\\n*Every piece of knowledge must have a single, unambiguous, authoritative representation within a system.*\\n\\nWhy do we call it DRY?\\n\\n![](_page_72_Picture_4.jpeg)\\n\\nThe alternative is to have the same thing expressed in two or more places. If you change one, you have to remember to change the others, or, like the alien computers, your program will be brought to its knees by a contradiction. It isn\\'t a question of whether you\\'ll remember: it\\'s a question of when you\\'ll forget.\\n\\nYou\\'ll find the DRY principle popping up time and time again throughout this book, often in contexts that have nothing to do with coding. We feel that it is one of the most important tools in the Pragmatic Programmer\\'s tool box.\\n\\nIn this section we\\'ll outline the problems of duplication and suggest general strategies for dealing with it.\\n\\n## **DRY Is More Than Code**\\n\\nLet\\'s get something out of the way up-front. In the first edition of this book we did a poor job of explaining just what we meant by *Don\\'t Repeat Yourself*. Many people took it to refer to code only: they thought that DRY means \"don\\'t copy-and-paste lines of source.\"\\n\\nThat *is* part of DRY, but it\\'s a tiny and fairly trivial part.\\n\\nDRY is about the duplication of *knowledge*, of *intent*. It\\'s about expressing the same thing in two different places, possibly in two totally different ways.\\n\\nHere\\'s the acid test: when some single facet of the code has to change, do you find yourself making that change in multiple places, and in multiple different formats? Do you have to change code and documentation, or a database schema and a structure that holds it, or…? If so, your code isn\\'t DRY.\\n\\nSo let\\'s look at some typical examples of duplication.\\n\\n## **Duplication in Code**\\n\\nIt may be trivial, but code duplication is oh, so common. Here\\'s an example:\\n\\n```\\ndef print_balance(account)\\n printf \"Debits: %10.2f\\\\n\", account.debits\\n printf \"Credits: %10.2f\\\\n\", account.credits\\n if account.fees < 0\\n printf \"Fees: %10.2f-\\\\n\", -account.fees\\n else\\n printf \"Fees: %10.2f\\\\n\", account.fees\\n end\\n printf \" ———-\\\\n\"\\n if account.balance < 0\\n printf \"Balance: %10.2f-\\\\n\", -account.balance\\n else\\n printf \"Balance: %10.2f\\\\n\", account.balance\\n end\\nend\\n```\\nFor now ignore the implication that we\\'re committing the newbie mistake of storing currencies in floats. Instead see if you can spot duplications in this code. (We can see at least three things, but you might see more.)\\n\\nWhat did you find? Here\\'s our list.\\n\\nFirst, there\\'s clearly a copy-and-paste duplication of handling the negative numbers. We can fix that by adding another function:\\n\\n```\\ndef format_amount(value)\\n result = sprintf( \"%10.2f\", value.abs)\\n if value < 0\\n result + \"-\"\\n else\\n result + \" \"\\n end\\nend\\ndef print_balance(account)\\n printf \"Debits: %10.2f\\\\n\", account.debits\\n printf \"Credits: %10.2f\\\\n\", account.credits\\n printf \"Fees: %s\\\\n\", format_amount(account.fees)\\n printf \" ———-\\\\n\"\\n printf \"Balance: %s\\\\n\", format_amount(account.balance)\\nend\\n```\\nAnother duplication is the repetition of the field width in all the printf calls. We *could* fix this by introducing a constant and passing it to each call, but why not just use the existing function?\\n\\n```\\ndef format_amount(value)\\n result = sprintf( \"%10.2f\", value.abs)\\n if value < 0\\n result + \"-\"\\n else\\n result + \" \"\\n end\\nend\\ndef print_balance(account)\\n printf \"Debits: %s\\\\n\", format_amount(account.debits)\\n printf \"Credits: %s\\\\n\", format_amount(account.credits)\\n printf \"Fees: %s\\\\n\", format_amount(account.fees)\\n printf \" ———-\\\\n\"\\n printf \"Balance: %s\\\\n\", format_amount(account.balance)\\n```\\n**end**\\n\\nAnything more? Well, what if the client asks for an extra space between the labels and the numbers? We\\'d have to change five lines. Let\\'s remove that duplication:\\n\\n```\\ndef format_amount(value)\\n result = sprintf( \"%10.2f\", value.abs)\\n if value < 0\\n result + \"-\"\\n else\\n result + \" \"\\n end\\nend\\ndef print_line(label, value)\\n printf \"%-9s%s\\\\n\", label, value\\nend\\ndef report_line(label, amount)\\n print_line(label + \":\", format_amount(amount))\\nend\\ndef print_balance(account)\\n report_line( \"Debits\", account.debits)\\n report_line( \"Credits\", account.credits)\\n report_line( \"Fees\", account.fees)\\n print_line( \"\", \"———-\")\\n report_line( \"Balance\", account.balance)\\nend\\n```\\nIf we have to change the formatting of amounts, we change format_amount. If we want to change the label format, we change report_line.\\n\\nThere\\'s still an implicit DRY violation: the number of hyphens in the separator line is related to the width of the amount field. But it isn\\'t an exact match: it\\'s currently one character shorter, so any trailing minus signs extend beyond the column. This is the customer\\'s intent, and it\\'s a different intent to the actual formatting of amounts.\\n\\n### **Not All Code Duplication Is Knowledge Duplication**\\n\\nAs part of your online wine ordering application you\\'re capturing and validating your user\\'s age, along with the quantity they\\'re ordering. According to the site owner, they should both be numbers, and both greater than zero. So you code up the validations:\\n\\n```\\ndef validate_age(value):\\n validate_type(value, :integer)\\n validate_min_integer(value, 0)\\ndef validate_quantity(value):\\n validate_type(value, :integer)\\n validate_min_integer(value, 0)\\n```\\nDuring code review, the resident know-it-all bounces this code, claiming it\\'s a DRY violation: both function bodies are the same.\\n\\nThey are wrong. The code is the same, but the knowledge they represent is different. The two functions validate two separate things that just happen to have the same rules. That\\'s a coincidence, not a duplication.\\n\\n## **Duplication in Documentation**\\n\\nSomehow the myth was born that you should comment all your functions. Those who believe in this insanity then produce something such as this:\\n\\n```\\n# Calculate the fees for this account.\\n#\\n# * Each returned check costs $20\\n# * If the account is in overdraft for more than 3 days,\\n# charge $10 for each day\\n# * If the average account balance is greater that $2,000\\n# reduce the fees by 50%\\ndef fees(a)\\n f = 0\\n if a.returned_check_count > 0\\n f += 20 * a.returned_check_count\\n end\\n```\\n\\n```\\n if a.overdraft_days > 3\\n f += 10*a.overdraft_days\\n end\\n if a.average_balance > 2_000\\n f /= 2\\n end\\n f\\nend\\n```\\nThe intent of this function is given twice: once in the comment and again in the code. The customer changes a fee, and we have to update both. Given time, we can pretty much guarantee the comment and the code will get out of step.\\n\\nAsk yourself what the comment adds to the code. From our point of view, it simply compensates for some bad naming and layout. How about just this:\\n\\n```\\ndef calculate_account_fees(account)\\n fees = 20 * account.returned_check_count\\n fees += 10 * account.overdraft_days if account.overdraft_days > 3\\n fees /= 2 if account.average_balance > 2_000\\n fees\\nend\\n```\\nThe name says what it does, and if someone needs details, they\\'re laid out in the source. That\\'s DRY!\\n\\n### **DRY Violations in Data**\\n\\nOur data structures represent knowledge, and they can fall afoul of the DRY principle. Let\\'s look at a class representing a line:\\n\\n```\\nclass Line {\\n Point start;\\n Point end;\\n double length;\\n};\\n```\\nAt first sight, this class might appear reasonable. A line clearly has a start and end, and will always have a length (even if it\\'s zero). But we have\\n\\nduplication. The length is defined by the start and end points: change one of the points and the length changes. It\\'s better to make the length a calculated field:\\n\\n```\\nclass Line {\\n Point start;\\n Point end;\\n double length() { return start.distanceTo(end); }\\n};\\n```\\nLater on in the development process, you may choose to violate the DRY principle for performance reasons. Frequently this occurs when you need to cache data to avoid repeating expensive operations. The trick is to localize the impact. The violation is not exposed to the outside world: only the methods within the class have to worry about keeping things straight:\\n\\n```\\nclass Line {\\n private double length;\\n private Point start;\\n private Point end;\\n public Line(Point start, Point end) {\\n this.start = start;\\n this.end = end;\\n calculateLength();\\n }\\n // public\\n void setStart(Point p) { this.start = p; calculateLength(); }\\n void setEnd(Point p) { this.end = p; calculateLength(); }\\n Point getStart() { return start; }\\n Point getEnd() { return end; }\\n double getLength() { return length; }\\n private void calculateLength() {\\n this.length = start.distanceTo(end);\\n }\\n};\\n```\\nThis example also illustrates an important issue: whenever a module exposes a data structure, you\\'re coupling all the code that uses that structure to the implementation of that module. Where possible, always use accessor functions to read and write the attributes of objects. It will make it easier to add functionality in the future.\\n\\nThis use of accessor functions ties in with Meyer\\'s *Uniform Access principle*, described in *Object-Oriented Software Construction* [Mey97], which states that\\n\\nAll services offered by a module should be available through a uniform notation, which does not betray whether they are implemented through storage or through computation.\\n\\n## **Representational Duplication**\\n\\nYour code interfaces to the outside world: other libraries via APIs, other services via remote calls, data in external sources, and so on. And pretty much each time you do, you introduce some kind of DRY violation: your code has to have knowledge that is also present in the external *thing*. It needs to know the API, or the schema, or the meaning of error codes, or whatever. The duplication here is that two things (your code and the external entity) have to have knowledge of the representation of their interface. Change it at one end, and the other end breaks.\\n\\nThis duplication is inevitable, but can be mitigated. Here are some strategies.\\n\\n### **Duplication Across Internal APIs**\\n\\nFor internal APIs, look for tools that let you specify the API in some kind of neutral format. These tools will typically generate documentation, mock APIs, functional tests, and API clients, the latter in a number of different languages. Ideally the tool will store all your APIs in a central repository, allowing them to be shared across teams.\\n\\n## **Duplication Across External APIs**\\n\\nIncreasingly, you\\'ll find that public APIs are documented formally using something like OpenAPI.[15] This allows you to import the API spec into your local API tools and integrate more reliably with the service.\\n\\nIf you can\\'t find such a specification, consider creating one and publishing it. Not only will others find it useful; you may even get help maintaining it.\\n\\n## **Duplication with Data Sources**\\n\\nMany data sources allow you to introspect on their data schema. This can be used to remove much of the duplication between them and your code. Rather than manually creating the code to contain this stored data, you can generate the containers directly from the schema. Many persistence frameworks will do this heavy lifting for you.\\n\\nThere\\'s another option, and one we often prefer. Rather than writing code that represents external data in a fixed structure (an instance of a struct or class, for example), just stick it into a key/value data structure (your language might call it a map, hash, dictionary, or even object).\\n\\nOn its own this is risky: you lose a lot of the security of knowing just what data you\\'re working with. So we recommend adding a second layer to this solution: a simple table-driven validation suite that verifies that the map you\\'ve created contains at least the data you need, in the format you need it. Your API documentation tool might be able to generate this.\\n\\n## **Interdeveloper Duplication**\\n\\nPerhaps the hardest type of duplication to detect and handle occurs between different developers on a project. Entire sets of functionality may be inadvertently duplicated, and that duplication could go undetected for years, leading to maintenance problems. We heard firsthand of a U.S. state whose governmental computer systems were surveyed for Y2K compliance. The\\n\\naudit turned up more than 10,000 programs that each contained a different version of Social Security Number validation code.\\n\\nAt a high level, deal with the problem by building a strong, tight-knit team with good communications.\\n\\nHowever, at the module level, the problem is more insidious. Commonly needed functionality or data that doesn\\'t fall into an obvious area of responsibility can get implemented many times over.\\n\\nWe feel that the best way to deal with this is to encourage active and frequent communication between developers.\\n\\nMaybe run a daily scrum standup meeting. Set up forums (such as Slack channels) to discuss common problems. This provides a nonintrusive way of communicating—even across multiple sites—while retaining a permanent history of everything said.\\n\\nAppoint a team member as the project librarian, whose job is to facilitate the exchange of knowledge. Have a central place in the source tree where utility routines and scripts can be deposited. And make a point of reading other people\\'s source code and documentation, either informally or during code reviews. You\\'re not snooping—you\\'re learning from them. And remember, the access is reciprocal—don\\'t get twisted about other people poring (pawing?) through *your* code, either.\\n\\n## **Tip 16** Make It Easy to Reuse\\n\\nWhat you\\'re trying to do is foster an environment where it\\'s easier to find and reuse existing stuff than to write it yourself. *If it isn\\'t easy, people won\\'t do it.* And if you fail to reuse, you risk duplicating knowledge.\\n\\n## **Related Sections Include**\\n\\n- Topic 8, *The Essence of Good Design*\\n- Topic 28, *Decoupling*\\n- Topic 32, *Configuration*\\n- Topic 38, *Programming by Coincidence*\\n- Topic 40, *Refactoring*\\n\\n**Topic 10** Orthogonality\\n\\nOrthogonality is a critical concept if you want to produce systems that are easy to design, build, test, and extend. However, the concept of orthogonality is rarely taught directly. Often it is an implicit feature of various other methods and techniques you learn. This is a mistake. Once you learn to apply the principle of orthogonality directly, you\\'ll notice an immediate improvement in the quality of systems you produce.\\n\\n## **What Is Orthogonality?**\\n\\n\"Orthogonality\\'\\' is a term borrowed from geometry. Two lines are orthogonal if they meet at right angles, such as the axes on a graph. In vector terms, the two lines are *independent*. As the number 1 on the diagram moves north, it doesn\\'t change\\n\\n![](_page_83_Figure_4.jpeg)\\n\\nhow far east or west it is. The number 2 moves east, but not north or south.\\n\\nIn computing, the term has come to signify a kind of independence or decoupling. Two or more things are orthogonal if changes in one do not affect any of the others. In a well-designed system, the database code will be orthogonal to the user interface: you can change the interface without affecting the database, and swap databases without changing the interface.\\n\\nBefore we look at the benefits of orthogonal systems, let\\'s first look at a system that isn\\'t orthogonal.\\n\\n## **A Nonorthogonal System**\\n\\nYou\\'re on a helicopter tour of the Grand Canyon when the pilot, who made the obvious mistake of eating fish for lunch, suddenly groans and faints. Fortunately, he left you hovering 100 feet above the ground.\\n\\nAs luck would have it, you had read a Wikipedia page about helicopters the previous night. You know that helicopters have four basic controls. The *cyclic* is the stick you hold in your right hand. Move it, and the helicopter moves in the corresponding direction. Your left hand holds the *collective pitch lever*. Pull up on this and you increase the pitch on all the blades, generating lift. At the end of the pitch lever is the *throttle*. Finally you have two *foot pedals*, which vary the amount of tail rotor thrust and so help turn the helicopter.\\n\\n\"Easy!,\" you think. \"Gently lower the collective pitch lever and you\\'ll descend gracefully to the ground, a hero.\" However, when you try it, you discover that life isn\\'t that simple. The helicopter\\'s nose drops, and you start to spiral down to the left. Suddenly you discover that you\\'re flying a system where every control input has secondary effects. Lower the lefthand lever and you need to add compensating backward movement to the right-hand stick and push the right pedal. But then each of these changes affects all of the other controls again. Suddenly you\\'re juggling an unbelievably complex system, where every change impacts all the other inputs. Your workload is phenomenal: your hands and feet are constantly moving, trying to balance all the interacting forces.\\n\\nHelicopter controls are decidedly not orthogonal.\\n\\n## **Benefits of Orthogonality**\\n\\nAs the helicopter example illustrates, nonorthogonal systems are inherently more complex to change and control. When components of any system are highly interdependent, there is no such thing as a local fix.\\n\\n## **Tip 17** Eliminate Effects Between Unrelated Things\\n\\nWe want to design components that are self-contained: independent, and with a single, well-defined purpose (what Yourdon and Constantine call\\n\\n*cohesion* in *Structured Design: Fundamentals of a Discipline of Computer Program and Systems Design* [YC79]). When components are isolated from one another, you know that you can change one without having to worry about the rest. As long as you don\\'t change that component\\'s external interfaces, you can be confident that you won\\'t cause problems that ripple through the entire system.\\n\\nYou get two major benefits if you write orthogonal systems: increased productivity and reduced risk.\\n\\n## **Gain Productivity**\\n\\n- Changes are localized, so development time and testing time are reduced. It is easier to write relatively small, self-contained components than a single large block of code. Simple components can be designed, coded, tested, and then forgotten—there is no need to keep changing existing code as you add new code.\\n- An orthogonal approach also promotes reuse. If components have specific, well-defined responsibilities, they can be combined with new components in ways that were not envisioned by their original implementors. The more loosely coupled your systems, the easier they are to reconfigure and reengineer.\\n- There is a fairly subtle gain in productivity when you combine orthogonal components. Assume that one component does distinct things and another does things. If they are orthogonal and you combine them, the result does things. However, if the two components are not orthogonal, there will be overlap, and the result will do less. You get more functionality per unit effort by combining orthogonal components.\\n\\n## **Reduce Risk**\\n\\nAn orthogonal approach reduces the risks inherent in any development.\\n\\n- Diseased sections of code are isolated. If a module is sick, it is less likely to spread the symptoms around the rest of the system. It is also easier to slice it out and transplant in something new and healthy.\\n- The resulting system is less fragile. Make small changes and fixes to a particular area, and any problems you generate will be restricted to that area.\\n- An orthogonal system will probably be better tested, because it will be easier to design and run tests on its components.\\n- You will not be as tightly tied to a particular vendor, product, or platform, because the interfaces to these third-party components will be isolated to smaller parts of the overall development.\\n\\nLet\\'s look at some of the ways you can apply the principle of orthogonality to your work.\\n\\n## **Design**\\n\\nMost developers are familiar with the need to design orthogonal systems, although they may use words such as *modular*, *component-based*, and *layered* to describe the process. Systems should be composed of a set of cooperating modules, each of which implements functionality independent of the others. Sometimes these components are organized into layers, each providing a level of abstraction. This layered approach is a powerful way to design orthogonal systems. Because each layer uses only the abstractions provided by the layers below it, you have great flexibility in changing underlying implementations without affecting code. Layering also reduces the risk of runaway dependencies between modules. You\\'ll often see layering expressed in diagrams:\\n\\n![](_page_87_Figure_0.jpeg)\\n\\nThere is an easy test for orthogonal design. Once you have your components mapped out, ask yourself: *If I dramatically change the requirements behind a particular function, how many modules are affected?* In an orthogonal system, the answer should be \"one.\\'\\' [16] Moving a button on a GUI panel should not require a change in the database schema. Adding context-sensitive help should not change the billing subsystem.\\n\\nLet\\'s consider a complex system for monitoring and controlling a heating plant. The original requirement called for a graphical user interface, but the requirements were changed to add a mobile interface that lets engineers monitor key values. In an orthogonally designed system, you would need to change only those modules associated with the user interface to handle this: the underlying logic of controlling the plant would remain unchanged. In fact, if you structure your system carefully, you should be able to support both interfaces with the same underlying code base.\\n\\nAlso ask yourself how decoupled your design is from changes in the real world. Are you using a telephone number as a customer identifier? What happens when the phone company reassigns area codes? Postal codes, Social Security Numbers or government IDs, email addresses, and domains are all external identifiers that you have no control over, and could change at any time for any reason. *Don\\'t rely on the properties of things you can\\'t control.*\\n\\n## **Toolkits and Libraries**\\n\\nBe careful to preserve the orthogonality of your system as you introduce third-party toolkits and libraries. Choose your technologies wisely.\\n\\nWhen you bring in a toolkit (or even a library from other members of your team), ask yourself whether it imposes changes on your code that shouldn\\'t be there. If an object persistence scheme is transparent, then it\\'s orthogonal. If it requires you to create or access objects in a special way, then it\\'s not. Keeping such details isolated from your code has the added benefit of making it easier to change vendors in the future.\\n\\nThe Enterprise Java Beans (EJB) system is an interesting example of orthogonality. In most transaction-oriented systems, the application code has to delineate the start and end of each transaction. With EJB, this information is expressed declaratively as annotations, outside the methods that do the work. The same application code can run in different EJB transaction environments with no change.\\n\\nIn a way, EJB is an example of the Decorator Pattern: adding functionality to things without changing them. This style of programming can be used in just about every programming language, and doesn\\'t necessarily require a framework or library. It just takes a little discipline when programming.\\n\\n## **Coding**\\n\\nEvery time you write code you run the risk of reducing the orthogonality of your application. Unless you constantly monitor not just what you are doing but also the larger context of the application, you might unintentionally duplicate functionality in some other module, or express existing knowledge twice.\\n\\nThere are several techniques you can use to maintain orthogonality:\\n\\n*Keep your code decoupled*\\n\\nWrite shy code—modules that don\\'t reveal anything unnecessary to other modules and that don\\'t rely on other modules\\' implementations. Try the Law of Demeter, which we discuss in Topic 28, *Decoupling*. If you need to change an object\\'s state, get the object to do it for you. This way your code remains isolated from the other code\\'s implementation and increases the chances that you\\'ll remain orthogonal.\\n\\n#### *Avoid global data*\\n\\nEvery time your code references global data, it ties itself into the other components that share that data. Even globals that you intend only to read can lead to trouble (for example, if you suddenly need to change your code to be multithreaded). In general, your code is easier to understand and maintain if you explicitly pass any required context into your modules. In object-oriented applications, context is often passed as parameters to objects\\' constructors. In other code, you can create structures containing the context and pass around references to them.\\n\\nThe Singleton pattern in *Design Patterns: Elements of Reusable Object-Oriented Software* [GHJV95] is a way of ensuring that there is only one instance of an object of a particular class. Many people use these singleton objects as a kind of global variable (particularly in languages, such as Java, that otherwise do not support the concept of globals). Be careful with singletons—they can also lead to unnecessary linkage.\\n\\n#### *Avoid similar functions*\\n\\nOften you\\'ll come across a set of functions that all look similar maybe they share common code at the start and end, but each has a different central algorithm. Duplicate code is a symptom of structural problems. Have a look at the Strategy pattern in *Design Patterns* for a better implementation.\\n\\nGet into the habit of being constantly critical of your code. Look for any opportunities to reorganize it to improve its structure and orthogonality. This process is called *refactoring*, and it\\'s so important that we\\'ve dedicated a section to it (see Topic 40, *Refactoring*).\\n\\n## **Testing**\\n\\nAn orthogonally designed and implemented system is easier to test. Because the interactions between the system\\'s components are formalized and limited, more of the system testing can be performed at the individual module level. This is good news, because module level (or unit) testing is considerably easier to specify and perform than integration testing. In fact, we suggest that these tests be performed automatically as part of the regular build process (see Topic 41, *Test to Code*).\\n\\nWriting unit tests is itself an interesting test of orthogonality. What does it take to get a unit test to build and run? Do you have to import a large percentage of the rest of the system\\'s code? If so, you\\'ve found a module that is not well decoupled from the rest of the system.\\n\\nBug fixing is also a good time to assess the orthogonality of the system as a whole. When you come across a problem, assess how localized the fix is. Do you change just one module, or are the changes scattered throughout the entire system? When you make a change, does it fix everything, or do other problems mysteriously arise? This is a good opportunity to bring automation to bear. If you use a version control system (and you will after reading Topic 19, *Version Control*), tag bug fixes when you check the code back in after testing. You can then run monthly reports analyzing trends in the number of source files affected by each bug fix.\\n\\n## **Documentation**\\n\\nPerhaps surprisingly, orthogonality also applies to documentation. The axes are content and presentation. With truly orthogonal documentation, you should be able to change the appearance dramatically without changing the\\n\\ncontent. Word processors provide style sheets and macros that help. We personally prefer using a markup system such as Markdown: when writing we focus only on the content, and leave the presentation to whichever tool we use to render it.[17]\\n\\n## **Living with Orthogonality**\\n\\nOrthogonality is closely related to the DRY principle. With DRY, you\\'re looking to minimize duplication within a system, whereas with orthogonality you reduce the interdependency among the system\\'s components. It may be a clumsy word, but if you use the principle of orthogonality, combined closely with the DRY principle, you\\'ll find that the systems you develop are more flexible, more understandable, and easier to debug, test, and maintain.\\n\\nIf you\\'re brought into a project where people are desperately struggling to make changes, and where every change seems to cause four other things to go wrong, remember the nightmare with the helicopter. The project probably is not orthogonally designed and coded. It\\'s time to refactor.\\n\\nAnd, if you\\'re a helicopter pilot, don\\'t eat the fish….\\n\\n## **Related Sections Include**\\n\\n- Topic 3, *Software Entropy*\\n- Topic 8, *The Essence of Good Design*\\n- Topic 11, *Reversibility*\\n- Topic 28, *Decoupling*\\n- Topic 31, *Inheritance Tax*\\n- Topic 33, *Breaking Temporal Coupling*\\n- Topic 34, *Shared State Is Incorrect State*\\n- Topic 36, *Blackboards*\\n\\n## **Challenges**\\n\\n- Consider the difference between tools which have a graphical user interface and small but combinable command-line utilities used at shell prompts. Which set is more orthogonal, and why? Which is easier to use for exactly the purpose for which it was intended? Which set is easier to combine with other tools to meet new challenges? Which set is easier to learn?\\n- C++ supports multiple inheritance, and Java allows a class to implement multiple interfaces. Ruby has mixins. What impact does using these facilities have on orthogonality? Is there a difference in impact between using multiple inheritance and multiple interfaces? Is there a difference between using delegation and using inheritance?\\n\\n## **Exercises**\\n\\n### **Exercise 1** (possible answer)\\n\\nYou\\'re asked to read a file a line at a time. For each line, you have to split it into fields. Which of the following sets of pseudo class definitions is likely to be more orthogonal?\\n\\n```\\nclass Split1 {\\n     constructor(fileName) # opens the file for reading\\n     def readNextLine() # moves to the next line\\n     def getField(n) # returns nth field in current line\\n    }\\nor\\n    class Split2 {\\n     constructor(line) # splits a line\\n     def getField(n) # returns nth field in current line\\n    }\\n```\\n\\n```\\nExercise 2 (possible answer)\\n```\\nWhat are the differences in orthogonality between object-oriented and functional languages? Are these differences inherent in the languages\\n\\nthemselves, or just in the way people use them?\\n\\n## **Topic 11** Reversibility\\n\\n*Nothing is more dangerous than an idea if it\\'s the only one you have.*\\n\\n*Emil-Auguste Chartier (Alain), Propos sur la religion, 1938* Engineers prefer simple, singular solutions to problems. Math tests that allow you to proclaim with great confidence that are much more comfortable than fuzzy, warm essays about the myriad causes of the French\\n\\nRevolution. Management tends to agree with the engineers: singular, easy answers fit nicely on spreadsheets and project plans.\\n\\nIf only the real world would cooperate! Unfortunately, while is today, it may need to be tomorrow, and next week. Nothing is forever—and if you rely heavily on some fact, you can almost guarantee that it *will* change.\\n\\nThere is always more than one way to implement something, and there is usually more than one vendor available to provide a third-party product. If you go into a project hampered by the myopic notion that there is only *one* way to do it, you may be in for an unpleasant surprise. Many project teams have their eyes forcibly opened as the future unfolds:\\n\\n\"But you said we\\'d use database XYZ! We are 85% done coding the project, we can\\'t change now!\" the programmer protested. \"Sorry, but our company decided to standardize on database PDQ instead—for all projects. It\\'s out of my hands. We\\'ll just have to recode. All of you will be working weekends until further notice.\"\\n\\nChanges don\\'t have to be that Draconian, or even that immediate. But as time goes by, and your project progresses, you may find yourself stuck in an untenable position. With every critical decision, the project team commits to a smaller target—a narrower version of reality that has fewer options.\\n\\nBy the time many critical decisions have been made, the target becomes so small that if it moves, or the wind changes direction, or a butterfly in Tokyo flaps its wings, you miss.[18] And you may miss by a huge amount.\\n\\nThe problem is that critical decisions aren\\'t easily reversible.\\n\\nOnce you decide to use this vendor\\'s database, or that architectural pattern, or a certain deployment model, you are committed to a course of action that cannot be undone, except at great expense.\\n\\n## **Reversibility**\\n\\nMany of the topics in this book are geared to producing flexible, adaptable software. By sticking to their recommendations—especially the DRY principle, decoupling, and use of external configuration—we don\\'t have to make as many critical, irreversible decisions. This is a good thing, because we don\\'t always make the best decisions the first time around. We commit to a certain technology only to discover we can\\'t hire enough people with the necessary skills. We lock in a certain third-party vendor just before they get bought out by their competitor. Requirements, users, and hardware change faster than we can get the software developed.\\n\\nSuppose you decide, early in the project, to use a relational database from vendor A. Much later, during performance testing, you discover that the database is simply too slow, but that the document database from vendor B is faster. With most conventional projects, you\\'d be out of luck. Most of the time, calls to third-party products are entangled throughout the code. But if you *really* abstracted the idea of a database out—to the point where it simply provides persistence as a service—then you have the flexibility to change horses in midstream.\\n\\nSimilarly, suppose the project begins as a browser-based application, but then, late in the game, marketing decides that what they really want is a mobile app. How hard would that be for you? In an ideal world, it shouldn\\'t impact you too much, at least on the server side. You\\'d be stripping out some HTML rendering and replacing it with an API.\\n\\nThe mistake lies in assuming that any decision is cast in stone—and in not preparing for the contingencies that might arise. Instead of carving decisions in stone, think of them more as being written in the sand at the beach. A big wave can come along and wipe them out at any time.\\n\\n- **Tip 18** There Are No Final Decisions\\n## **Flexible Architecture**\\n\\nWhile many people try to keep their *code* flexible, you also need to think about maintaining flexibility in the areas of architecture, deployment, and vendor integration.\\n\\nWe\\'re writing this in 2019. Since the turn of the century we\\'ve seen the following \"best practice\" server-side architectures:\\n\\n- Big hunk of iron\\n- Federations of big iron\\n- Load-balanced clusters of commodity hardware\\n- Cloud-based virtual machines running applications\\n- Cloud-based virtual machines running services\\n- Containerized versions of the above\\n- Cloud-supported serverless applications\\n- And, inevitably, an apparent move back to big hunks of iron for some tasks\\n\\nGo ahead and add the very latest and greatest fads to this list, and then regard it with awe: it\\'s a miracle that anything ever worked.\\n\\nHow can you plan for this kind of architectural volatility? You can\\'t.\\n\\nWhat you can do is make it easy to change. Hide third-party APIs behind your own abstraction layers. Break your code into components: even if you end up deploying them on a single massive server, this approach is a lot easier than taking a monolithic application and splitting it. (We have the scars to prove it.)\\n\\nAnd, although this isn\\'t particularly a reversibility issue, one final piece of advice.\\n\\n## **Tip 19** Forgo Following Fads\\n\\nNo one knows what the future may hold, especially not us! So enable your code to rock-n-roll: to \"rock on\\'\\' when it can, to roll with the punches when it must.\\n\\n## **Related Sections Include**\\n\\n- Topic 8, *The Essence of Good Design*\\n- Topic 10, *Orthogonality*\\n- Topic 19, *Version Control*\\n- Topic 28, *Decoupling*\\n- Topic 45, *The Requirements Pit*\\n- Topic 51, *Pragmatic Starter Kit*\\n\\n## **Challenges**\\n\\n- Time for a little quantum mechanics with Schrödinger\\'s cat.\\nSuppose you have a cat in a closed box, along with a radioactive particle. The particle has exactly a 50% chance of fissioning into two. If it does, the cat will be killed. If it doesn\\'t, the cat will be okay. So, is the cat dead or alive? According to Schrödinger, the correct answer is *both* (at least while the box remains closed). Every time a subnuclear reaction takes place that has two possible outcomes, the universe is\\n\\ncloned. In one, the event occurred, in the other it didn\\'t. The cat\\'s alive in one universe, dead in another. Only when you open the box do you know which universe *you* are in.\\n\\nNo wonder coding for the future is difficult.\\n\\nBut think of code evolution along the same lines as a box full of Schrödinger\\'s cats: every decision results in a different version of the future. How many possible futures can your code support? Which ones are more likely? How hard will it be to support them when the time comes?\\n\\nDare you open the box?\\n\\n**Topic 12** Tracer Bullets\\n\\n|\\n|  |\\n\\nWe often talk about hitting targets when we develop software. We\\'re not actually firing anything at the shooting range, but it\\'s still a useful and very visual metaphor. In particular, it\\'s\\n\\ninteresting to consider *how* to hit a target in a complex and shifting world.\\n\\nThe answer, of course, depends on the nature of the device you\\'re aiming with. With many you only get one chance to aim, and then get to see if you hit the bullseye or not. But there\\'s a better way.\\n\\nYou know all those movies, TV shows, and video games where people are shooting machine guns? In these scenes, you\\'ll often see the path of bullets as bright streaks in the air. These streaks come from tracer bullets.\\n\\nTracer bullets are loaded at intervals alongside regular ammunition. When they\\'re fired, their phosphorus ignites and leaves a pyrotechnic trail from the gun to whatever they hit. If the tracers are hitting the target, then so are the regular bullets. Soldiers use these tracer rounds to refine their aim: it\\'s pragmatic, real-time feedback under actual conditions.\\n\\nThat same principle applies to projects, particularly when you\\'re building something that hasn\\'t been built before. We use the term *tracer bullet development* to visually illustrate the need for immediate feedback under actual conditions with a moving goal.\\n\\nLike the gunners, you\\'re trying to hit a target in the dark. Because your users have never seen a system like this before, their requirements may be vague. Because you may be using algorithms, techniques, languages, or libraries you aren\\'t familiar with, you face a large number of unknowns.\\n\\nAnd because projects take time to complete, you can pretty much guarantee the environment you\\'re working in will change before you\\'re done.\\n\\nThe classic response is to specify the system to death. Produce reams of paper itemizing every requirement, tying down every unknown, and constraining the environment. Fire the gun using dead reckoning. One big calculation up front, then shoot and hope.\\n\\nPragmatic Programmers, however, tend to prefer using the software equivalent of tracer bullets.\\n\\n## **Code That Glows in the Dark**\\n\\nTracer bullets work because they operate in the same environment and under the same constraints as the real bullets. They get to the target fast, so the gunner gets immediate feedback. And from a practical standpoint they\\'re a relatively cheap solution.\\n\\nTo get the same effect in code, we look for something that gets us from a requirement to some aspect of the final system quickly, visibly, and repeatably.\\n\\nLook for the important requirements, the ones that define the system. Look for the areas where you have doubts, and where you see the biggest risks. Then prioritize your development so that these are the first areas you code.\\n\\n## **Tip 20** Use Tracer Bullets to Find the Target\\n\\nIn fact, given the complexity of today\\'s project setup, with swarms of external dependencies and tools, tracer bullets become even more important. For us, the very first tracer bullet is simply *create the project, add a \"hello world!,\" and make sure it compiles and runs.* Then we look for areas of uncertainty in the overall application and add the skeleton needed to make it work.\\n\\nHave a look at the following diagram. This system has five architectural layers. We have some concerns about how they\\'d integrate, so we look for a simple feature that lets us exercise them together. The diagonal line shows the path that feature takes through the code. To make it work, we just have to implement the solidly shaded areas in each layer: the stuff with the squiggles will be done later.\\n\\n![](_page_101_Figure_1.jpeg)\\n\\n![](_page_101_Figure_2.jpeg)\\n\\nWe once undertook a complex client-server database marketing project. Part of its requirement was the ability to specify and execute temporal queries. The servers were a range of relational and specialized databases. The client UI, written in random language A, used a set of libraries written in a different language to provide an interface to the servers. The user\\'s query was stored on the server in a Lisp-like notation before being converted to optimized SQL just prior to execution. There were many unknowns and many different environments, and no one was too sure how the UI should behave.\\n\\nThis was a great opportunity to use tracer code. We developed the framework for the front end, libraries for representing the queries, and a structure for converting a stored query into a database-specific query. Then we put it all together and checked that it worked. For that initial build, all we could do was submit a query that listed all the rows in a table, but it\\n\\nproved that the UI could talk to the libraries, the libraries could serialize and unserialize a query, and the server could generate SQL from the result. Over the following months we gradually fleshed out this basic structure, adding new functionality by augmenting each component of the tracer code in parallel. When the UI added a new query type, the library grew and the SQL generation was made more sophisticated.\\n\\nTracer code is not disposable: you write it for keeps. It contains all the error checking, structuring, documentation, and self-checking that any piece of production code has. It simply is not fully functional. However, once you have achieved an end-to-end connection among the components of your system, you can check how close to the target you are, adjusting if necessary. Once you\\'re on target, adding functionality is easy.\\n\\nTracer development is consistent with the idea that a project is never finished: there will always be changes required and functions to add. It is an incremental approach.\\n\\nThe conventional alternative is a kind of heavy engineering approach: code is divided into modules, which are coded in a vacuum. Modules are combined into subassemblies, which are then further combined, until one day you have a complete application. Only then can the application as a whole be presented to the user and tested.\\n\\nThe tracer code approach has many advantages:\\n\\n#### *Users get to see something working early*\\n\\nIf you have successfully communicated what you are doing (see Topic 52, *Delight Your Users*), your users will know they are seeing something immature. They won\\'t be disappointed by a lack of functionality; they\\'ll be ecstatic to see some visible progress toward their system. They also get to contribute as the project progresses, increasing their buy-in. These same users will likely be the people who\\'ll tell you how close to the target each iteration is.\\n\\n#### *Developers build a structure to work in*\\n\\nThe most daunting piece of paper is the one with nothing written on it. If you have worked out all the end-to-end interactions of your application, and have embodied them in code, then your team won\\'t need to pull as much out of thin air. This makes everyone more productive, and encourages consistency.\\n\\n#### *You have an integration platform*\\n\\nAs the system is connected end-to-end, you have an environment to which you can add new pieces of code once they have been unit-tested. Rather than attempting a big-bang integration, you\\'ll be integrating every day (often many times a day). The impact of each new change is more apparent, and the interactions are more limited, so debugging and testing are faster and more accurate.\\n\\n#### *You have something to demonstrate*\\n\\nProject sponsors and top brass have a tendency to want to see demos at the most inconvenient times. With tracer code, you\\'ll always have something to show them.\\n\\n### *You have a better feel for progress*\\n\\nIn a tracer code development, developers tackle use cases one by one. When one is done, they move to the next. It is far easier to measure performance and to demonstrate progress to your user. Because each individual development is smaller, you avoid creating those monolithic blocks of code that are reported as 95% complete week after week.\\n\\n## **Tracer Bullets Don\\'t Always Hit Their Target**\\n\\nTracer bullets show what you\\'re hitting. This may not always be the target. You then adjust your aim until they\\'re on target. That\\'s the point.\\n\\nIt\\'s the same with tracer code. You use the technique in situations where you\\'re not 100% certain of where you\\'re going. You shouldn\\'t be surprised if your first couple of attempts miss: the user says \"that\\'s not what I\\n\\nmeant,\\'\\' or data you need isn\\'t available when you need it, or performance problems seem likely. So change what you\\'ve got to bring it nearer the target, and be thankful that you\\'ve used a lean development methodology; a small body of code has low inertia—it is easy and quick to change. You\\'ll be able to gather feedback on your application and generate a new, more accurate version quickly and cheaply. And because every major application component is represented in your tracer code, your users can be confident that what they\\'re seeing is based on reality, not just a paper specification.\\n\\n## **Tracer Code versus Prototyping**\\n\\nYou might think that this tracer code concept is nothing more than prototyping under an aggressive name. There is a difference. With a prototype, you\\'re aiming to explore specific aspects of the final system. With a true prototype, you will throw away whatever you lashed together when trying out the concept, and recode it properly using the lessons you\\'ve learned.\\n\\nFor example, say you\\'re producing an application that helps shippers determine how to pack odd-sized boxes into containers. Among other problems, the user interface needs to be intuitive and the algorithms you use to determine optimal packing are very complex.\\n\\nYou could prototype a user interface for your end users in a UI tool. You code only enough to make the interface responsive to user actions. Once they\\'ve agreed to the layout, you might throw it away and recode it, this time with the business logic behind it, using the target language. Similarly, you might want to prototype a number of algorithms that perform the actual packing. You might code functional tests in a high-level, forgiving language such as Python, and code low-level performance tests in something closer to the machine. In any case, once you\\'d made your decision, you\\'d start again and code the algorithms in their final environment, interfacing to the real world. This is *prototyping*, and it is very useful.\\n\\nThe tracer code approach addresses a different problem. You need to know how the application as a whole hangs together. You want to show your users how the interactions will work in practice, and you want to give your developers an architectural skeleton on which to hang code. In this case, you might construct a tracer consisting of a trivial implementation of the container packing algorithm (maybe something like first-come, first-served) and a simple but working user interface. Once you have all the components in the application plumbed together, you have a framework to show your users and your developers. Over time, you add to this framework with new functionality, completing stubbed routines. But the framework stays intact, and you know the system will continue to behave the way it did when your first tracer code was completed.\\n\\nThe distinction is important enough to warrant repeating. Prototyping generates disposable code. Tracer code is lean but complete, and forms part of the skeleton of the final system. Think of prototyping as the reconnaissance and intelligence gathering that takes place before a single tracer bullet is fired.\\n\\n## **Related Sections Include**\\n\\n- Topic 13, *Prototypes and Post-it Notes*\\n- Topic 27, *Don\\'t Outrun Your Headlights*\\n- Topic 40, *Refactoring*\\n- Topic 49, *Pragmatic Teams*\\n- Topic 50, *Coconuts Don\\'t Cut It*\\n- Topic 51, *Pragmatic Starter Kit*\\n- Topic 52, *Delight Your Users*\\n\\n**Topic 13** Prototypes and Post-it Notes\\n\\nMany industries use prototypes to try out specific ideas; prototyping is much cheaper than full-scale production. Car makers, for example, may build many different prototypes of a new car design. Each one is designed to test a specific aspect of the car—the aerodynamics, styling, structural characteristics, and so on. Old school folks might use a clay model for wind tunnel testing, maybe a balsa wood and duct tape model will do for the art department, and so on. The less romantic will do their modeling on a computer screen or in virtual reality, reducing costs even further. In this way, risky or uncertain elements can be tried out without committing to building the real item.\\n\\nWe build software prototypes in the same fashion, and for the same reasons —to analyze and expose risk, and to offer chances for correction at a greatly reduced cost. Like the car makers, we can target a prototype to test one or more specific aspects of a project.\\n\\nWe tend to think of prototypes as code-based, but they don\\'t always have to be. Like the car makers, we can build prototypes out of different materials. Post-it notes are great for prototyping dynamic things such as workflow and application logic. A user interface can be prototyped as a drawing on a whiteboard, as a nonfunctional mock-up drawn with a paint program, or with an interface builder.\\n\\nPrototypes are designed to answer just a few questions, so they are much cheaper and faster to develop than applications that go into production. The code can ignore unimportant details—unimportant to you at the moment, but probably very important to the user later on. If you are prototyping a UI, for instance, you can get away with incorrect results or data. On the other\\n\\nhand, if you\\'re just investigating computational or performance aspects, you can get away with a pretty poor UI, or perhaps even no UI at all.\\n\\nBut if you find yourself in an environment where you *cannot* give up the details, then you need to ask yourself if you are really building a prototype at all. Perhaps a tracer bullet style of development would be more appropriate in this case (see Topic 12, *Tracer Bullets*).\\n\\n## **Things to Prototype**\\n\\nWhat sorts of things might you choose to investigate with a prototype? Anything that carries risk. Anything that hasn\\'t been tried before, or that is absolutely critical to the final system. Anything unproven, experimental, or doubtful. Anything you aren\\'t comfortable with. You can prototype:\\n\\n- Architecture\\n- New functionality in an existing system\\n- Structure or contents of external data\\n- Third-party tools or components\\n- Performance issues\\n- User interface design\\n\\nPrototyping is a learning experience. Its value lies not in the code produced, but in the lessons learned. That\\'s really the point of prototyping.\\n\\n**Tip 21** Prototype to Learn\\n\\n## **How to Use Prototypes**\\n\\nWhen building a prototype, what details can you ignore?\\n\\n*Correctness*\\n\\nYou may be able to use dummy data where appropriate.\\n\\n## *Completeness*\\n\\nThe prototype may function only in a very limited sense, perhaps with only one preselected piece of input data and one menu item.\\n\\n*Robustness*\\n\\nError checking is likely to be incomplete or missing entirely. If you stray from the predefined path, the prototype may crash and burn in a glorious display of pyrotechnics. That\\'s okay.\\n\\n*Style*\\n\\nPrototype code shouldn\\'t have much in the way of comments or documentation (although you may produce reams of documentation as a result of your experience with the prototype).\\n\\nPrototypes gloss over details, and focus in on specific aspects of the system being considered, so you may want to implement them using a high-level scripting language—higher than the rest of the project (maybe a language such as Python or Ruby), as these languages can get out of your way. You may choose to continue to develop in the language used for the prototype, or you can switch; after all, you\\'re going to throw the prototype away anyway.\\n\\nTo prototype user interfaces, use a tool that lets you focus on the appearance and/or interactions without worrying about code or markup.\\n\\nScripting languages also work well as the \"glue\\'\\' to combine low-level pieces into new combinations. Using this approach, you can rapidly assemble existing components into new configurations to see how things work.\\n\\n## **Prototyping Architecture**\\n\\nMany prototypes are constructed to model the entire system under consideration. As opposed to tracer bullets, none of the individual modules in the prototype system need to be particularly functional. In fact, you may not even need to code in order to prototype architecture—you can prototype on a whiteboard, with Post-it notes or index cards. What you are looking for is how the system hangs together as a whole, again deferring details. Here are some specific areas you may want to look for in the architectural prototype:\\n\\n- Are the responsibilities of the major areas well defined and appropriate?\\n- Are the collaborations between major components well defined?\\n- Is coupling minimized?\\n- Can you identify potential sources of duplication?\\n- Are interface definitions and constraints acceptable?\\n- Does every module have an access path to the data it needs during execution? Does it have that access *when* it needs it?\\n\\nThis last item tends to generate the most surprises and the most valuable results from the prototyping experience.\\n\\n## **How** *Not* **to Use Prototypes**\\n\\nBefore you embark on any code-based prototyping, make sure that everyone understands that you are writing disposable code. Prototypes can be deceptively attractive to people who don\\'t know that they are just prototypes. You must make it *very* clear that this code is disposable, incomplete, and unable to be completed.\\n\\nIt\\'s easy to become misled by the apparent completeness of a demonstrated prototype, and project sponsors or management may insist on deploying the prototype (or its progeny) if you don\\'t set the right expectations. Remind them that you can build a great prototype of a new car out of balsa wood and duct tape, but you wouldn\\'t try to drive it in rush-hour traffic!\\n\\nIf you feel there is a strong possibility in your environment or culture that the purpose of prototype code may be misinterpreted, you may be better off with the tracer bullet approach. You\\'ll end up with a solid framework on which to base future development.\\n\\nProperly used prototypes can save you huge amounts of time, money, and pain by identifying and correcting potential problem spots early in the development cycle—the time when fixing mistakes is both cheap and easy.\\n\\n## **Related Sections Include**\\n\\n- Topic 12, *Tracer Bullets*\\n- Topic 14, *Domain Languages*\\n- Topic 17, *Shell Games*\\n- Topic 27, *Don\\'t Outrun Your Headlights*\\n- Topic 37, *Listen to Your Lizard Brain*\\n- Topic 45, *The Requirements Pit*\\n- Topic 52, *Delight Your Users*\\n\\n## **Exercises**\\n\\n#### **Exercise 3** (possible answer)\\n\\nMarketing would like to sit down and brainstorm a few web page designs with you. They are thinking of clickable image maps to take you to other pages, and so on. But they can\\'t decide on a model for the image—maybe it\\'s a car, or a phone, or a house. You have a list of target pages and content; they\\'d like to see a few prototypes. Oh, by the way, you have 15 minutes. What tools might you use?\\n\\n### **Topic 14** Domain Languages\\n\\n*The limits of language are the limits of one\\'s world.*\\n\\n*Ludwig Wittgenstein*\\n\\nComputer languages influence *how* you think about a problem, and how you think about communicating. Every language comes with a list of features: buzzwords such as static versus dynamic typing, early versus late\\n\\nbinding, functional versus OO, inheritance models, mixins, macros—all of which may suggest or obscure certain solutions. Designing a solution with C++ in mind will produce different results than a solution based on Haskellstyle thinking, and vice versa. Conversely, and we think more importantly, the language of the problem domain may also suggest a programming solution.\\n\\nWe always try to write code using the vocabulary of the application domain (see *Maintain a Glossary*). In some cases, Pragmatic Programmers can go to the next level and actually program using the vocabulary, syntax, and semantics—the language—of the domain.\\n\\n**Tip 22** Program Close to the Problem Domain\\n\\n## **Some Real-World Domain Languages**\\n\\nLet\\'s look at a few examples where folks have done just that.\\n\\n### **RSpec**\\n\\nRSpec[19] is a testing library for Ruby. It inspired versions for most other modern languages. A test in RSpec is intended to reflect the behavior you expect from your code.\\n\\n```\\ndescribe BowlingScore do\\n it \"totals 12 if you score 3 four times\" do\\n score = BowlingScore.new\\n 4.times { score.add_pins(3) }\\n expect(score.total).to eq(12)\\n end\\nend\\n```\\n## **Cucumber**\\n\\nCucumber[20] is programming-language neutral way of specifying tests. You run the tests using a version of Cucumber appropriate to the language you\\'re using. In order to support the natural-language like syntax, you also have to write specific matchers that recognize phrases and extract parameters for the tests.\\n\\n```\\nFeature: Scoring\\nBackground:\\n Given an empty scorecard\\nScenario: bowling a lot of 3s\\n Given I throw a 3\\n And I throw a 3\\n And I throw a 3\\n And I throw a 3\\n Then the score should be 12\\n```\\nCucumber tests were intended to be read by the customers of the software (although that happens fairly rarely in practice; the following aside considers why that might be).\\n\\n#### **Why Don\\'t Many Business Users Read Cucumber Features?**\\n\\nOne of the reasons that the classic *gather requirements, design, code, ship* approach doesn\\'t work is that it is anchored by the concept that we know what the requirements are. But we rarely do. Your business users will have a vague idea of what they want to achieve, but they neither know nor care about the details. That\\'s part of our value: we intuit intent and convert it to code.\\n\\nSo when you force a business person to sign off on a requirements document, or get them to agree to a set of Cucumber features, you\\'re doing the equivalent of getting them to check the spelling in an essay written in Sumerian. They\\'ll make some random changes to save face and sign it off to get you out of their office.\\n\\nGive them code that runs, however, and they can play with it. That\\'s where their real needs will surface.\\n\\n## **Phoenix Routes**\\n\\nMany web frameworks have a routing facility, mapping incoming HTTP requests onto handler functions in the code. Here\\'s an example from Phoenix.[21]\\n\\n```\\nscope \"/\", HelloPhoenix do\\n pipe_through :browser # Use the default browser stack\\n get \"/\", PageController, :index\\n resources \"/users\", UserController\\nend\\n```\\nThis says that requests starting \"/\" will be run through a series of filters appropriate for browsers. A request to \"/\" itself will be handled by the index function in the PageController module. The UsersController implements the functions needed to manage a resource accessible via the url /users.\\n\\n### **Ansible**\\n\\nAnsible[22] is a tool that configures software, typically on a bunch of remote servers. It does this by reading a specification that you provide, then doing whatever is needed on the servers to make them mirror that spec. The\\n\\nspecification can be written in YAML,[23] a language that builds data structures from text descriptions:\\n\\n```\\n---\\n- name: install nginx\\n apt: name=nginx state=latest\\n- name: ensure nginx is running (and enable it at boot)\\n service: name=nginx state=started enabled=yes\\n- name: write the nginx config file\\n template: src=templates/nginx.conf.j2 dest=/etc/nginx/nginx.conf\\n notify:\\n - restart nginx\\n```\\nThis example ensures that the latest version of nginx is installed on my servers, that it is started by default, and that it uses a configuration file that you\\'ve provided.\\n\\n## **Characteristics of Domain Languages**\\n\\nLet\\'s look at these examples more closely.\\n\\nRSpec and the Phoenix router are written in their host languages (Ruby and Elixir). They employ some fairly devious code, including metaprogramming and macros, but ultimately they are compiled and run as regular code.\\n\\nCucumber tests and Ansible configurations are written in their own languages. A Cucumber test is converted into code to be run or into a datastructure, whereas Ansible specs are always converted into a data structure that is run by Ansible itself.\\n\\nAs a result, RSpec and the router code are embedded into the code you run: they are true extensions to your code\\'s vocabulary. Cucumber and Ansible are *read* by code and converted into some form the code can use.\\n\\nWe call RSpec and the router examples of *internal* domain languages, while Cucumber and Ansible use *external* languages.\\n\\n## **Trade-Offs Between Internal and External Languages**\\n\\nIn general, an internal domain language can take advantage of the features of its host language: the domain language you create is more powerful, and that power comes for free. For example, you could use some Ruby code to create a bunch of RSpec tests automatically. In this case we can test scores where there are no spares or strikes:\\n\\n```\\ndescribe BowlingScore do\\n (0..4).each do |pins|\\n (1..20).each do |throws|\\n target = pins * throws\\n it \"totals #{target } if you score #{pins } #{throws } times\" do\\n score = BowlingScore.new\\n throws.times { score.add_pins(pins) }\\n expect(score.total).to eq(target)\\n end\\n end\\n end\\nend\\n```\\nThat\\'s 100 tests you just wrote. Take the rest of the day off.\\n\\nThe downside of internal domain languages is that you\\'re bound by the syntax and semantics of that language. Although some languages are remarkably flexible in this regards, you\\'re still forced to compromise between the language you want and the language you can implement.\\n\\nUltimately, whatever you come up with must still be valid syntax in your target language. Languages with macros (such as Elixir, Clojure, and Crystal) gives you a little more flexibility, but ultimately syntax is syntax.\\n\\nExternal languages have no such restrictions. As long as you can write a parser for the language, you\\'re good to go. Sometimes you can use\\n\\nsomeone else\\'s parser (as Ansible did by using YAML), but then you\\'re back to making a compromise.\\n\\nWriting a parser probably means adding new libraries and possibly tools to your application. And writing a good parser is not a trivial job. But, if you\\'re feeling stout of heart, you could look at parser generators such as bison or ANTLR, and parsing frameworks such as the many PEG parsers out there.\\n\\nOur suggestion is fairly simple: don\\'t spend more effort than you save. Writing a domain language adds some cost to your project, and you\\'ll need to be convinced that there are offsetting savings (potentially in the long term).\\n\\nIn general, use off-the-shelf external languages (such as YAML, JSON, or CSV) if you can. If not, look at internal languages. We\\'d recommend using external languages only in cases where your language will be written by the users of your application.\\n\\n## **An Internal Domain Language on the Cheap**\\n\\nFinally, there\\'s a cheat for creating internal domain languages if you don\\'t mind the host language syntax leaking through. Don\\'t do a bunch of metaprogramming. Instead, just write functions to do the work. In fact, this is pretty much what RSpec does:\\n\\n```\\ndescribe BowlingScore do\\n it \"totals 12 if you score 3 four times\" do\\n score = BowlingScore.new\\n 4.times { score.add_pins(3) }\\n expect(score.total).to eq(12)\\n end\\nend\\n```\\nIn this code, describe, it, expect, to, and eq are just Ruby methods. There\\'s a little plumbing behind the scenes in terms of how objects are passed around, but it\\'s all just code. We\\'ll explore that a little in the exercises.\\n\\n## **Related Sections Include**\\n\\n- Topic 8, *The Essence of Good Design*\\n- Topic 13, *Prototypes and Post-it Notes*\\n- Topic 32, *Configuration*\\n\\n## **Challenges**\\n\\n- Could some of the requirements of your current project be expressed in a domain-specific language? Would it be possible to write a compiler or translator that could generate most of the code required?\\n- If you decide to adopt mini-languages as a way of programming closer to the problem domain, you\\'re accepting that some effort will be required to implement them. Can you see ways in which the framework you develop for one project can be reused in others?\\n\\n## **Exercises**\\n\\n## **Exercise 4** (possible answer)\\n\\nWe want to implement a mini-language to control a simple turtle-graphics system. The language consists of single-letter commands, some followed by a single number. For example, the following input would draw a rectangle:\\n\\n```\\nP 2 # select pen 2\\nD # pen down\\nW 2 # draw west 2cm\\nN 1 # then north 1\\nE 2 # then east 2\\nS 1 # then back south\\nU # pen up\\n```\\nImplement the code that parses this language. It should be designed so that it is simple to add new commands.\\n\\n## **Exercise 5** (possible answer)\\n\\nIn the previous exercise we implemented a parser for the drawing language —it was an external domain language. Now implement it again as an internal language. Don\\'t do anything clever: just write a function for each of the commands. You may have to change the names of the commands to lower case, and maybe to wrap them inside something to provide some context.\\n\\n## **Exercise 6** (possible answer)\\n\\nDesign a BNF grammar to parse a time specification. All of the following examples should be accepted:\\n\\n```\\n4pm, 7:38pm, 23:42, 3:16, 3:16am\\n```\\n**Exercise 7** (possible answer)\\n\\nImplement a parser for the BNF grammar in the previous exercise using a PEG parser generator in the language of your choice. The output should be an integer containing the number of minutes past midnight.\\n\\n### **Exercise 8** (possible answer)\\n\\nImplement the time parser using a scripting language and regular expressions.\\n\\n**Topic 15** Estimating\\n\\nThe Library of Congress in Washington, DC, currently has about 75 terabytes of digital information online. Quick! How long will it take to send all that information over a 1Gbps network? How much storage will you need for a million names and addresses? How long does it take to compress 100Mb of text? How many months will it take to deliver your project?\\n\\nAt one level, these are all meaningless questions—they are all missing information. And yet they can all be answered, as long as you are comfortable estimating. And, in the process of producing an estimate, you\\'ll come to understand more about the world your programs inhabit.\\n\\nBy learning to estimate, and by developing this skill to the point where you have an intuitive feel for the magnitudes of things, you will be able to show an apparent magical ability to determine their feasibility. When someone says \"we\\'ll send the backup over a network connection to S3,\" you\\'ll be able to know intuitively whether this is practical. When you\\'re coding, you\\'ll be able to know which subsystems need optimizing and which ones can be left alone.\\n\\n## **Tip 23** Estimate to Avoid Surprises\\n\\nAs a bonus, at the end of this section we\\'ll reveal the single correct answer to give whenever anyone asks you for an estimate.\\n\\n## **How Accurate Is Accurate Enough?**\\n\\nTo some extent, all answers are estimates. It\\'s just that some are more accurate than others. So the first question you have to ask yourself when someone asks you for an estimate is the context in which your answer will be taken. Do they need high accuracy, or are they looking for a ballpark figure?\\n\\nOne of the interesting things about estimating is that the units you use make a difference in the interpretation of the result. If you say that something will take about 130 working days, then people will be expecting it to come in pretty close. However, if you say \"Oh, about six months,\" then they know to look for it any time between five and seven months from now. Both numbers represent the same duration, but \"130 days\" probably implies a higher degree of accuracy than you feel. We recommend that you scale time estimates as follows:\\n\\n| Duration | Quote estimate in |\\n| --- | --- |\\n| 1–15 days | Days |\\n| 3–6 weeks | Weeks |\\n| 8–20 weeks | Months |\\n| 20+ weeks | Think hard before giving an estimate |\\n\\nSo, if after doing all the necessary work, you decide that a project will take 125 working days (25 weeks), you might want to deliver an estimate of \"about six months.\"\\n\\nThe same concepts apply to estimates of any quantity: choose the units of your answer to reflect the accuracy you intend to convey.\\n\\n## **Where Do Estimates Come From?**\\n\\nAll estimates are based on models of the problem. But before we get too deeply into the techniques of building models, we have to mention a basic estimating trick that always gives good answers: ask someone who\\'s already done it. Before you get too committed to model building, cast around for someone who\\'s been in a similar situation in the past. See how their problem got solved. It\\'s unlikely you\\'ll ever find an exact match, but\\n\\nyou\\'d be surprised how many times you can successfully draw on others\\' experiences.\\n\\n#### **Understand What\\'s Being Asked**\\n\\nThe first part of any estimation exercise is building an understanding of what\\'s being asked. As well as the accuracy issues discussed above, you need to have a grasp of the scope of the domain. Often this is implicit in the question, but you need to make it a habit to think about the scope before starting to guess. Often, the scope you choose will form part of the answer you give: \"Assuming there are no traffic accidents and there\\'s gas in the car, I should be there in 20 minutes.\"\\n\\n#### **Build a Model of the System**\\n\\nThis is the fun part of estimating. From your understanding of the question being asked, build a rough-and-ready bare-bones mental model. If you\\'re estimating response times, your model may involve a server and some kind of arriving traffic. For a project, the model may be the steps that your organization uses during development, along with a very rough picture of how the system might be implemented.\\n\\nModel building can be both creative and useful in the long term. Often, the process of building the model leads to discoveries of underlying patterns and processes that weren\\'t apparent on the surface. You may even want to reexamine the original question: \"You asked for an estimate to do *X*. However, it looks like *Y*, a variant of *X*, could be done in about half the time, and you lose only one feature.\"\\n\\nBuilding the model introduces inaccuracies into the estimating process. This is inevitable, and also beneficial. You are trading off model simplicity for accuracy. Doubling the effort on the model may give you only a slight increase in accuracy. Your experience will tell you when to stop refining.\\n\\n### **Break the Model into Components**\\n\\nOnce you have a model, you can decompose it into components. You\\'ll need to discover the mathematical rules that describe how these components interact. Sometimes a component contributes a single value that is added into the result. Some components may supply multiplying factors, while others may be more complicated (such as those that simulate the arrival of traffic at a node).\\n\\nYou\\'ll find that each component will typically have parameters that affect how it contributes to the overall model. At this stage, simply identify each parameter.\\n\\n## **Give Each Parameter a Value**\\n\\nOnce you have the parameters broken out, you can go through and assign each one a value. You expect to introduce some errors in this step. The trick is to work out which parameters have the most impact on the result, and concentrate on getting them about right. Typically, parameters whose values are added into a result are less significant than those that are multiplied or divided. Doubling a line speed may double the amount of data received in an hour, while adding a 5ms transit delay will have no noticeable effect.\\n\\nYou should have a justifiable way of calculating these critical parameters. For the queuing example, you might want to measure the actual transaction arrival rate of the existing system, or find a similar system to measure. Similarly, you could measure the current time taken to serve a request, or come up with an estimate using the techniques described in this section. In fact, you\\'ll often find yourself basing an estimate on other subestimates. This is where your largest errors will creep in.\\n\\n### **Calculate the Answers**\\n\\nOnly in the simplest of cases will an estimate have a single answer. You might be happy to say \"I can walk five cross-town blocks in 15 minutes.\" However, as the systems get more complex, you\\'ll want to hedge your answers. Run multiple calculations, varying the values of the critical\\n\\nparameters, until you work out which ones really drive the model. A spreadsheet can be a big help. Then couch your answer in terms of these parameters. \"The response time is roughly three quarters of a second if the system has SSDs and 32GB of memory, and one second with 16GB memory.\" (Notice how \"three quarters of a second\" conveys a different feeling of accuracy than 750ms.)\\n\\nDuring the calculation phase, you get answers that seem strange. Don\\'t be too quick to dismiss them. If your arithmetic is correct, your understanding of the problem or your model is probably wrong. This is valuable information.\\n\\n## **Keep Track of Your Estimating Prowess**\\n\\nWe think it\\'s a great idea to record your estimates so you can see how close you were. If an overall estimate involved calculating subestimates, keep track of these as well. Often you\\'ll find your estimates are pretty good—in fact, after a while, you\\'ll come to expect this.\\n\\nWhen an estimate turns out wrong, don\\'t just shrug and walk away—find out why. Maybe you chose some parameters that didn\\'t match the reality of the problem. Maybe your model was wrong. Whatever the reason, take some time to uncover what happened. If you do, your next estimate will be better.\\n\\n## **Estimating Project Schedules**\\n\\nNormally you\\'ll be asked to estimate how long something will take. If that \"something\" is complex, the estimate can be very difficult to produce. In this section, we\\'ll look at two techniques for reducing that uncertainty.\\n\\n## **Painting the Missile**\\n\\n*\"How long will it take to paint the house?\"*\\n\\n*\"Well, if everything goes right, and this paint has the coverage they claim, it might be as few as 10 hours. But that\\'s unlikely: I\\'d guess a more realistic figure is closer to 18 hours. And, of course, if the weather turns bad, that could push it out to 30 or more.\"*\\n\\nThat\\'s how people estimate in the real world. Not with a single number (unless you force them to give you one) but with a range of scenarios.\\n\\nWhen the U.S. Navy needed to plan the Polaris submarine project, they adopted this style of estimating with a methodology they called the *Program Evaluation Review Technique*, or **PERT**.\\n\\nEvery PERT task has an *optimistic*, a *most likely*, and a *pessimistic estimate*. The tasks are arranged into a dependency network, and then you use some simple statistics to identify likely best and worst times for the overall project.\\n\\nUsing a range of values like this is a great way to avoid one of the most common causes of estimation error: padding a number because you\\'re unsure. Instead, the statistics behind PERT spreads the uncertainty out for you, giving you better estimations of the whole project.\\n\\nHowever, we\\'re not big fans of this. People tend to produce wall-sized charts of all the tasks in a project, and implicitly believe that, just because they used a *formula*, they have an accurate estimate. The chances are they don\\'t, because they have never done this before.\\n\\n### **Eating the Elephant**\\n\\nWe find that often the only way to determine the timetable for a project is by gaining experience on that same project. This needn\\'t be a paradox if you practice incremental development, repeating the following steps with very thin slices of functionality:\\n\\n- Check requirements\\n- Analyze risk (and prioritize riskiest items earlier)\\n- Design, implement, integrate\\n- Validate with the users\\n\\nInitially, you may have only a vague idea of how many iterations will be required, or how long they may be. Some methods require you to nail this down as part of the initial plan; however, for all but the most trivial of projects this is a mistake. Unless you are doing an application similar to a previous one, with the same team and the same technology, you\\'d just be guessing.\\n\\nSo you complete the coding and testing of the initial functionality and mark this as the end of the first iteration. Based on that experience, you can refine your initial guess on the number of iterations and what can be included in each. The refinement gets better and better each time, and confidence in the schedule grows along with it. This kind of estimating is often done during the team\\'s review at the end of each iterative cycle.\\n\\nThat\\'s also how the old joke says to eat an elephant: one bite at a time.\\n\\n**Tip 24** Iterate the Schedule with the Code\\n\\nThis may not be popular with management, who typically want a single, hard-and-fast number before the project even starts. You\\'ll have to help them understand that the team, their productivity, and the environment will determine the schedule. By formalizing this, and refining the schedule as part of each iteration, you\\'ll be giving them the most accurate scheduling estimates you can.\\n\\n## **What to Say When Asked for an Estimate**\\n\\nYou say *\"I\\'ll get back to you.\"*\\n\\nYou almost always get better results if you slow the process down and spend some time going through the steps we describe in this section. Estimates given at the coffee machine will (like the coffee) come back to haunt you.\\n\\n## **Related Sections Include**\\n\\n- Topic 7, *Communicate!*\\n- Topic 39, *Algorithm Speed*\\n\\n## **Challenges**\\n\\n- Start keeping a log of your estimates. For each, track how accurate you turned out to be. If your error was greater than 50%, try to find out where your estimate went wrong.\\n## **Exercises**\\n\\n### **Exercise 9** (possible answer)\\n\\nYou are asked \"Which has a higher bandwidth: a 1Gbps net connection or a person walking between two computers with a full 1TB of storage device in their pocket?\\'\\' What constraints will you put on your answer to ensure that the scope of your response is correct? (For example, you might say that the time taken to access the storage device is ignored.)\\n\\n## **Exercise 10** (possible answer)\\n\\nSo, which has the higher bandwidth?\\n\\n#### **Footnotes**\\n\\n- [13] To paraphrase the old Arlen/Mercer song…\\n- [14] Or, perhaps, to keep your sanity, every 10th time…\\n- [15] https://github.com/OAI/OpenAPI-Specification\\n- [16] In reality, this is naive. Unless you are remarkably lucky, most real-world requirements changes will affect multiple functions in the system. However, if you analyze the change in terms of functions, each functional change should still ideally affect just one module.\\n- [17] In fact, this book is written in Markdown, and typeset directly from the Markdown source.\\n- [18] Take a nonlinear, or chaotic, system and apply a small change to one of its inputs. You may get a large and often unpredictable result. The clichéd butterfly flapping its wings in Tokyo could be the start of a chain of events that ends up generating a tornado in Texas. Does this sound like any projects you know?\\n- [19] https://rspec.info\\n- [20] https://cucumber.io/\\n- [21] https://phoenixframework.org/\\n- [22] https://www.ansible.com/\\n- [23] https://yaml.org/\\n\\nCopyright © 2020 Pearson Education, Inc.\\n\\n# Chapter 3\\n\\n# **The Basic Tools**\\n\\nEvery maker starts their journey with a basic set of good-quality tools. A woodworker might need rules, gauges, a couple of saws, some good planes, fine chisels, drills and braces, mallets, and clamps. These tools will be lovingly chosen, will be built to last, will perform specific jobs with little overlap with other tools, and, perhaps most importantly, will feel right in the budding woodworker\\'s hands.\\n\\nThen begins a process of learning and adaptation. Each tool will have its own personality and quirks, and will need its own special handling. Each must be sharpened in a unique way, or held just so. Over time, each will wear according to use, until the grip looks like a mold of the woodworker\\'s hands and the cutting surface aligns perfectly with the angle at which the tool is held. At this point, the tools become conduits from the maker\\'s brain to the finished product—they have become extensions of their hands. Over time, the woodworker will add new tools, such as biscuit cutters, laserguided miter saws, dovetail jigs—all wonderful pieces of technology. But you can bet that they\\'ll be happiest with one of those original tools in hand, feeling the plane sing as it slides through the wood.\\n\\nTools amplify your talent. The better your tools, and the better you know how to use them, the more productive you can be. Start with a basic set of generally applicable tools. As you gain experience, and as you come across special requirements, you\\'ll add to this basic set. Like the maker, expect to\\n\\nadd to your toolbox regularly. Always be on the lookout for better ways of doing things. If you come across a situation where you feel your current tools can\\'t cut it, make a note to look for something different or more powerful that would have helped. Let need drive your acquisitions.\\n\\nMany new programmers make the mistake of adopting a single power tool, such as a particular integrated development environment (IDE), and never leave its cozy interface. This really is a mistake. You need to be comfortable beyond the limits imposed by an IDE. The only way to do this is to keep the basic tool set sharp and ready to use.\\n\\nIn this chapter we\\'ll talk about investing in your own basic toolbox. As with any good discussion on tools, we\\'ll start (in Topic 16, *The Power of Plain Text*) by looking at your raw materials, the stuff you\\'ll be shaping. From there we\\'ll move to the workbench, or in our case the computer. How can you use your computer to get the most out of the tools you use? We\\'ll discuss this in Topic 17, *Shell Games*. Now that we have material and a bench to work on, we\\'ll turn to the tool you\\'ll probably use more than any other, your editor. In Topic 18, *Power Editing*, we\\'ll suggest ways of making you more efficient.\\n\\nTo ensure that we never lose any of our precious work, we should always use a Topic 19, *Version Control* system—even for personal things such as recipes or notes. And, since Murphy was really an optimist after all, you can\\'t be a great programmer until you become highly skilled at Topic 20, *Debugging*.\\n\\nYou\\'ll need some glue to bind much of the magic together. We discuss some possibilities in Topic 21, *Text Manipulation*.\\n\\nFinally, the palest ink is still better than the best memory. Keep track of your thoughts and your history, as we describe in Topic 22, *Engineering Daybooks*.\\n\\nSpend time learning to use these tools, and at some point you\\'ll be surprised to discover your fingers moving over the keyboard, manipulating text without conscious thought. The tools will have become extensions of your hands.\\n\\nAs Pragmatic Programmers, our base material isn\\'t wood or iron, it\\'s knowledge. We gather requirements as knowledge, and then express that knowledge in our designs, implementations, tests, and documents. And we believe that the best format for storing knowledge persistently is *plain text*. With plain text, we give ourselves the ability to manipulate knowledge, both manually and programmatically, using virtually every tool at our disposal.\\n\\nThe problem with most binary formats is that the context necessary to understand the data is separate from the data itself. You are artificially divorcing the data from its meaning. The data may as well be encrypted; it is absolutely meaningless without the application logic to parse it. With plain text, however, you can achieve a self-describing data stream that is independent of the application that created it.\\n\\n## **What Is Plain Text?**\\n\\n*Plain text* is made up of printable characters in a form that conveys information. It can be as simple as a shopping list:\\n\\n- * milk * lettuce\\n- * coffee\\n\\nor as complex as the source of this book (yes, it\\'s in plain text, much to the chagrin of the publisher, who wanted us to use a word processor).\\n\\nThe information part is important. The following is not useful plain text:\\n\\nhlj;uijn bfjxrrctvh jkni\\'pio6p7gu;vh bjxrdi5rgvhj\\n\\nNeither is this:\\n\\nField19=467abe\\n\\nThe reader has no idea what the significance of 467abe may be. We like our plain text to be *understandable* to humans.\\n\\n## **Tip 25** Keep Knowledge in Plain Text\\n\\n## **The Power of Text**\\n\\nPlain text doesn\\'t mean that the text is unstructured; HTML, JSON, YAML, and so on are all plain text. So are the majority of the fundamental protocols on the net, such as HTTP, SMTP, IMAP, and so on. And that\\'s for some good reasons:\\n\\n- Insurance against obsolescence\\n- Leverage existing tools\\n- Easier testing\\n\\n### **Insurance Against Obsolescence**\\n\\nHuman-readable forms of data, and self-describing data, will outlive all other forms of data and the applications that created them. Period. As long as the data survives, you will have a chance to be able to use it—potentially long after the original application that wrote it is defunct.\\n\\nYou can parse such a file with only partial knowledge of its format; with most binary files, you must know all the details of the entire format in order to parse it successfully.\\n\\nConsider a data file from some legacy system that you are given.[24] You know little about the original application; all that\\'s important to you is that it maintained a list of clients\\' Social Security numbers, which you need to find and extract. Among the data, you see\\n\\n<FIELD10>123-45-6789</FIELD10>\\n\\n```\\n...\\n<FIELD10>567-89-0123</FIELD10>\\n...\\n<FIELD10>901-23-4567</FIELD10>\\n```\\nRecognizing the format of a Social Security number, you can quickly write a small program to extract that data—even if you have no information on anything else in the file.\\n\\nBut imagine if the file had been formatted this way instead:\\n\\n```\\nAC27123456789B11P\\n...\\nXY43567890123QTYL\\n...\\n6T2190123456788AM\\n```\\nYou may not have recognized the significance of the numbers quite as easily. This is the difference between *human readable* and *human understandable*.\\n\\nWhile we\\'re at it, FIELD10 doesn\\'t help much either. Something like\\n\\n<SOCIAL-SECURITY-NO>123-45-6789</SOCIAL-SECURITY-NO>\\n\\nmakes the exercise a no-brainer—and ensures that the data will outlive any project that created it.\\n\\n### **Leverage**\\n\\nVirtually every tool in the computing universe, from version control systems to editors to command-line tools, can operate on plain text.\\n\\n#### **The Unix Philosophy**\\n\\nUnix is famous for being designed around the philosophy of small, sharp tools, each intended to do one thing well. This philosophy is enabled by using a common underlying format—the line-oriented, plain-text file. Databases used for system administration (users and passwords, networking configuration, and so on) are all kept as plain-text files. (Some systems also maintain a binary form of certain databases as a performance optimization. The plain-text version is kept as an interface to the binary version.)\\n\\nWhen a system crashes, you may be faced with only a minimal environment to restore it (you may not be able to access graphics drivers, for instance). Situations such as this can really make you appreciate the simplicity of plain text.\\n\\nPlain text is also easier to search. If you can\\'t remember which configuration file manages your system backups, a quick grep -r backup /etc should tell you.\\n\\nFor instance, suppose you have a production deployment of a large application with a complex site-specific configuration file. If this file is in plain text, you could place it under a version control system (see Topic 19, *Version Control*), so that you automatically keep a history of all changes. File comparison tools such as diff and fc allow you to see at a glance what changes have been made, while sum allows you to generate a checksum to monitor the file for accidental (or malicious) modification.\\n\\n### **Easier Testing**\\n\\nIf you use plain text to create synthetic data to drive system tests, then it is a simple matter to add, update, or modify the test data *without having to create any special tools to do so.* Similarly, plain-text output from regression tests can be trivially analyzed with shell commands or a simple script.\\n\\n## **Lowest Common Denominator**\\n\\nEven in the future of blockchain-based intelligent agents that travel the wild and dangerous internet autonomously, negotiating data interchange among themselves, the ubiquitous text file will still be there. In fact, in\\n\\nheterogeneous environments the advantages of plain text can outweigh all of the drawbacks. You need to ensure that all parties can communicate using a common standard. Plain text is that standard.\\n\\n## **Related Sections Include**\\n\\n- Topic 17, *Shell Games*\\n- Topic 21, *Text Manipulation*\\n- Topic 32, *Configuration*\\n\\n## **Challenges**\\n\\n- Design a small address book database (name, phone number, and so on) using a straightforward binary representation in your language of choice. Do this before reading the rest of this challenge.\\n\\t- Translate that format into a plain-text format using XML or JSON.\\n\\t- For each version, add a new, variable-length field called *directions* in which you might enter directions to each person\\'s house.\\n\\nWhat issues come up regarding versioning and extensibility? Which form was easier to modify? What about converting existing data?\\n\\n**Topic 17** Shell Games\\n\\nEvery woodworker needs a good, solid, reliable workbench, somewhere to hold work pieces at a convenient height while they\\'re being shaped. The workbench becomes the center of the woodshop, the maker returning to it time and time again as a piece takes shape.\\n\\nFor a programmer manipulating files of text, that workbench is the command shell. From the shell prompt, you can invoke your full repertoire of tools, using pipes to combine them in ways never dreamt of by their original developers. From the shell, you can launch applications, debuggers, browsers, editors, and utilities. You can search for files, query the status of the system, and filter output. And by programming the shell, you can build complex macro commands for activities you perform often.\\n\\nFor programmers raised on GUI interfaces and integrated development environments (IDEs), this might seem an extreme position. After all, can\\'t you do everything equally well by pointing and clicking?\\n\\nThe simple answer is \"no.\\'\\' GUI interfaces are wonderful, and they can be faster and more convenient for some simple operations. Moving files, reading and writing email, and building and deploying your project are all things that you might want to do in a graphical environment. But if you do all your work using GUIs, you are missing out on the full capabilities of your environment. You won\\'t be able to automate common tasks, or use the full power of the tools available to you. And you won\\'t be able to combine your tools to create customized *macro tools*. A benefit of GUIs is **WYSIWYG**—what you see is what you get. The disadvantage is **WYSIAYG**—what you see is *all* you get.\\n\\nGUI environments are normally limited to the capabilities that their designers intended. If you need to go beyond the model the designer provided, you are usually out of luck—and more often than not, you *do* need to go beyond the model. Pragmatic Programmers don\\'t just cut code, or develop object models, or write documentation, or automate the build process—we do *all* of these things. The scope of any one tool is usually limited to the tasks that the tool is expected to perform. For instance, suppose you need to integrate a code preprocessor (to implement design-bycontract, or multi-processing pragmas, or some such) into your IDE. Unless the designer of the IDE explicitly provided hooks for this capability, you can\\'t do it.\\n\\n## **Tip 26** Use the Power of Command Shells\\n\\nGain familiarity with the shell, and you\\'ll find your productivity soaring. Need to create a list of all the unique package names explicitly imported by your Java code? The following stores it in a file called \"list\\'\\':\\n\\nsh/packages.sh\\n\\n```\\ngrep \\'^import \\' *.java |\\n sed -e \\'s/^import *//\\' -e \\'s/;.*$//\\' |\\n sort -u >list\\n```\\nIf you haven\\'t spent much time exploring the capabilities of the command shell on the systems you use, this might appear daunting. However, invest some energy in becoming familiar with your shell and things will soon start falling into place. Play around with your command shell, and you\\'ll be surprised at how much more productive it makes you.\\n\\n## **A Shell of Your Own**\\n\\nIn the same way that a woodworker will customize their workspace, a developer should customize their shell. This typically also involves changing the configuration of the terminal program you use.\\n\\nCommon changes include:\\n\\n- *Setting color themes.* Many, many hours can be spent trying out *every single* theme that\\'s available online for your particular shell.\\n- *Configuring a prompt*. The prompt that tells you the shell is ready for you to type a command can be configured to display just about any information you might want (and a bunch of stuff you\\'d never want). Personal preferences are everything here: we tend to like simple prompts, with a shortened current directory name and version control status along with the time.\\n- *Aliases and shell functions*. Simplify your workflow by turning commands you use a lot into simple aliases. Maybe you regularly update your Linux box, but can never remember whether you update and upgrade, or upgrade and update. Create an alias:\\n\\nalias apt-up= *\\'sudo apt-get update && sudo apt-get upgrade\\'*\\n\\nMaybe you\\'ve accidentally deleted files with the rm command just one time too often. Write an alias so that it will always prompt in future:\\n\\nalias rm = *\\'rm -iv\\'*\\n\\n- *Command completion*. Most shells will complete the names of commands and files: type the first few characters, hit tab, and it\\'ll fill in what it can. But you can take this a lot further, configuring the shell to recognize the command you\\'re entering and offer context-specific completions. Some even customize the completion depending on the current directory.\\nYou\\'ll spend a lot of time living in one of these shells. Be like a hermit crab and make it your own home.\\n\\n## **Related Sections Include**\\n\\n- Topic 13, *Prototypes and Post-it Notes*\\n- Topic 16, *The Power of Plain Text*\\n- Topic 21, *Text Manipulation*\\n- Topic 30, *Transforming Programming*\\n- Topic 51, *Pragmatic Starter Kit*\\n\\n## **Challenges**\\n\\n- Are there things that you\\'re currently doing manually in a GUI? Do you ever pass instructions to colleagues that involve a number of individual \"click this button,\" \"select this item\" steps? Could these be automated?\\n- Whenever you move to a new environment, make a point of finding out what shells are available. See if you can bring your current shell with you.\\n- Investigate alternatives to your current shell. If you come across a problem your shell can\\'t address, see if an alternative shell would cope better.\\n\\n**Topic 18** Power Editing\\n\\nWe\\'ve talked before about tools being an extension of your hand. Well, this applies to editors more than to any other software tool. You need to be able to manipulate text as effortlessly as possible, because text is the basic raw material of programming.\\n\\nIn the first edition of this book we recommended using a single editor for everything: code, documentation, memos, system administration, and so on. We\\'ve softened that position a little. We\\'re happy for you to use as many editors as you want. We\\'d just like you to be working toward fluency in each.\\n\\n## **Tip 27** Achieve Editor Fluency\\n\\nWhy is this a big deal? Are we saying you\\'ll save lots of time? Actually yes: over the course of a year, you might actually gain an additional week if you make your editing just 4% more efficient and you edit for 20 hours a week.\\n\\nBut that\\'s not the real benefit. No, the major gain is that by becoming fluent, you no longer have to think about the mechanics of editing. The distance between thinking something and having it appear in an editor buffer drop way down. Your thoughts will flow, and your programming will benefit. (If you\\'ve ever taught someone to drive, then you\\'ll understand the difference between someone who has to think about every action they take and a more experienced driver who controls the car instinctively.)\\n\\n## **What Does \"Fluent\" Mean?**\\n\\nWhat counts as being fluent? Here\\'s the challenge list:\\n\\n- When editing text, move and make selections by character, word, line, and paragraph.\\n- When editing code, move by various syntactic units (matching delimiters, functions, modules, …).\\n- Reindent code following changes.\\n- Comment and uncomment blocks of code with a single command.\\n- Undo and redo changes.\\n- Split the editor window into multiple panels, and navigate between them.\\n- Navigate to a particular line number.\\n- Sort selected lines.\\n- Search for both strings and regular expressions, and repeat previous searches.\\n- Temporarily create multiple cursors based on a selection or on a pattern match, and edit the text at each in parallel.\\n- Display compilation errors in the current project.\\n- Run the current project\\'s tests.\\n\\nCan you do all this without using a mouse/trackpad?\\n\\nYou might say that your current editor can\\'t do some of these things. Maybe it\\'s time to switch?\\n\\n## **Moving Toward Fluency**\\n\\nWe doubt there are more than a handful of people who know *all* the commands in any particular powerful editor. We don\\'t expect you to, either. Instead, we suggest a more pragmatic approach: learn the commands that make your life easier.\\n\\nThe recipe for this is fairly simple.\\n\\nFirst, look at yourself while you\\'re editing. Every time you find yourself doing something repetitive, get into the habit of thinking \"there must be a better way.\" Then find it.\\n\\nOnce you\\'ve discovered a new, useful feature, you now need to get it installed into your muscle memory, so you can use it without thinking. The only way we know to do that is through repetition. Consciously look for opportunities to use your new superpower, ideally many times a day. After a week or so, you\\'ll find you use it without thinking.\\n\\n## **Growing Your Editor**\\n\\nMost of the powerful code editors are built around a basic core that is then augmented through extensions. Many are supplied with the editor, and others can be added later.\\n\\nWhen you bump into some apparent limitation of the editor you\\'re using, search around for an extension that will do the job. The chances are that you are not alone in needing that capability, and if you\\'re lucky someone else will have published their solution.\\n\\nTake this a step further. Dig into your editor\\'s extension language. Work out how to use it to automate some of the repetitive things you do. Often you\\'ll just need a line or two of code.\\n\\nSometimes you might take it further still, and you\\'ll find yourself writing a full-blown extension. If so, publish it: if you had a need for it, other people will, too.\\n\\n## **Related Sections Include**\\n\\n- Topic 7, *Communicate!*\\n## **Challenges**\\n\\n- No more autorepeat.\\nEveryone does it: you need to delete the last word you typed, so you press down on backspace and wait for autorepeat to kick in. In fact, we bet that your brain has done this so much that you can judge pretty much exactly when to release the key.\\n\\nSo turn off autorepeat, and instead learn the key sequences to move, select, and delete by characters, words, lines, and blocks.\\n\\n- This one is going to hurt.\\nLose the mouse/trackpad. For one whole week, edit using just the keyboard. You\\'ll discover a bunch of stuff that you can\\'t do without pointing and clicking, so now\\'s the time to learn. Keep notes (we recommend going old-school and using pencil and paper) of the key sequences you learn.\\n\\nYou\\'ll take a productivity hit for a few days. But, as you learn to do stuff without moving your hands away from the home position, you\\'ll find that your editing becomes faster and more fluent than it ever was in the past.\\n\\n- Look for integrations. While writing this chapter, Dave wondered if he could preview the final layout (a PDF file) in an editor buffer. One download later, the layout is sitting alongside the original text, all in the editor. Keep a list of things you\\'d like to bring into your editor, then look for them.\\n- Somewhat more ambitiously, if you can\\'t find a plugin or extension that does what you want, write one. Andy is fond of making custom, local file-based Wiki plugins for his favorite editors. If you can\\'t find it, build it!\\n## **Topic 19** Version Control\\n\\n*Progress, far from consisting in change, depends on retentiveness. Those who cannot remember the past are condemned to repeat it.*\\n\\n*George Santayana, Life of Reason*\\n\\nOne of the important things we look for in a user interface is the undo key—a single button that forgives us our mistakes. It\\'s even better if the environment supports multiple levels of undo and redo, so you can go back and recover from something that happened\\n\\na couple of minutes ago.\\n\\nBut what if the mistake happened last week, and you\\'ve turned your computer on and off ten times since then? Well, that\\'s one of the many benefits of using a version control system (VCS): it\\'s a giant undo key—a project-wide time machine that can return you to those halcyon days of last week, when the code actually compiled and ran.\\n\\nFor many folks, that\\'s the limit of their VCS usage. Those folks are missing out on a whole bigger world of collaboration, deployment pipelines, issue tracking, and general team interaction.\\n\\nSo let\\'s take a look at VCS, first as a repository of changes, and then as a central meeting place for your team and their code.\\n\\n#### **Shared Directories Are NOT Version Control**\\n\\nWe still come across the occasional team who share their project source files across a network: either internally or using some kind of cloud storage.\\n\\nThis is not viable.\\n\\nTeams that do this are constantly messing up each other\\'s work, losing changes, breaking builds, and getting into fist fights in the car park. It\\'s like writing concurrent code with shared data and no synchronization mechanism. Use version control.\\n\\nBut there\\'s more! Some folks *do* use version control, and keep their main repository on a network or cloud drive. They reason that this is the best of both worlds: their files are accessible anywhere and (in the case of cloud storage) it\\'s backed up off-site.\\n\\nTurns out that this is even worse, and you risk losing everything. The version control software uses a set of interacting files and directories. If two instances simultaneously make changes, the overall state can become corrupted, and there\\'s no telling how much damage will be done. And no one likes seeing developers cry.\\n\\n## **It Starts at the Source**\\n\\nVersion control systems keep track of every change you make in your source code and documentation. With a properly configured source code control system, *you can always go back to a previous version of your software.*\\n\\nBut a version control system does far more than undo mistakes. A good VCS will let you track changes, answering questions such as: Who made changes in this line of code? What\\'s the difference between the current version and last week\\'s? How many lines of code did we change in this release? Which files get changed most often? This kind of information is invaluable for bug-tracking, audit, performance, and quality purposes.\\n\\nA VCS will also let you identify releases of your software. Once identified, you will always be able to go back and regenerate the release, independent of changes that may have occurred later.\\n\\nVersion control systems may keep the files they maintain in a central repository—a great candidate for archiving.\\n\\nFinally, version control systems allow two or more users to be working concurrently on the same set of files, even making concurrent changes in the same file. The system then manages the merging of these changes when the files are sent back to the repository. Although seemingly risky, such systems work well in practice on projects of all sizes.\\n\\n## **Tip 28** Always Use Version Control\\n\\nAlways. Even if you are a single-person team on a one-week project. Even if it\\'s a \"throw-away\\'\\' prototype. Even if the stuff you\\'re working on isn\\'t source code. Make sure that *everything* is under version control: documentation, phone number lists, memos to vendors, makefiles, build and release procedures, that little shell script that tidies up log files everything. We routinely use version control on just about everything we type (including the text of this book). Even if we\\'re not working on a project, our day-to-day work is secured in a repository.\\n\\n## **Branching Out**\\n\\nVersion control systems don\\'t just keep a single history of your project. One of their most powerful and useful features is the way they let you isolate islands of development into things called *branches*. You can create a branch at any point in your project\\'s history, and any work you do in that branch will be isolated from all other branches. At some time in the future you can *merge* the branch you\\'re working on back into another branch, so the target branch now contains the changes you made in your branch. Multiple people can even be working on a branch: in a way, branches are like little clone projects.\\n\\nOne benefit of branches is the isolation they give you. If you develop feature A in one branch, and a teammate works on feature B in another, you\\'re not going to interfere with each other.\\n\\nA second benefit, which may be surprising, is that branches are often at the heart of a team\\'s project workflow.\\n\\nAnd this is where things get a little confusing. Version control branches and test organization have something in common: they both have thousands of people out there telling you how you should do it. And that advice is largely meaningless, because what they\\'re really saying is \"this is what worked for me.\"\\n\\nSo use version control in your project, and if you bump into workflow issues, search for possible solutions. And remember to review and adjust what you\\'re doing as you gain experience.\\n\\n#### **A Thought Experiment**\\n\\nSpill an entire cup of tea (English breakfast, with a little milk) onto your laptop keyboard. Take the machine to the smart-person bar, and have them tut and frown. Buy a new computer. Take it home.\\n\\nHow long would it take to get that machine back to the same state it was in (with all the SSH keys, editor configuration, shell setup, installed applications, and so on) at the point where you first lifted that fateful cup? This was an issue one of us faced recently.\\n\\nJust about everything that defined the configuration and usage of the original machine was stored in version control, including:\\n\\n- All the user preferences and dotfiles\\n- The editor configuration\\n- The list of software installed using Homebrew\\n- The Ansible script used to configure apps\\n- All current projects\\n\\nThe machine was restored by the end of the afternoon.\\n\\n## **Version Control as a Project Hub**\\n\\nAlthough version control is incredibly useful on personal projects, it really comes into its own when working with a team. And much of this value comes from how you host your repository.\\n\\nNow, many version control systems don\\'t need any hosting. They are completely decentralized, with each developer cooperating on a peer-topeer basis. But even with these systems, it\\'s worth looking into having a central repository, because once you do, you can take advantage of a ton of integrations to make the project flow easier.\\n\\nMany of the repository systems are open source, so you can install and run them in your company. But that\\'s not really your line of business, so we\\'d recommend most people host with a third party. Look for features such as:\\n\\n- Good security and access control\\n- Intuitive UI\\n- The ability to do everything from the command line, too (because you may need to automate it)\\n- Automated builds and tests\\n- Good support for branch merging (sometimes called pull requests)\\n- Issue management (ideally integrated into commits and merges, so you can keep metrics)\\n- Good reporting (a Kanban board-like display of pending issues and tasks can be very useful)\\n- Good team communications: emails or other notifications on changes, a wiki, and so on\\n\\nMany teams have their VCS configured so that a push to a particular branch will automatically build the system, run the tests, and if successful deploy the new code into production.\\n\\nSound scary? Not when you realize you\\'re using version control. You can always roll it back.\\n\\n## **Related Sections Include**\\n\\n- Topic 11, *Reversibility*\\n- Topic 49, *Pragmatic Teams*\\n- Topic 51, *Pragmatic Starter Kit*\\n\\n## **Challenges**\\n\\n- Knowing you can roll back to any previous state using the VCS is one thing, but can you actually do it? Do you know the commands to do it properly? Learn them now, not when disaster strikes and you\\'re under pressure.\\n- Spend some time thinking about recovering your own laptop environment in case of a disaster. What would you need to recover? Many of the things you need are just text files. If they\\'re not in a VCS (hosted off your laptop), find a way to add them. Then think about the other stuff: installed applications, system configuration, and so on. How can you express all that stuff in text files so it, too, can be saved?\\n\\nAn interesting experiment, once you\\'ve made some progress, is to find an old computer you no longer use and see if your new system can be used to set it up.\\n\\n- Consciously explore the features of your current VCS and hosting provider that you\\'re not using. If your team isn\\'t using feature branches, experiment with introducing them. The same with pull/merge requests. Continuous integration. Build pipelines. Even continuous deployment. Look into the team communication tools, too: wikis, Kanban boards, and the like.\\nYou don\\'t have to use any of it. But you do need to know what it does so you can make that decision.\\n\\n- Use version control for nonproject things, too.\\n## **Topic 20** Debugging\\n\\n*It is a painful thing To look at your own trouble and know That you yourself and no one else has made it*\\n\\n*Sophocles, Ajax*\\n\\nThe word *bug* has been used to describe an \"object of terror\\'\\' ever since the fourteenth century. Rear Admiral Dr. Grace Hopper, the inventor of COBOL, is credited with observing the first *computer bug* literally, a moth caught in a relay in an\\n\\nearly computer system. When asked to explain why the machine wasn\\'t behaving as intended, a technician reported that there was \"a bug in the system,\" and dutifully taped it—wings and all—into the log book.\\n\\nRegrettably, we still have bugs in the system, albeit not the flying kind. But the fourteenth century meaning—a bogeyman—is perhaps even more applicable now than it was then. Software defects manifest themselves in a variety of ways, from misunderstood requirements to coding errors. Unfortunately, modern computer systems are still limited to doing what you *tell* them to do, not necessarily what you *want* them to do.\\n\\nNo one writes perfect software, so it\\'s a given that debugging will take up a major portion of your day. Let\\'s look at some of the issues involved in debugging and some general strategies for finding elusive bugs.\\n\\n## **Psychology of Debugging**\\n\\nDebugging is a sensitive, emotional subject for many developers. Instead of attacking it as a puzzle to be solved, you may encounter denial, finger pointing, lame excuses, or just plain apathy.\\n\\nEmbrace the fact that debugging is just *problem solving*, and attack it as such.\\n\\nHaving found someone else\\'s bug, you can spend time and energy laying blame on the filthy culprit who created it. In some workplaces this is part of the culture, and may be cathartic. However, in the technical arena, you want to concentrate on fixing the *problem*, not the blame.\\n\\n- \\n- **Tip 29** Fix the Problem, Not the Blame\\n\\nIt doesn\\'t really matter whether the bug is your fault or someone else\\'s. It is still your problem.\\n\\n## **A Debugging Mindset**\\n\\nBefore you start debugging, it\\'s important to adopt the right mindset. You need to turn off many of the defenses you use each day to protect your ego, tune out any project pressures you may be under, and get yourself comfortable. Above all, remember the first rule of debugging:\\n\\n**Tip 30** Don\\'t Panic\\n\\nIt\\'s easy to get into a panic, especially if you are facing a deadline, or have a nervous boss or client breathing down your neck while you are trying to find the cause of the bug. But it is very important to step back a pace, and actually *think* about what could be causing the symptoms that you believe indicate a bug.\\n\\nIf your first reaction on witnessing a bug or seeing a bug report is \"that\\'s impossible,\" you are plainly wrong. Don\\'t waste a single neuron on the train of thought that begins \"but that can\\'t happen\" because quite clearly it *can*, and has.\\n\\nBeware of myopia when debugging. Resist the urge to fix just the symptoms you see: it is more likely that the actual fault may be several steps removed from what you are observing, and may involve a number of other related things. Always try to discover the root cause of a problem, not just this particular appearance of it.\\n\\n## **Where to Start**\\n\\nBefore you *start* to look at the bug, make sure that you are working on code that built cleanly—without warnings. We routinely set compiler warning levels as high as possible. It doesn\\'t make sense to waste time trying to find a problem that the computer could find for you! We need to concentrate on the harder problems at hand.\\n\\nWhen trying to solve any problem, you need to gather all the relevant data. Unfortunately, bug reporting isn\\'t an exact science. It\\'s easy to be misled by coincidences, and you can\\'t afford to waste time debugging coincidences. You first need to be accurate in your observations.\\n\\nAccuracy in bug reports is further diminished when they come through a third party—you may actually need to *watch* the user who reported the bug in action to get a sufficient level of detail.\\n\\nAndy once worked on a large graphics application. Nearing release, the testers reported that the application crashed every time they painted a stroke with a particular brush. The programmer responsible argued that there was nothing wrong with it; he had tried painting with it, and it worked just fine. This dialog went back and forth for several days, with tempers rapidly rising.\\n\\nFinally, we got them together in the same room. The tester selected the brush tool and painted a stroke from the upper right corner to the lower left corner. The application exploded. \"Oh,\" said the programmer, in a small voice, who then sheepishly admitted that he had made test strokes only from the lower left to the upper right, which did not expose the bug.\\n\\nThere are two points to this story:\\n\\n- You may need to interview the user who reported the bug in order to gather more data than you were initially given.\\n- Artificial tests (such as the programmer\\'s single brush stroke from bottom to top) don\\'t exercise enough of an application. You must brutally test both boundary conditions and realistic end-user usage patterns. You need to do this systematically (see *Ruthless and Continuous Testing*).\\n\\n## **Debugging Strategies**\\n\\nOnce *you* think you know what is going on, it\\'s time to find out what the *program* thinks is going on.\\n\\n## **Reproducing Bugs**\\n\\nNo, our bugs aren\\'t really multiplying (although some of them are probably old enough to do it legally). We\\'re talking about a different kind of reproduction.\\n\\nThe best way to start fixing a bug is to make it reproducible. After all, if you can\\'t reproduce it, how will you know if it is ever fixed?\\n\\nBut we want more than a bug that can be reproduced by following some long series of steps; we want a bug that can be reproduced with a *single command*. It\\'s a lot harder to fix a bug if you have to go through 15 steps to get to the point where the bug shows up.\\n\\nSo here\\'s the most important rule of debugging:\\n\\n- **Tip 31** Failing Test Before Fixing Code\\nSometimes by forcing yourself to isolate the circumstances that display the bug, you\\'ll even gain an insight on how to fix it. The act of writing the test informs the solution.\\n\\n## **Coder in a Strange Land**\\n\\nAll this talk about isolating the bug is fine, when faced with 50,000 lines of code and a ticking clock, what\\'s a poor coder to do?\\n\\nFirst, look at the problem. Is it a crash? It\\'s always surprising when we teach courses that involve programming how many developers see an exception pop up in red and immediately tab across to the code.\\n\\n![](_page_155_Figure_3.jpeg)\\n\\n\\'nuf said.\\n\\n## **Bad Results**\\n\\nWhat if it\\'s not a crash? What if it\\'s just a bad result?\\n\\nGet in there with a debugger and use your failing test to trigger the problem.\\n\\nBefore anything else, make sure that you\\'re also seeing the incorrect value in the debugger. We\\'ve both wasted hours trying to track down a bug only to discover that this particular run of the code worked fine.\\n\\nSometimes the problem is obvious: interest_rate is 4.5 and should be 0.045. More often you have to look deeper to find out why the value is wrong in the first place. Make sure you know how to move up and down the call stack and examine the local stack environment.\\n\\nWe find it often helps to keep pen and paper nearby so we can jot down notes. In particular we often come across a clue and chase it down, only to find it didn\\'t pan out. If we didn\\'t jot down where we were when we started the chase, we could lose a lot of time getting back there.\\n\\nSometimes you\\'re looking at a stack trace that seems to scroll on forever. In this case, there\\'s often a quicker way to find the problem than examining\\n\\neach and every stack frame: use a *binary chop*. But before we discuss that, let\\'s look at two other common bug scenarios.\\n\\n#### **Sensitivity to Input Values**\\n\\nYou\\'ve been there. Your program works fine with all the test data, and survives its first week in production with honor. Then it suddenly crashes when fed a particular dataset.\\n\\nYou can try looking at the place it crashes and work backwards. But sometimes it\\'s easier to start with the data. Get a copy of the dataset and feed it through a locally running copy of the app, making sure it still crashes. Then binary chop the data until you isolate exactly which input values are leading to the crash.\\n\\n#### **Regressions Across Releases**\\n\\nYou\\'re on a good team, and you release your software into production. At some point a bug pops up in code that worked OK a week ago. Wouldn\\'t it be nice if you could identify the specific change that introduced it? Guess what? Binary chop time.\\n\\n## **The Binary Chop**\\n\\nEvery CS undergraduate has been forced to code a binary chop (sometimes called a binary search). The idea is simple. You\\'re looking for a particular value in a sorted array. You could just look at each value in turn, but you\\'d end up looking at roughly half the entries on average until you either found the value you wanted, or you found a value greater than it, which would mean the value\\'s not in the array.\\n\\nBut it\\'s faster to use a *divide and conquer* approach. Choose a value in the middle of the array. If it\\'s the one you\\'re looking for, stop. Otherwise you can chop the array in two. If the value you find is greater than the target then you know it must be in the first half of the array, otherwise it\\'s in the second half. Repeat the procedure in the appropriate subarray, and in no\\n\\ntime you\\'ll have a result. (As we\\'ll see when we talk about *Big-O Notation*, a linear search is , and a binary chop is ).\\n\\nSo, the binary chop is way, way faster on any decent sized problem. Let\\'s see how to apply it to debugging.\\n\\nWhen you\\'re facing a massive stacktrace and you\\'re trying to find out exactly which function mangled the value in error, you do a chop by choosing a stack frame somewhere in the middle and seeing if the error is manifest there. If it is, then you know to focus on the frames before, otherwise the problem is in the frames after. Chop again. Even if you have 64 frames in the stacktrace, this approach will give you an answer after at most six attempts.\\n\\nIf you find bugs that appear on certain datasets, you might be able to do the same thing. Split the dataset into two, and see if the problem occurs if you feed one or the other through the app. Keep dividing the data until you get a minimum set of values that exhibit the problem.\\n\\nIf your team has introduced a bug during a set of releases, you can use the same type of technique. Create a test that causes the current release to fail. Then choose a half-way release between now and the last known working version. Run the test again, and decide how to narrow your search. Being able to do this is just one of the many benefits of having good version control in your projects. Indeed, many version control systems will take this further and will automate the process, picking releases for you depending on the result of the test.\\n\\n## **Logging and/or Tracing**\\n\\nDebuggers generally focus on the state of the program *now*. Sometimes you need more—you need to watch the state of a program or a data structure over time. Seeing a stack trace can only tell you how you got here directly. It typically can\\'t tell you what you were doing prior to this call chain, especially in event-based systems.[25]\\n\\n*Tracing statements* are those little diagnostic messages you print to the screen or to a file that say things such as \"got here\" and \"value of x = 2.\" It\\'s a primitive technique compared with IDE-style debuggers, but it is peculiarly effective at diagnosing several classes of errors that debuggers can\\'t. Tracing is invaluable in any system where time itself is a factor: concurrent processes, real-time systems, and event-based applications.\\n\\nYou can use tracing statements to drill down into the code. That is, you can add tracing statements as you descend the call tree.\\n\\nTrace messages should be in a regular, consistent format as you may want to parse them automatically. For instance, if you needed to track down a resource leak (such as unbalanced file opens/closes), you could trace each open and each close in a log file. By processing the log file with text processing tools or shell commands, you can easily identify where the offending open was occurring.\\n\\n## **Rubber Ducking**\\n\\nA very simple but particularly useful technique for finding the cause of a problem is simply to explain it to someone else. The other person should look over your shoulder at the screen, and nod his or her head constantly (like a rubber duck bobbing up and down in a bathtub). They do not need to say a word; the simple act of explaining, step by step, what the code is supposed to do often causes the problem to leap off the screen and announce itself.[26]\\n\\nIt sounds simple, but in explaining the problem to another person you must explicitly state things that you may take for granted when going through the code yourself. By having to verbalize some of these assumptions, you may suddenly gain new insight into the problem. And if you don\\'t have a person, a rubber duck, or teddy bear, or potted plant will do.[27]\\n\\n## **Process of Elimination**\\n\\nIn most projects, the code you are debugging may be a mixture of application code written by you and others on your project team, third-party products (database, connectivity, web framework, specialized communications or algorithms, and so on) and the platform environment (operating system, system libraries, and compilers).\\n\\nIt is possible that a bug exists in the OS, the compiler, or a third-party product—but this should not be your first thought. It is much more likely that the bug exists in the application code under development. It is generally more profitable to assume that the application code is incorrectly calling into a library than to assume that the library itself is broken. Even if the problem *does* lie with a third party, you\\'ll still have to eliminate your code before submitting the bug report.\\n\\nWe worked on a project where a senior engineer was convinced that the select system call was broken on a Unix system. No amount of persuasion or logic could change his mind (the fact that every other networking application on the box worked fine was irrelevant). He spent weeks writing workarounds, which, for some odd reason, didn\\'t seem to fix the problem. When finally forced to sit down and read the documentation on select, he discovered the problem and corrected it in a matter of minutes. We now use the phrase \"select is broken\\'\\' as a gentle reminder whenever one of us starts blaming the system for a fault that is likely to be our own.\\n\\n### **Tip 33** \"select\" Isn\\'t Broken\\n\\nRemember, if you see hoof prints, think horses—not zebras. The OS is probably not broken. And select is probably just fine.\\n\\nIf you \"changed only one thing\\'\\' and the system stopped working, that one thing was likely to be responsible, directly or indirectly, no matter how farfetched it seems. Sometimes the thing that changed is outside of your\\n\\ncontrol: new versions of the OS, compiler, database, or other third-party software can wreak havoc with previously correct code. New bugs might show up. Bugs for which you had a workaround get fixed, breaking the workaround. APIs change, functionality changes; in short, it\\'s a whole new ball game, and you must retest the system under these new conditions. So keep a close eye on the schedule when considering an upgrade; you may want to wait until *after* the next release.\\n\\n## **The Element of Surprise**\\n\\nWhen you find yourself surprised by a bug (perhaps even muttering \"that\\'s impossible\" under your breath where we can\\'t hear you), you must reevaluate truths you hold dear. In that discount calculation algorithm—the one you knew was bulletproof and couldn\\'t possibly be the cause of this bug—did you test *all* the boundary conditions? That other piece of code you\\'ve been using for years—it couldn\\'t possibly still have a bug in it. Could it?\\n\\nOf course it can. The amount of surprise you feel when something goes wrong is proportional to the amount of trust and faith you have in the code being run. That\\'s why, when faced with a \"surprising\\'\\' failure, you must accept that one or more of your assumptions is wrong. Don\\'t gloss over a routine or piece of code involved in the bug because you \"know\" it works. Prove it. Prove it in *this* context, with *this* data, with *these* boundary conditions.\\n\\n**Tip 34** Don\\'t Assume It—Prove It\\n\\nWhen you come across a surprise bug, beyond merely fixing it, you need to determine why this failure wasn\\'t caught earlier. Consider whether you need to amend the unit or other tests so that they would have caught it.\\n\\nAlso, if the bug is the result of bad data that was propagated through a couple of levels before causing the explosion, see if better parameter checking in those routines would have isolated it earlier (see the discussions on crashing early and assertions here and here, respectively).\\n\\nWhile you\\'re at it, are there any other places in the code that may be susceptible to this same bug? Now is the time to find and fix them. Make sure that *whatever* happened, you\\'ll know if it happens again.\\n\\nIf it took a long time to fix this bug, ask yourself why. Is there anything you can do to make fixing this bug easier the next time around? Perhaps you could build in better testing hooks, or write a log file analyzer.\\n\\nFinally, if the bug is the result of someone\\'s wrong assumption, discuss the problem with the whole team: if one person misunderstands, then it\\'s possible many people do.\\n\\nDo all this, and hopefully you won\\'t be surprised next time.\\n\\n## **Debugging Checklist**\\n\\n- Is the problem being reported a direct result of the underlying bug, or merely a symptom?\\n- Is the bug *really* in the framework you\\'re using? Is it in the OS? Or is it in your code?\\n- If you explained this problem in detail to a coworker, what would you say?\\n- If the suspect code passes its unit tests, are the tests complete enough? What happens if you run the tests with *this* data?\\n- Do the conditions that caused this bug exist anywhere else in the system? Are there other bugs still in the larval stage, just waiting to\\n\\nhatch?\\n\\n## **Related Sections Include**\\n\\n- Topic 24, *Dead Programs Tell No Lies*\\n## **Challenges**\\n\\n- Debugging is challenge enough.\\n## **Topic 21** Text Manipulation\\n\\nPragmatic Programmers manipulate text the same way woodworkers shape wood. In previous sections we discussed some specific tools—shells, editors, debuggers—that we use. These are similar to a woodworker\\'s chisels, saws, and planes—tools specialized to do one or two jobs well. However, every now and then we need to perform some transformation not readily handled by the basic tool set. We need a general-purpose text manipulation tool.\\n\\nText manipulation languages are to programming what routers[28] are to woodworking. They are noisy, messy, and somewhat brute force. Make mistakes with them, and entire pieces can be ruined. Some people swear they have no place in the toolbox. But in the right hands, both routers and text manipulation languages can be incredibly powerful and versatile. You can quickly trim something into shape, make joints, and carve. Used properly, these tools have surprising finesse and subtlety. But they take time to master.\\n\\nFortunately, there are a number of great text manipulation languages. Unix developers (and we include macOS users here) often like to use the power of their command shells, augmented with tools such as awk and sed. People who prefer a more structured tool may prefer languages such as Python or Ruby.\\n\\nThese languages are important enabling technologies. Using them, you can quickly hack up utilities and prototype ideas—jobs that might take five or ten times as long using conventional languages. And that multiplying factor is crucially important to the kind of experimenting that we do. Spending 30 minutes trying out a crazy idea is a whole lot better than spending five hours. Spending a day automating important components of a project is\\n\\nacceptable; spending a week might not be. In their book *The Practice of Programming* [KP99], Kernighan and Pike built the same program in five different languages. The Perl version was the shortest (17 lines, compared with C\\'s 150). With Perl you can manipulate text, interact with programs, talk over networks, drive web pages, perform arbitrary precision arithmetic, and write programs that look like Snoopy swearing.\\n\\n## **Tip 35** Learn a Text Manipulation Language\\n\\nTo show the wide-ranging applicability of text manipulation languages, here\\'s a sample of some stuff we\\'ve done with Ruby and Python just related to the creation of this book:\\n\\n#### *Building the Book*\\n\\nThe build system for the Pragmatic Bookshelf is written in Ruby. Authors, editors, layout people, and support folks use Rake tasks to coordinate the building of PDFs and ebooks.\\n\\n#### *Code inclusion and highlighting*\\n\\nWe think it is important that any code presented in a book should have been tested first. Most of the code in this book has been. However, using the DRY principle (see Topic 9, *DRY—The Evils of Duplication*) we didn\\'t want to copy and paste lines of code from the tested programs into the book. That would mean we\\'d be duplicating code, virtually guaranteeing that we\\'d forget to update an example when the corresponding program was changed. For some examples, we also didn\\'t want to bore you with all the framework code needed to make our example compile and run. We turned to Ruby. A relatively simple script is invoked when we format the book—it extracts a named segment of a source file, does syntax highlighting, and converts the result into the typesetting language we use.\\n\\n#### *Website update*\\n\\nWe have a simple script that does a partial book build, extracts the table of contents, then uploads it to the book\\'s page on our website. We also have a script that extracts sections of a book and uploads them as samples.\\n\\n#### *Including equations*\\n\\nThere\\'s a Python script that converts LaTeX math markup into nicely formatted text.\\n\\n### *Index generation*\\n\\nMost indexes are created as separate documents (which makes maintaining them difficult if a document changes). Ours are marked up in the text itself, and a Ruby script collates and formats the entries.\\n\\nAnd so on. In a very real way, the Pragmatic Bookshelf is built around text manipulation. And if you follow our advice to keep things in plain text, then using these languages to manipulate that text will bring a whole host of benefits.\\n\\n## **Related Sections Include**\\n\\n- Topic 16, *The Power of Plain Text*\\n- Topic 17, *Shell Games*\\n\\n## **Exercises**\\n\\n### **Exercise 11**\\n\\nYou\\'re rewriting an application that used to use YAML as a configuration language. Your company has now standardized on JSON, so you have a bunch of .yaml files that need to be turned into .json. Write a script that takes a directory and converts each .yaml file into a corresponding .json file (so database.yaml becomes database.json, and the contents are valid JSON).\\n\\n## **Exercise 12**\\n\\nYour team initially chose to use camelCase names for variables, but then changed their collective mind and switched to snake_case. Write a script that scans all the source files for camelCase names and reports on them.\\n\\n## **Exercise 13**\\n\\nFollowing on from the previous exercise, add the ability to change those variable names automatically in one or more files. Remember to keep a backup of the originals in case something goes horribly, horribly wrong.\\n\\nDave once worked for a small computer manufacturer, which meant working alongside electronic and sometimes mechanical engineers.\\n\\nMany of them walked around with a paper notebook, normally with a pen stuffed down the spine. Every now and then when we were talking, they\\'d pop the notebook open and scribble something.\\n\\nEventually Dave asked the obvious question. It turned out that they\\'d been trained to keep an engineering *daybook*, a kind of journal in which they recorded what they did, things they\\'d learned, sketches of ideas, readings from meters: basically anything to do with their work. When the notebook became full, they\\'d write the date range on the spine, then stick it on the shelf next to previous daybooks. There may have been a gentle competition going on for whose set of books took the most shelf space.\\n\\nWe use daybooks to take notes in meetings, to jot down what we\\'re working on, to note variable values when debugging, to leave reminders where we put things, to record wild ideas, and sometimes just to doodle.[29]\\n\\nThe daybook has three main benefits:\\n\\n- It is more reliable than memory. People might ask \"What was the name of that company you called last week about the power supply problem?\" and you can flip back a page or so and give them the name and number.\\n- It gives you a place to store ideas that aren\\'t immediately relevant to the task at hand. That way you can continue to concentrate on what you are doing, knowing that the great idea won\\'t be forgotten.\\n\\n- It acts as a kind of rubber duck (described here). When you stop to write something down, your brain may switch gears, almost as if talking to someone—a great chance to reflect. You may start to make a note and then suddenly realize that what you\\'d just done, the topic of the note, is just plain wrong.\\nThere\\'s an added benefit, too. Every now and then you can look back at what you were doing oh-so-many-years-ago and think about the people, the projects, and the awful clothes and hairstyles.\\n\\nSo, try keeping an engineering daybook. Use paper, not a file or a wiki: there\\'s something special about the act of writing compared to typing. Give it a month, and see if you\\'re getting any benefits.\\n\\nIf nothing else, it\\'ll make writing your memoir easier when you\\'re rich and famous.\\n\\n## **Related Sections Include**\\n\\n- Topic 6, *Your Knowledge Portfolio*\\n- Topic 37, *Listen to Your Lizard Brain*\\n\\n#### **Footnotes**\\n\\n- [24] All software becomes legacy software as soon as it\\'s written.\\n- [25] Although the Elm language does have a time-traveling debugger.\\n- [26] Why \"rubber ducking\\'\\'? While an undergraduate at Imperial College in London, Dave did a lot of work with a research assistant named Greg Pugh, one of the best developers Dave has known. For several months Greg carried around a small yellow rubber duck, which he\\'d place on his terminal while coding. It was a while before Dave had the courage to ask….\\n- [27] Earlier versions of the book talked about talking to your *pot plant*. It was a typo. Honest.\\n- [28] Here *router* means the tool that spins cutting blades very, very fast, not a device for interconnecting networks.\\n\\n[29]\\n\\nThere is some evidence that doodling helps focus and improves cognitive skills, for example, see *What does doodling do?* [And10].\\n\\nCopyright © 2020 Pearson Education, Inc.\\n\\n# Chapter 4\\n\\n# **Pragmatic Paranoia**\\n\\n## **Tip 36** You Can\\'t Write Perfect Software\\n\\nDid that hurt? It shouldn\\'t. Accept it as an axiom of life. Embrace it. Celebrate it. Because perfect software doesn\\'t exist. No one in the brief history of computing has ever written a piece of perfect software. It\\'s unlikely that you\\'ll be the first. And unless you accept this as a fact, you\\'ll end up wasting time and energy chasing an impossible dream.\\n\\nSo, given this depressing reality, how does a Pragmatic Programmer turn it into an advantage? That\\'s the topic of this chapter.\\n\\nEveryone knows that they personally are the only good driver on Earth. The rest of the world is out there to get them, blowing through stop signs, weaving between lanes, not indicating turns, texting on the phone, and just generally not living up to our standards. So we drive defensively. We look out for trouble before it happens, anticipate the unexpected, and never put ourselves into a position from which we can\\'t extricate ourselves.\\n\\nThe analogy with coding is pretty obvious. We are constantly interfacing with other people\\'s code—code that might not live up to our high standards —and dealing with inputs that may or may not be valid. So we are taught to code defensively. If there\\'s any doubt, we validate all information we\\'re given. We use assertions to detect bad data, and distrust data from potential\\n\\nattackers or trolls. We check for consistency, put constraints on database columns, and generally feel pretty good about ourselves.\\n\\nBut Pragmatic Programmers take this a step further. *They don\\'t trust themselves, either.* Knowing that no one writes perfect code, including themselves, Pragmatic Programmers build in defenses against their own mistakes. We describe the first defensive measure in Topic 23, *Design by Contract*: clients and suppliers must agree on rights and responsibilities.\\n\\nIn Topic 24, *Dead Programs Tell No Lies*, we want to ensure that we do no damage while we\\'re working the bugs out. So we try to check things often and terminate the program if things go awry.\\n\\nTopic 25, *Assertive Programming* describes an easy method of checking along the way—write code that actively verifies your assumptions.\\n\\nAs your programs get more dynamic, you\\'ll find yourself juggling system resources—memory, files, devices, and the like. In Topic 26, *How to Balance Resources*, we\\'ll suggest ways of ensuring that you don\\'t drop any of the balls.\\n\\nAnd most importantly, we stick to small steps always, as described in Topic 27, *Don\\'t Outrun Your Headlights*, so we don\\'t fall off the edge of the cliff.\\n\\nIn a world of imperfect systems, ridiculous time scales, laughable tools, and impossible requirements, let\\'s play it safe. As Woody Allen said, \"When everybody actually *is* out to get you, paranoia is just good thinking.\"\\n\\n## **Topic 23** Design by Contract\\n\\n*Nothing astonishes men so much as common sense and plain dealing.*\\n\\n*Ralph Waldo Emerson, Essays*\\n\\nDealing with computer systems is hard. Dealing with people is even harder. But as a species, we\\'ve had longer to figure out issues of human interactions. Some of the solutions we\\'ve come up with during the last few millennia can be\\n\\napplied to writing software as well. One of the best solutions for ensuring plain dealing is the *contract*.\\n\\nA contract defines your rights and responsibilities, as well as those of the other party. In addition, there is an agreement concerning repercussions if either party fails to abide by the contract.\\n\\nMaybe you have an employment contract that specifies the hours you\\'ll work and the rules of conduct you must follow. In return, the company pays you a salary and other perks. Each party meets its obligations and everyone benefits.\\n\\nIt\\'s an idea used the world over—both formally and informally—to help humans interact. Can we use the same concept to help software modules interact? The answer is \"yes.\\'\\'\\n\\n## **DBC**\\n\\nBertrand Meyer (*Object-Oriented Software Construction* [Mey97]) developed the concept of *Design by Contract* for the language Eiffel.[30] It is a simple yet powerful technique that focuses on documenting (and agreeing to) the rights and responsibilities of software modules to ensure program correctness. What is a correct program? One that does no more and no less\\n\\nthan it claims to do. Documenting and verifying that claim is the heart of *Design by Contract* (DBC, for short).\\n\\nEvery function and method in a software system *does something*. Before it starts that *something*, the function may have some expectation of the state of the world, and it may be able to make a statement about the state of the world when it concludes. Meyer describes these expectations and claims as follows:\\n\\n#### *Preconditions*\\n\\nWhat must be true in order for the routine to be called; the routine\\'s requirements. A routine should never get called when its preconditions would be violated. It is the caller\\'s responsibility to pass good data (see the box here).\\n\\n#### *Postconditions*\\n\\nWhat the routine is guaranteed to do; the state of the world when the routine is done. The fact that the routine has a postcondition implies that it *will* conclude: infinite loops aren\\'t allowed.\\n\\n#### *Class invariants*\\n\\nA class ensures that this condition is always true from the perspective of a caller. During internal processing of a routine, the invariant may not hold, but by the time the routine exits and control returns to the caller, the invariant must be true. (Note that a class cannot give unrestricted write-access to any data member that participates in the invariant.)\\n\\nThe contract between a routine and any potential caller can thus be read as\\n\\nIf all the routine\\'s preconditions are met by the caller, the routine shall guarantee that all postconditions and invariants will be true when it completes.\\n\\nIf either party fails to live up to the terms of the contract, then a remedy (which was previously agreed to) is invoked—maybe an exception is raised, or the program terminates. Whatever happens, make no mistake that failure to live up to the contract is a bug. It is not something that should ever happen, which is why preconditions should not be used to perform things such as user-input validation.\\n\\nSome languages have better support for these concepts than others. Clojure, for example, supports pre- and post-conditions as well as the more comprehensive instrumentation provided by *specs*. Here\\'s an example of a banking function to make a deposit using simple pre- and post-conditions:\\n\\n```\\n( defn accept-deposit [account-id amount]\\n { :pre [ (> amount 0.00)\\n (account-open? account-id) ]\\n :post [ (contains? (account-transactions account-id) %) ] }\\n \"Accept a deposit and return the new transaction id\"\\n ;; Some other processing goes here...\\n ;; Return the newly created transaction:\\n (create-transaction account-id :deposit amount))\\n```\\nThere are two preconditions for the accept-deposit function. The first is that the amount is greater than zero, and the second is that the account is open and valid, as determined by some function named account-open?. There is also a postcondition: the function guarantees that the new transaction (the return value of this function, represented here by \\'%\\') can be found among the transactions for this account.\\n\\nIf you call accept-deposit with a positive amount for the deposit and a valid account, it will proceed to create a transaction of the appropriate type and do whatever other processing it does. However, if there\\'s a bug in the program and you somehow passed in a negative amount for the deposit, you\\'ll get a runtime exception:\\n\\n```\\nException in thread \"main\"...\\nCaused by: java.lang.AssertionError: Assert failed: (> amount 0.0)\\n```\\nSimilarly, this function requires that the specified account is open and valid. If it\\'s not, you\\'ll see that exception instead:\\n\\n```\\nException in thread \"main\"...\\nCaused by: java.lang.AssertionError: Assert failed: (account-open? account-\\nid)\\n```\\nOther languages have features that, while not DBC-specific, can still be used to good effect. For example, Elixir uses *guard clauses* to dispatch function calls against several available bodies:\\n\\n```\\ndefmodule Deposits do\\n def accept_deposit(account_id, amount) when (amount > 100000) do\\n # Call the manager!\\n end\\n def accept_deposit(account_id, amount) when (amount > 10000) do\\n # Extra Federal requirements for reporting\\n # Some processing...\\n end\\n def accept_deposit(account_id, amount) when (amount > 0) do\\n # Some processing...\\n end\\nend\\n```\\nIn this case, calling accept_deposit with a large enough amount may trigger additional steps and processing. Try to call it with an amount less than or equal to zero, however, and you\\'ll get an exception informing you that you can\\'t:\\n\\n```\\n** (FunctionClauseError) no function clause matching in\\nDeposits.accept_deposit/2\\n```\\nThis is a better approach than simply checking your inputs; in this case, you simply *can not* call this function if your arguments are out of range.\\n\\n![](_page_175_Picture_7.jpeg)\\n\\nIn Topic 10, *Orthogonality*, we recommended writing \"shy\" code. Here, the emphasis is on \"lazy\" code: be strict in what you will accept before you begin, and promise as little as possible in return. Remember, if your contract indicates that you\\'ll accept anything and promise the world in return, then you\\'ve got a lot of code to write!\\n\\nIn any programming language, whether it\\'s functional, object-oriented, or procedural, DBC forces you to *think.*\\n\\n#### **DBC and Test-Driven Development**\\n\\nIs Design by Contract needed in a world where developers practice unit testing, test-driven development (TDD), property-based testing, or defensive programming?\\n\\nThe short answer is \"yes.\"\\n\\nDBC and testing are different approaches to the broader topic of program correctness. They both have value and both have uses in different situations. DBC offers several advantages over specific testing approaches:\\n\\n- DBC doesn\\'t require any setup or mocking\\n- DBC defines the parameters for success or failure in *all* cases, whereas testing can only target one specific case at a time\\n- TDD and other testing happens only at \"test time\" within the build cycle. But DBC and assertions are forever: during design, development, deployment, and maintenance\\n- TDD does not focus on checking internal invariants within the code under test, it\\'s more black-box style to check the public interface\\n- DBC is more efficient (and DRY-er) than defensive programming, where *everyone* has to validate data in case no one else does.\\n\\nTDD is a great technique, but as with many techniques, it might invite you to concentrate on the \"happy path,\" and not the real world full of bad data, bad actors, bad versions, and bad specifications.\\n\\n#### **Class Invariants and Functional Languages**\\n\\nIt\\'s a naming thing. Eiffel is an object-oriented language, so Meyer named this idea \"class invariant.\" But, really, it\\'s more general than that. What this idea really refers to is *state*. In an object-oriented language, the state is associated with instances of classes. But other languages have state, too.\\n\\nIn a functional language, you typically pass state to functions and receive updated state as a result. The concepts of invariants is just as useful in these circumstances.\\n\\n## **Implementing DBC**\\n\\nSimply enumerating what the input domain range is, what the boundary conditions are, and what the routine promises to deliver—or, more importantly, what it *doesn\\'t* promise to deliver—before you write the code is a huge leap forward in writing better software. By not stating these things, you are back to *programming by coincidence* (see the discussion here), which is where many projects start, finish, and fail.\\n\\nIn languages that do not support DBC in the code, this might be as far as you can go—and that\\'s not too bad. DBC is, after all, a *design* technique. Even without automatic checking, you can put the contract in the code as comments or in the unit tests and still get a very real benefit.\\n\\n## **Assertions**\\n\\nWhile documenting these assumptions is a great start, you can get much greater benefit by having the compiler check your contract for you. You can partially emulate this in some languages by using *assertions:* runtime checks of logical conditions (see Topic 25, *Assertive Programming*). Why only partially? Can\\'t you use assertions to do everything DBC can do?\\n\\nUnfortunately, the answer is no. To begin with, in object-oriented languages there probably is no support for propagating assertions down an inheritance hierarchy. This means that if you override a base class method that has a contract, the assertions that implement that contract will not be called\\n\\ncorrectly (unless you duplicate them manually in the new code). You must remember to call the class invariant (and all base class invariants) manually before you exit every method. The basic problem is that the contract is not automatically enforced.\\n\\nIn other environments, the exceptions generated from DBC-style assertions might be turned off globally or ignored in the code.\\n\\nAlso, there is no built-in concept of \"old\\'\\' values; that is, values as they existed at the entry to a method. If you\\'re using assertions to enforce contracts, you must add code to the precondition to save any information you\\'ll want to use in the postcondition, if the language will even allow that. In the Eiffel language, where DBC was born, you can just use old *expression*.\\n\\nFinally, conventional runtime systems and libraries are not designed to support contracts, so these calls are not checked. This is a big loss, because it is often at the boundary between your code and the libraries it uses that the most problems are detected (see Topic 24, *Dead Programs Tell No Lies* for a more detailed discussion).\\n\\n#### **Who\\'s Responsible?**\\n\\nWho is responsible for checking the precondition, the caller or the routine being called? When implemented as part of the language, the answer is neither: the precondition is tested behind the scenes after the caller invokes the routine but before the routine itself is entered. Thus if there is any explicit checking of parameters to be done, it must be performed by the *caller*, because the routine itself will never see parameters that violate its precondition. (For languages without built-in support, you would need to bracket the *called* routine with a preamble and/or postamble that checks these assertions.)\\n\\nConsider a program that reads a number from the console, calculates its square root (by calling sqrt), and prints the result. The sqrt function has a precondition—its argument must not be negative. If the user enters a negative number at the console, it is up to the calling code to ensure that it never gets passed to sqrt. This calling code has many options: it could terminate, it could issue a warning and read another number, or it could make the number positive and append an *i* to the result returned by sqrt. Whatever its choice, this is definitely not sqrt\\'s problem.\\n\\nBy expressing the domain of the square root function in the precondition of the sqrt routine, you shift the burden of correctness to the caller—where it belongs. You can then design the sqrt routine secure in the knowledge that its input will be in range.\\n\\n## **DBC and Crashing Early**\\n\\nDBC fits in nicely with our concept of crashing early (see Topic 24, *Dead Programs Tell No Lies*). By using an assert or DBC mechanism to validate the preconditions, postconditions, and invariants, you can crash early and report more accurate information about the problem.\\n\\nFor example, suppose you have a method that calculates square roots. It needs a DBC precondition that restricts the domain to positive numbers. In languages that support DBC, if you pass sqrt a negative parameter, you\\'ll get an informative error such as sqrt_arg_must_be_positive, along with a stack trace.\\n\\nThis is better than the alternative in other languages such as Java, C, and C++ where passing a negative number to sqrt returns the special value NaN (Not a Number). It may be some time later in the program that you attempt to do some math on NaN, with surprising results.\\n\\nIt\\'s much easier to find and diagnose the problem by crashing early, at the site of the problem.\\n\\n## **Semantic Invariants**\\n\\nYou can use *semantic invariants* to express inviolate requirements, a kind of \"philosophical contract.\\'\\'\\n\\nWe once wrote a debit card transaction switch. A major requirement was that the user of a debit card should never have the same transaction applied to their account twice. In other words, no matter what sort of failure mode might happen, the error should be on the side of *not* processing a transaction rather than processing a duplicate transaction.\\n\\nThis simple law, driven directly from the requirements, proved to be very helpful in sorting out complex error recovery scenarios, and guided the detailed design and implementation in many areas.\\n\\nBe sure not to confuse requirements that are fixed, inviolate laws with those that are merely policies that might change with a new management regime. That\\'s why we use the term *semantic* invariants—it must be central to the very *meaning* of a thing, and not subject to the whims of policy (which is what more dynamic business rules are for).\\n\\nWhen you find a requirement that qualifies, make sure it becomes a wellknown part of whatever documentation you are producing—whether it is a bulleted list in the requirements document that gets signed in triplicate or just a big note on the common whiteboard that everyone sees. Try to state it clearly and unambiguously. For example, in the debit card example, we might write\\n\\nErr in favor of the consumer.\\n\\nThis is a clear, concise, unambiguous statement that\\'s applicable in many different areas of the system. It is our contract with all users of the system, our guarantee of behavior.\\n\\n## **Dynamic Contracts and Agents**\\n\\nUntil now, we have talked about contracts as fixed, immutable specifications. But in the landscape of autonomous agents, this doesn\\'t need to be the case. By the definition of \"autonomous,\" agents are free to *reject* requests that they do not want to honor. They are free to renegotiate the contract—\"I can\\'t provide that, but if you give me this, then I might provide something else.\"\\n\\nCertainly any system that relies on agent technology has a *critical* dependence on contractual arrangements—even if they are dynamically generated.\\n\\nImagine: with enough components and agents that can negotiate their own contracts among themselves to achieve a goal, we might just solve the software productivity crisis by letting software solve it for us.\\n\\nBut if we can\\'t use contracts by hand, we won\\'t be able to use them automatically. So next time you design a piece of software, design its contract as well.\\n\\n## **Related Sections Include**\\n\\n- Topic 24, *Dead Programs Tell No Lies*\\n- Topic 25, *Assertive Programming*\\n- Topic 38, *Programming by Coincidence*\\n- Topic 42, *Property-Based Testing*\\n- Topic 43, *Stay Safe Out There*\\n- Topic 45, *The Requirements Pit*\\n\\n## **Challenges**\\n\\n- Points to ponder: If DBC is so powerful, why isn\\'t it used more widely? Is it hard to come up with the contract? Does it make you think about issues you\\'d rather ignore for now? Does it force you to *THINK!*? Clearly, this is a dangerous tool!\\n## **Exercises**\\n\\n## **Exercise 14** (possible answer)\\n\\nDesign an interface to a kitchen blender. It will eventually be a web-based, IoT-enabled blender, but for now we just need the interface to control it. It has ten speed settings (0 means off). You can\\'t operate it empty, and you can change the speed only one unit at a time (that is, from 0 to 1, and from 1 to 2, not from 0 to 2).\\n\\nHere are the methods. Add appropriate pre- and postconditions and an invariant.\\n\\n```\\nint getSpeed()\\nvoid setSpeed(int x)\\nboolean isFull()\\nvoid fill()\\nvoid empty()\\n```\\n**Exercise 15** (possible answer)\\n\\nHow many numbers are in the series 0, 5, 10, 15, …, 100?\\n\\n## **Topic 24** Dead Programs Tell No Lies\\n\\nHave you noticed that sometimes other people can detect that things aren\\'t well with you before you\\'re aware of the problem yourself? It\\'s the same with other people\\'s code. If something is starting to go awry with one of our programs, sometimes it is a library or framework routine that catches it first. Maybe we\\'ve passed in a nil value, or an empty list. Maybe there\\'s a missing key in that hash, or the value we thought contained a hash really contains a list instead. Maybe there was a network error or filesystem error that we didn\\'t catch, and we\\'ve got empty or corrupted data. A logic error a couple of million instructions ago means that the selector for a case statement is no longer the expected 1, 2, or 3. We\\'ll hit the default case unexpectedly. That\\'s also one reason why each and every case/switch statement needs to have a default clause: we want to know when the \"impossible\" has happened.\\n\\nIt\\'s easy to fall into the \"it can\\'t happen\" mentality. Most of us have written code that didn\\'t check that a file closed successfully, or that a trace statement got written as we expected. And all things being equal, it\\'s likely that we didn\\'t need to—the code in question wouldn\\'t fail under any normal conditions. But we\\'re coding defensively. We\\'re making sure that the data is what we think it is, that the code in production is the code we think it is. We\\'re checking that the correct versions of dependencies were actually loaded.\\n\\nAll errors give you information. You could convince yourself that the error can\\'t happen, and choose to ignore it. Instead, Pragmatic Programmers tell themselves that if there is an error, something very, very bad has happened. Don\\'t forget to Read the Damn Error Message (see *Coder in a Strange Land*).\\n\\n## **Catch and Release Is for Fish**\\n\\nSome developers feel that is it good style to catch or rescue all exceptions, re-raising them after writing some kind of message. Their code is full of things like this (where a bare raise statement reraises the current exception):\\n\\n```\\ntry do\\n add_score_to_board(score);\\nrescue InvalidScore\\n Logger.error( \"Can\\'t add invalid score. Exiting\");\\n raise\\nrescue BoardServerDown\\n Logger.error( \"Can\\'t add score: board is down. Exiting\");\\n raise\\nrescue StaleTransaction\\n Logger.error( \"Can\\'t add score: stale transaction. Exiting\");\\n raise\\nend\\n```\\nHere\\'s how Pragmatic Programmers would write this:\\n\\nadd_score_to_board(score);\\n\\nWe prefer it for two reasons. First, the application code isn\\'t eclipsed by the error handling. Second, and perhaps more important, the code is less coupled. In the verbose example, we have to list every exception the add_score_to_board method could raise. If the writer of that method adds another exception, our code is subtly out of date. In the more pragmatic second version, the new exception is automatically propagated.\\n\\n**Tip 38** Crash Early\\n\\n## **Crash, Don\\'t Trash**\\n\\nOne of the benefits of detecting problems as soon as you can is that you can crash earlier, and crashing is often the best thing you can do. The alternative\\n\\nmay be to continue, writing corrupted data to some vital database or commanding the washing machine into its twentieth consecutive spin cycle.\\n\\nThe Erlang and Elixir languages embrace this philosophy. Joe Armstrong, inventor of Erlang and author of *Programming Erlang: Software for a Concurrent World* [Arm07], is often quoted as saying, \"Defensive programming is a waste of time. Let it crash!\" In these environments, programs are designed to fail, but that failure is managed with *supervisors*. A supervisor is responsible for running code and knows what to do in case the code fails, which could include cleaning up after it, restarting it, and so on. What happens when the supervisor itself fails? Its own supervisor manages that event, leading to a design composed of *supervisor trees.* The technique is very effective and helps to account for the use of these languages in high-availability, fault-tolerant systems.\\n\\nIn other environments, it may be inappropriate simply to exit a running program. You may have claimed resources that might not get released, or you may need to write log messages, tidy up open transactions, or interact with other processes.\\n\\nHowever, the basic principle stays the same—when your code discovers that something that was supposed to be impossible just happened, your program is no longer viable. Anything it does from this point forward becomes suspect, so terminate it as soon as possible.\\n\\nA dead program normally does a lot less damage than a crippled one.\\n\\n## **Related Sections Include**\\n\\n- Topic 20, *Debugging*\\n- Topic 23, *Design by Contract*\\n- Topic 25, *Assertive Programming*\\n- Topic 26, *How to Balance Resources*\\n- Topic 43, *Stay Safe Out There*\\n\\n## **Topic 25** Assertive Programming\\n\\n*There is a luxury in self-reproach. When we blame ourselves we feel no one else has a right to blame us.*\\n\\n> *Oscar Wilde, The Picture of Dorian Gray*\\n\\nIt seems that there\\'s a mantra that every programmer must memorize early in his or her career. It is a fundamental tenet of computing, a core belief that we learn to apply to requirements, designs, code, comments, just about everything we do. It goes\\n\\nThis can never happen…\\n\\n\"This application will never be used abroad, so why internationalize it?\" \"count can\\'t be negative.\" \"Logging can\\'t fail.\"\\n\\nLet\\'s not practice this kind of self-deception, particularly when coding.\\n\\n![](_page_186_Picture_8.jpeg)\\n\\n## **Tip 39** Use Assertions to Prevent the Impossible\\n\\nWhenever you find yourself thinking \"but of course that could never happen,\" add code to check it. The easiest way to do this is with assertions. In many language implementations, you\\'ll find some form of assert that checks a Boolean condition.[31] These checks can be invaluable. If a parameter or a result should never be null, then check for it explicitly:\\n\\n```\\nassert (result != null);\\n```\\nIn the Java implementation, you can (and should) add a descriptive string:\\n\\n**assert** result != **null** && result.size() > 0 : *\"Empty result from XYZ\"*;\\n\\nAssertions are also useful checks on an algorithm\\'s operation. Maybe you\\'ve written a clever sort algorithm, named my_sort. Check that it works:\\n\\n```\\nbooks = my_sort(find( \"scifi\"))\\nassert(is_sorted?(books))\\n```\\nDon\\'t use assertions in place of real error handling. Assertions check for things that should never happen: you don\\'t want to be writing code such as the following:\\n\\n```\\nputs( \"Enter \\'Y\\' or \\'N\\': \")\\nans = gets[0] # Grab first character of response\\nassert((ch == \\'Y\\') || (ch == \\'N\\')) # Very bad idea!\\n```\\nAnd just because most assert implementations will terminate the process when an assertion fails, there\\'s no reason why versions you write should. If you need to free resources, catch the assertion\\'s exception or trap the exit, and run your own error handler. Just make sure the code you execute in those dying milliseconds doesn\\'t rely on the information that triggered the assertion failure in the first place.\\n\\n## **Assertions and Side Effects**\\n\\nIt\\'s embarrassing when the code we add to detect errors actually ends up creating new errors. This can happen with assertions if evaluating the condition has side effects. For example, it would be a bad idea to code something such as\\n\\n```\\nwhile (iter.hasMoreElements()) {\\n assert(iter.nextElement() != null);\\n Object obj = iter.nextElement();\\n // ....\\n}\\n```\\nThe .nextElement() call in the assertion has the side effect of moving the iterator past the element being fetched, and so the loop will process only half the elements in the collection. It would be better to write\\n\\n```\\nwhile (iter.hasMoreElements()) {\\n Object obj = iter.nextElement();\\n assert(obj != null);\\n // ....\\n}\\n```\\nThis problem is a kind of Heisenbug[32]—debugging that changes the behavior of the system being debugged.\\n\\n(We also believe that nowadays, when most languages have decent support for iterating functions over collections, this kind of explicit loop is unnecessary and bad form.)\\n\\n## **Leave Assertions Turned On**\\n\\nThere is a common misunderstanding about assertions. It goes something like this:\\n\\nAssertions add some overhead to code. Because they check for things that should never happen, they\\'ll get triggered only by a bug in the code. Once the code has been tested and shipped, they are no longer needed, and should be turned off to make the code run faster. Assertions are a debugging facility.\\n\\nThere are two patently wrong assumptions here. First, they assume that testing finds all the bugs. In reality, for any complex program you are unlikely to test even a minuscule percentage of the permutations your code will be put through. Second, the optimists are forgetting that your program runs in a dangerous world. During testing, rats probably won\\'t gnaw through a communications cable, someone playing a game won\\'t exhaust memory, and log files won\\'t fill the storage partition. These things might happen when your program runs in a production environment. Your first line of defense is checking for any possible error, and your second is using assertions to try to detect those you\\'ve missed.\\n\\nTurning off assertions when you deliver a program to production is like crossing a high wire without a net because you once made it across in practice. There\\'s dramatic value, but it\\'s hard to get life insurance.\\n\\nEven if you *do* have performance issues, turn off only those assertions that really hit you. The sort example above may be a critical part of your application, and may need to be fast. Adding the check means another pass through the data, which might be unacceptable. Make that particular check optional, but leave the rest in.\\n\\n#### **Use Assertions in Production, Win Big Money**\\n\\nA former neighbor of Andy\\'s headed up a small startup company that made network devices. One of their secrets to success was the decision to leave assertions in place in production releases. These assertions were well crafted to report all the pertinent data leading to the failure, and presented via a nice-looking UI to the end user. This level of feedback, from real users under actual conditions, allowed the developers to plug the holes and fix these obscure, hard-to-reproduce bugs, resulting in remarkably stable, bullet-proof software.\\n\\nThis small, unknown company had such a solid product, it was soon acquired for hundreds of millions of dollars.\\n\\nJust sayin\\'.\\n\\n#### **Exercise 16** (possible answer)\\n\\nA quick reality check. Which of these \"impossible\" things can happen?\\n\\n- A month with fewer than 28 days\\n- Error code from a system call: can\\'t access the current directory\\n- In C++: a = 2; b = 3; but (a + b) does not equal 5\\n- A triangle with an interior angle sum ≠ 180°\\n- A minute that doesn\\'t have 60 seconds\\n- (a + 1) <= a\\n\\n## **Related Sections Include**\\n\\n- Topic 23, *Design by Contract*\\n- Topic 24, *Dead Programs Tell No Lies*\\n- Topic 42, *Property-Based Testing*\\n- Topic 43, *Stay Safe Out There*\\n\\n## **Topic 26** How to Balance Resources\\n\\n*To light a candle is to cast a shadow...*\\n\\n*Ursula K. Le Guin, A Wizard of Earthsea* We all manage resources whenever we code: memory, transactions, threads, network connections, files, timers—all kinds of things with limited availability. Most of the time, resource usage follows a predictable pattern: you\\n\\nallocate the resource, use it, and then deallocate it.\\n\\nHowever, many developers have no consistent plan for dealing with resource allocation and deallocation. So let us suggest a simple tip:\\n\\n**Tip 40** Finish What You Start\\n\\nThis tip is easy to apply in most circumstances. It simply means that the function or object that allocates a resource should be responsible for deallocating it. Let\\'s see how it applies by looking at an example of some bad code—part of a Ruby program that opens a file, reads customer information from it, updates a field, and writes the result back. We\\'ve eliminated error handling to make the example clearer:\\n\\n```\\ndef read_customer\\n @customer_file = File.open(@name + \".rec\", \"r+\")\\n @balance = BigDecimal(@customer_file.gets)\\nend\\ndef write_customer\\n @customer_file.rewind\\n @customer_file.puts @balance.to_s\\n @customer_file.close\\nend\\n```\\n\\n```\\ndef update_customer(transaction_amount)\\n read_customer\\n @balance = @balance.add(transaction_amount,2)\\n write_customer\\nend\\n```\\nAt first sight, the routine update_customer looks reasonable. It seems to implement the logic we require—reading a record, updating the balance, and writing the record back out. However, this tidiness hides a major problem. The routines read_customer and write_customer are tightly coupled[33] —they share the instance variable customer_file. read_customer opens the file and stores the file reference in customer_file, and then write_customer uses that stored reference to close the file when it finishes. This shared variable doesn\\'t even appear in the update_customer routine.\\n\\nWhy is this bad? Let\\'s consider the unfortunate maintenance programmer who is told that the specification has changed—the balance should be updated only if the new value is not negative. They go into the source and change update_customer:\\n\\n```\\ndef update_customer(transaction_amount)\\n read_customer\\n if (transaction_amount >= 0.00)\\n @balance = @balance.add(transaction_amount,2)\\n write_customer\\n end\\nend\\n```\\nAll seems fine during testing. However, when the code goes into production, it collapses after several hours, complaining of *too many open files*. It turns out that write_customer is not getting called in some circumstances. When that happens, the file is not getting closed.\\n\\nA very *bad* solution to this problem would be to deal with the special case in update_customer:.\\n\\n```\\ndef update_customer(transaction_amount)\\n```\\n\\n```\\n read_customer\\n if (transaction_amount >= 0.00)\\n @balance += BigDecimal(transaction_amount, 2)\\n write_customer\\n else\\n @customer_file.close # Bad idea!\\n end\\nend\\n```\\nThis will fix the problem—the file will now get closed regardless of the new balance—but the fix now means that *three* routines are coupled through the shared variable customer_file, and keeping track of when the file is open or not is going to start to get messy. We\\'re falling into a trap, and things are going to start going downhill rapidly if we continue on this course. This is not balanced!\\n\\nThe *finish what you start* tip tells us that, ideally, a routine that allocates a resource should also free it. We can apply it here by refactoring the code slightly:\\n\\n```\\ndef read_customer(file)\\n @balance=BigDecimal(file.gets)\\nend\\ndef write_customer(file)\\n file.rewind\\n file.puts @balance.to_s\\nend\\ndef update_customer(transaction_amount)\\n file=File.open(@name + \".rec\", \"r+\") # >--\\n read_customer(file) # |\\n @balance = @balance.add(transaction_amount,2) # |\\n write_customer(file) # |\\n file.close # <--\\nend\\n```\\nInstead of holding on to the file reference, we\\'ve changed the code to pass it as a parameter. [34] Now all the responsibility for the file is in the\\n\\nupdate_customer routine. It opens the file and (finishing what it starts) closes it before returning. The routine balances the use of the file: the open and close are in the same place, and it is apparent that for every open there will be a corresponding close. The refactoring also removes an ugly shared variable.\\n\\nThere\\'s another small but important improvement we can make. In many modern languages, you can scope the lifetime of a resource to an enclosed block of some sort. In Ruby, there\\'s a variation of the file open that passes in the open file reference to a block, shown here between the do and the end:\\n\\n```\\ndef update_customer(transaction_amount)\\n File.open(@name + \".rec\", \"r+\") do |file| # >--\\n read_customer(file) # |\\n @balance = @balance.add(transaction_amount,2) # |\\n write_customer(file) # |\\n end # <--\\nend\\n```\\nIn this case, at the end of the block the file variable goes out of scope and the external file is closed. Period. No need to remember to close the file and release the source, it is guaranteed to happen for you.\\n\\nWhen in doubt, it always pays to reduce scope.\\n\\n![](_page_194_Picture_5.jpeg)\\n\\n#### **Balancing Over Time**\\n\\nIn this topic we\\'re mostly looking at ephemeral resources used by your running process. But you might want to consider what other messes you might be leaving behind.\\n\\nFor instance, how are your logging files handled? You are creating data and using up storage space. Is there something in place to rotate the logs and clean them up? How about for your unofficial debug files you\\'re dropping? If you\\'re adding logging records in a database, is there a similar process in place to expire them? For anything that you create that takes up a finite resource, consider how to balance it.\\n\\nWhat else are you leaving behind?\\n\\n## **Nest Allocations**\\n\\nThe basic pattern for resource allocation can be extended for routines that need more than one resource at a time. There are just two more suggestions:\\n\\n- Deallocate resources in the opposite order to that in which you allocate them. That way you won\\'t orphan resources if one resource contains references to another.\\n- When allocating the same set of resources in different places in your code, always allocate them in the same order. This will reduce the possibility of deadlock. (If process A claims resource1 and is about to claim resource2, while process B has claimed resource2 and is trying to get resource1, the two processes will wait forever.)\\n\\nIt doesn\\'t matter what kind of resources we\\'re using—transactions, network connections, memory, files, threads, windows—the basic pattern applies: whoever allocates a resource should be responsible for deallocating it. However, in some languages we can develop the concept further.\\n\\n## **Objects and Exceptions**\\n\\nThe equilibrium between allocations and deallocations is reminiscent of an object-oriented class\\'s constructor and destructor. The class represents a\\n\\nresource, the constructor gives you a particular object of that resource type, and the destructor removes it from your scope.\\n\\nIf you are programming in an object-oriented language, you may find it useful to encapsulate resources in classes. Each time you need a particular resource type, you instantiate an object of that class. When the object goes out of scope, or is reclaimed by the garbage collector, the object\\'s destructor then deallocates the wrapped resource.\\n\\nThis approach has particular benefits when you\\'re working with languages where exceptions can interfere with resource deallocation.\\n\\n## **Balancing and Exceptions**\\n\\nLanguages that support exceptions can make resource deallocation tricky. If an exception is thrown, how do you guarantee that everything allocated prior to the exception is tidied up? The answer depends to some extent on the language support. You generally have two choices:\\n\\n- 1. Use variable scope (for example, stack variables in C++ or Rust)\\n- 2. Use a finally clause in a try…catch block\\n\\nWith usual scoping rules in languages such as C++ or Rust, the variable\\'s memory will be reclaimed when the variable goes out of scope via a return, block exit, or exception. But you can also hook in to the variable\\'s destructor to cleanup any external resources. In this example, the Rust variable named accounts will automatically close the associated file when it goes out of scope:\\n\\n```\\n{\\n let mut accounts = File::open( \"mydata.txt\")?; // >--\\n // use \\'accounts\\' // |\\n ... // |\\n} // <--\\n// \\'accounts\\' is now out of scope, and the file is\\n// automatically closed\\n```\\nThe other option, if the language supports it, is the finally clause. A finally clause will ensure that the specified code will run whether or not an exception was raised in the try…catch block:\\n\\n```\\ntry\\n // some dodgy stuff\\ncatch\\n // exception was raised\\nfinally\\n // clean up in either case\\n```\\nHowever, there is a catch.\\n\\n## **An Exception Antipattern**\\n\\nWe commonly see folks writing something like this:\\n\\n```\\nbegin\\n thing = allocate_resource()\\n process(thing)\\nfinally\\n deallocate(thing)\\nend\\n```\\nCan you see what\\'s wrong?\\n\\nWhat happens if the resource allocation fails and raises an exception? The finally clause will catch it, and try to deallocate a *thing* that was never allocated.\\n\\nThe correct pattern for handling resource deallocation in an environment with exceptions is\\n\\n```\\nthing = allocate_resource()\\nbegin\\n process(thing)\\nfinally\\n deallocate(thing)\\nend\\n```\\n## **When You Can\\'t Balance Resources**\\n\\nThere are times when the basic resource allocation pattern just isn\\'t appropriate. Commonly this is found in programs that use dynamic data structures. One routine will allocate an area of memory and link it into some larger structure, where it may stay for some time.\\n\\nThe trick here is to establish a semantic invariant for memory allocation. You need to decide who is responsible for data in an aggregate data structure. What happens when you deallocate the top-level structure? You have three main options:\\n\\n- The top-level structure is also responsible for freeing any substructures that it contains. These structures then recursively delete data they contain, and so on.\\n- The top-level structure is simply deallocated. Any structures that it pointed to (that are not referenced elsewhere) are orphaned.\\n- The top-level structure refuses to deallocate itself if it contains any substructures.\\n\\nThe choice here depends on the circumstances of each individual data structure. However, you need to make it explicit for each, and implement your decision consistently. Implementing any of these options in a procedural language such as C can be a problem: data structures themselves are not active. Our preference in these circumstances is to write a module for each major structure that provides standard allocation and deallocation facilities for that structure. (This module can also provide facilities such as debug printing, serialization, deserialization, and traversal hooks.)\\n\\n## **Checking the Balance**\\n\\nBecause Pragmatic Programmers trust no one, including ourselves, we feel that it is always a good idea to build code that actually checks that resources are indeed freed appropriately. For most applications, this normally means producing wrappers for each type of resource, and using these wrappers to keep track of all allocations and deallocations. At certain points in your code, the program logic will dictate that the resources will be in a certain state: use the wrappers to check this. For example, a long-running program that services requests will probably have a single point at the top of its main processing loop where it waits for the next request to arrive. This is a good place to ensure that resource usage has not increased since the last execution of the loop.\\n\\nAt a lower, but no less useful level, you can invest in tools that (among other things) check your running programs for memory leaks.\\n\\n## **Related Sections Include**\\n\\n- Topic 24, *Dead Programs Tell No Lies*\\n- Topic 30, *Transforming Programming*\\n- Topic 33, *Breaking Temporal Coupling*\\n\\n## **Challenges**\\n\\n- Although there are no guaranteed ways of ensuring that you always free resources, certain design techniques, when applied consistently, will help. In the text we discussed how establishing a semantic invariant for major data structures could direct memory deallocation decisions. Consider how Topic 23, *Design by Contract*, could help refine this idea.\\n#### **Exercise 17** (possible answer)\\n\\nSome C and C++ developers make a point of setting a pointer to NULL after they deallocate the memory it references. Why is this a good idea?\\n\\n**Exercise 18** (possible answer)\\n\\nSome Java developers make a point of setting an object variable to NULL after they have finished using the object. Why is this a good idea?\\n\\n## **Topic 27** Don\\'t Outrun Your Headlights\\n\\n*It\\'s tough to make predictions, especially about the future.*\\n\\n> *Lawrence \"Yogi\" Berra, after a Danish Proverb*\\n\\nIt\\'s late at night, dark, pouring rain. The two-seater whips around the tight curves of the twisty little mountain roads, barely holding the corners. A hairpin comes up and the car misses it, crashing though the skimpy guardrail\\n\\nand soaring to a fiery crash in the valley below. State troopers arrive on the scene, and the senior officer sadly shakes their head. \"Must have outrun their headlights.\"\\n\\nHad the speeding two-seater been going faster than the speed of light? No, that speed limit is firmly fixed. What the officer referred to was the driver\\'s ability to stop or steer in time in response to the headlight\\'s illumination.\\n\\nHeadlights have a certain limited range, known as the *throw distance*. Past that point, the light spread is too diffuse to be effective. In addition, headlights only project in a straight line, and won\\'t illuminate anything offaxis, such as curves, hills, or dips in the road. According to the National Highway Traffic Safety Administration, the average distance illuminated by low-beam headlights is about 160 feet. Unfortunately, stopping distance at 40mph is 189 feet, and at 70mph a whopping 464 feet.[35] So indeed, it\\'s actually pretty easy to outrun your headlights.\\n\\nIn software development, our \"headlights\" are similarly limited. We can\\'t see too far ahead into the future, and the further off-axis you look, the darker it gets. So Pragmatic Programmers have a firm rule:\\n\\n**Tip 42** Take Small Steps—Always\\n\\nAlways take small, deliberate steps, checking for feedback and adjusting before proceeding. Consider that the rate of feedback is your speed limit. You never take on a step or a task that\\'s \"too big.\"\\n\\nWhat do we mean exactly by feedback? Anything that independently confirms or disproves your action. For example:\\n\\n- Results in a REPL provide feedback on your understanding of APIs and algorithms\\n- Unit tests provide feedback on your last code change\\n- User demo and conversation provide feedback on features and usability\\n\\nWhat\\'s a task that\\'s too big? Any task that requires \"fortune telling.\" Just as the car headlights have limited throw, we can only see into the future perhaps one or two steps, maybe a few hours or days at most. Beyond that, you can quickly get past *educated guess* and into *wild speculation.* You might find yourself slipping into fortune telling when you have to:\\n\\n- Estimate completion dates months in the future\\n- Plan a design for future maintenance or extendability\\n- Guess user\\'s future needs\\n- Guess future tech availability\\n\\nBut, we hear you cry, aren\\'t we supposed to design for future maintenance? Yes, but only to a point: only as far ahead as you can see. The more you have to predict what the future will look like, the more risk you incur that you\\'ll be wrong. Instead of wasting effort designing for an uncertain future, you can always fall back on designing your code to be replaceable. Make it easy to throw out your code and replace it with something better suited. Making code replaceable will also help with cohesion, decoupling, and DRY, leading to a better design overall.\\n\\nEven though you may feel confident of the future, there\\'s always the chance of a black swan around the corner.\\n\\n## **Black Swans**\\n\\nIn his book, *The Black Swan: The Impact of the Highly Improbable* [Tal10], Nassim Nicholas Taleb posits that all significant events in history have come from high-profile, hard-to-predict, and rare events that are beyond the realm of normal expectations. These outliers, while statistically rare, have disproportionate effects. In addition, our own cognitive biases tend to blind us to changes creeping up on the edges of our work (see Topic 4, *Stone Soup and Boiled Frogs*).\\n\\nAround the time of the first edition of *The Pragmatic Programmer*, debate raged in computer magazines and online forums over the burning question: \"Who would win the desktop GUI wars, Motif or OpenLook?\"[36] It was the wrong question. Odds are you\\'ve probably never heard of these technologies as neither \"won\" and the browser-centric web quickly dominated the landscape.\\n\\n## **Tip 43** Avoid Fortune-Telling\\n\\nMuch of the time, tomorrow looks a lot like today. But don\\'t count on it.\\n\\n## **Related Sections Include**\\n\\n- Topic 12, *Tracer Bullets*\\n- Topic 13, *Prototypes and Post-it Notes*\\n- Topic 40, *Refactoring*\\n- Topic 41, *Test to Code*\\n- Topic 48, *The Essence of Agility*\\n- Topic 50, *Coconuts Don\\'t Cut It*\\n\\n#### **Footnotes**\\n\\n[30] Based in part on earlier work by Dijkstra, Floyd, Hoare, Wirth, and others.\\n\\n#### [31]\\n\\nIn C and C++ these are usually implemented as macros. In Java, assertions are disabled by default. Invoke the Java VM with the –enableassertions flag to enable them, and leave them enabled.\\n\\n- [32] http://www.eps.mcgill.ca/jargon/jargon.html#heisenbug\\n- [33] For a discussion of the dangers of coupled code, see Topic 28, *Decoupling*.\\n- [34] See the tip here.\\n- [35] Per the NHTSA, Stopping Distance = Reaction Distance + Braking Distance, assuming an average reaction time of 1.5s and deceleration of 17.02ft/s².\\n- [36] Motif and OpenLook were GUI standards for X-Window based Unix workstations.\\n\\nCopyright © 2020 Pearson Education, Inc.\\n\\n# Chapter 5\\n\\n# **Bend, or Break**\\n\\nLife doesn\\'t stand still. Neither can the code that we write. In order to keep up with today\\'s near-frantic pace of change, we need to make every effort to write code that\\'s as loose—as flexible—as possible. Otherwise we may find our code quickly becoming outdated, or too brittle to fix, and may ultimately be left behind in the mad dash toward the future.\\n\\nBack in Topic 11, *Reversibility* we talked about the perils of irreversible decisions. In this chapter, we\\'ll tell you how to make *reversible* decisions, so your code can stay flexible and adaptable in the face of an uncertain world.\\n\\nFirst we look at *coupling*—the dependencies between bits of code. Topic 28, *Decoupling* shows how to keep separate concepts separate, decreasing coupling.\\n\\nNext, we\\'ll look at different techniques you can use when Topic 29, *Juggling the Real World*. We\\'ll examine four different strategies to help manage and react to events—a critical aspect of modern software applications.\\n\\nTraditional procedural and object-oriented code might be too tightly coupled for your purposes. In Topic 30, *Transforming Programming*, we\\'ll take advantage of the more flexible and clearer style offered by function pipelines, even if your language doesn\\'t support them directly.\\n\\nCommon object-oriented style can tempt you with another trap. Don\\'t fall for it, or you\\'ll end up paying a hefty Topic 31, *Inheritance Tax*. We\\'ll explore better alternatives to keep your code flexible and easier to change.\\n\\nAnd of course a good way to stay flexible is to write *less* code. Changing code leaves you open to the possibility of introducing new bugs. Topic 32, *Configuration* will explain how to move details out of the code completely, where they can be changed more safely and easily.\\n\\nAll these techniques will help you write code that bends and doesn\\'t break.\\n\\n## **Topic 28** Decoupling\\n\\n*When we try to pick out anything by itself, we find it hitched to everything else in the Universe.*\\n\\n> *John Muir, My First Summer in the Sierra*\\n\\nIn Topic 8, *The Essence of Good Design* we claim that using good design principles will make the code you write easy to change. Coupling is the enemy of change, because it links together things that must change in parallel. This makes change more\\n\\ndifficult: either you spend time tracking down all the parts that need changing, or you spend time wondering why things broke when you changed \"just one thing\" and not the other things to which it was coupled.\\n\\nWhen you are designing something you want to be rigid, a bridge or a tower perhaps, you couple the components together:\\n\\n![](_page_207_Picture_7.jpeg)\\n\\nThe links work together to make the structure rigid.\\n\\nCompare that with something like this:\\n\\n![](_page_207_Picture_10.jpeg)\\n\\nHere there\\'s no structural rigidity: individual links can change and others just accommodate it.\\n\\nWhen you\\'re designing bridges, you want them to hold their shape; you need them to be rigid. But when you\\'re designing software that you\\'ll want to change, you want exactly the opposite: you want it to be flexible. And to be flexible, individual components should be coupled to as few other components as possible.\\n\\nAnd, to make matters worse, coupling is transitive: if A is coupled to B and C, and B is coupled to M and N, and C to X and Y, then A is actually coupled to B, C, M, N, X, and Y.\\n\\nThis means there\\'s a simple principle you should follow:\\n\\n- **Tip 44** Decoupled Code Is Easier to Change\\nGiven that we don\\'t normally code using steel beams and rivets, just what does it mean to decouple code? In this section we\\'ll talk about:\\n\\n- Train wrecks—chains of method calls\\n- Globalization—the dangers of static things\\n- Inheritance—why subclassing is dangerous\\n\\nTo some extent this list is artificial: coupling can occur just about any time two pieces of code share something, so as you read what follows keep an eye out for the underlying patterns so you can apply them to *your* code. And keep a lookout for some of the symptoms of coupling:\\n\\n- Wacky dependencies between unrelated modules or libraries.\\n- \"Simple\" changes to one module that propagate through unrelated modules in the system or break stuff elsewhere in the system.\\n- Developers who are afraid to change code because they aren\\'t sure what might be affected.\\n- Meetings where everyone has to attend because no one is sure who will be affected by a change.\\n\\n## **Train Wrecks**\\n\\nWe\\'ve all seen (and probably written) code like this:\\n\\n```\\npublic void applyDiscount(customer, order_id, discount) {\\n totals = customer\\n .orders\\n .find(order_id)\\n .getTotals();\\n totals.grandTotal = totals.grandTotal - discount;\\n totals.discount = discount;\\n}\\n```\\nWe\\'re getting a reference to some orders from a customer object, using that to find a particular order, and then getting the set of totals for the order. Using those totals, we subtract the discount from the order grand total and also update them with that discount.\\n\\nThis chunk of code is traversing five levels of abstraction, from customer to total amounts. Ultimately our top-level code has to know that a customer object exposes orders, that the orders have a find method that takes an order id and returns an order, and that the order object has a totals object which has getters and setters for grand totals and discounts. That\\'s a lot of implicit knowledge. But worse, that\\'s a lot of things that *cannot change in the future* if this code is to continue to work. All the cars in a train are coupled together, as are all the methods and attributes in a train wreck.\\n\\nLet\\'s imagine that the business decides that no order can have a discount of more than 40%. Where would we put the code that enforces that rule?\\n\\nYou might say it belongs in the applyDiscount function we just wrote. That\\'s certainly part of the answer. But with the code the way it is now, you can\\'t know that this is the *whole* answer. Any piece of code, anywhere, could set fields in the totals object, and if the maintainer of that code didn\\'t get the memo, it wouldn\\'t be checking against the new policy.\\n\\nOne way to look at this is to think about responsibilities. Surely the totals object should be responsible for managing the totals. And yet it isn\\'t: it\\'s really just a container for a bunch of fields that anyone can query and update.\\n\\nThe fix for that is to apply something we call:\\n\\n![](_page_210_Picture_3.jpeg)\\n\\nThis principle says that you shouldn\\'t make decisions based on the internal state of an object and then update that object. Doing so totally destroys the benefits of encapsulation and, in doing so, spreads the knowledge of the implementation throughout the code. So the first fix for our train wreck is to delegate the discounting to the total object:\\n\\n```\\npublic void applyDiscount(customer, order_id, discount) {\\n customer\\n .orders\\n .find(order_id)\\n .getTotals()\\n .applyDiscount(discount);\\n}\\n```\\nWe have the same kind of *tell-don\\'t-ask* (TDA) issue with the customer object and its orders: we shouldn\\'t fetch its list of orders and search them. We should instead get the order we want directly from the customer:\\n\\n```\\npublic void applyDiscount(customer, order_id, discount) {\\n customer\\n```\\n\\n```\\n .findOrder(order_id)\\n .getTotals()\\n .applyDiscount(discount);\\n}\\n```\\nThe same thing applies to our order object and its totals. Why should the outside world have to know that the implementation of an order uses a separate object to store its totals?\\n\\n```\\npublic void applyDiscount(customer, order_id, discount) {\\n customer\\n .findOrder(order_id)\\n .applyDiscount(discount);\\n}\\n```\\nAnd this is where we\\'d probably stop.\\n\\nAt this point you might be thinking that TDA would make us add an applyDiscountToOrder(order_id) method to customers. And, if followed slavishly, it would.\\n\\nBut TDA is not a law of nature; it\\'s just a pattern to help us recognize problems. In this case, we\\'re comfortable exposing the fact that a customer has orders, and that we can find one of those orders by asking the customer object for it. This is a pragmatic decision.\\n\\nIn every application there are certain top-level concepts that are universal. In this application, those concepts include *customers* and *orders*. It makes no sense to hide orders totally inside customer objects: they have an existence of their own. So we have no problem creating APIs that expose order objects.\\n\\n## **The Law of Demeter**\\n\\nPeople often talk about something called the *Law of Demeter*, or LoD, in relation to coupling. The LoD is a set of guidelines[37] written in the late \\'80s by Ian Holland. He created them to help developers on the Demeter Project keep their functions cleaner and decoupled.\\n\\nThe LoD says that a method defined in a class *C* should only call:\\n\\n- Other instance methods in C\\n- Its parameters\\n- Methods in objects that it creates, both on the stack and in the heap\\n- Global variables\\n\\nIn the first edition of this book we spent some time describing the LoD. In the intervening 20 years the bloom has faded on that particular rose. We now don\\'t like the \"global variable\" clause (for reasons we\\'ll go into in the next section). We also discovered that it\\'s difficult to use this in practice: it\\'s a little like having to parse a legal document whenever you call a method.\\n\\nHowever, the principle is still sound. We just recommend a somewhat simpler way of expressing almost the same thing:\\n\\n- **Tip 46** Don\\'t Chain Method Calls\\nTry not to have more than one \".\" when you access something. And *access something* also covers cases where you use intermediate variables, as in the following code:\\n\\n```\\n# This is pretty poor style\\namount = customer.orders.last().totals().amount;\\n# and so is this…\\norders = customer.orders;\\nlast = orders.last();\\ntotals = last.totals();\\namount = totals.amount;\\n```\\nThere\\'s a big exception to the one-dot rule: the rule doesn\\'t apply if the things you\\'re chaining are really, really unlikely to change. In practice, anything in your application should be considered likely to change. Anything in a third-party library should be considered volatile, particularly if the maintainers of that library are known to change APIs between releases. Libraries that come with the language, however, are probably pretty stable, and so we\\'d be happy with code such as:\\n\\n```\\npeople\\n.sort_by {|person| person.age }\\n.first(10)\\n.map {| person | person.name }\\n```\\nThat Ruby code worked when we wrote the first edition, 20 years ago, and will likely still work when we enter the home for old programmers (any day now…).\\n\\n## **Chains and Pipelines**\\n\\nIn Topic 30, *Transforming Programming* we talk about composing functions into pipelines. These pipelines transform data, passing it from one function to the next. This is not the same as a train wreck of method calls, as we are not relying on hidden implementation details.\\n\\nThat\\'s not to say that pipelines don\\'t introduce some coupling: they do. The format of the data returned by one function in a pipeline must be compatible with the format accepted by the next.\\n\\nOur experience is that this form of coupling is far less a barrier to changing the code than the form introduced by train wrecks.\\n\\n## **The Evils of Globalization**\\n\\nGlobally accessible data is an insidious source of coupling between application components. Each piece of global data acts as if every method in your application suddenly gained an additional parameter: after all, that global data is available inside *every* method.\\n\\nGlobals couple code for many reasons. The most obvious is that a change to the implementation of the global potentially affects all the code in the system. In practice, of course, the impact is fairly limited; the problem\\n\\nreally comes down to knowing that you\\'ve found every place you need to change.\\n\\nGlobal data also creates coupling when it comes to teasing your code apart.\\n\\nMuch has been made of the benefits of code reuse. Our experience has been that reuse should probably not be a primary concern when creating code, but the thinking that goes into making code reusable should be part of your coding routine. When you make code reusable, you give it clean interfaces, decoupling it from the rest of your code. This allows you to extract a method or module without dragging everything else along with it. And if your code uses global data, then it becomes difficult to split it out from the rest.\\n\\nYou\\'ll see this problem when you\\'re writing unit tests for code that uses global data. You\\'ll find yourself writing a bunch of setup code to create a global environment just to allow your test to run.\\n\\n**Tip 47** Avoid Global Data\\n\\n## **Global Data Includes Singletons**\\n\\nIn the previous section we were careful to talk about *global data* and not *global variables*. That\\'s because people often tell us \"Look! No global variables. I wrapped it all as instance data in a singleton object or global module.\"\\n\\nTry again, Skippy. If all you have is a singleton with a bunch of exported instance variables, then it\\'s still just global data. It just has a longer name.\\n\\nSo then folks take this singleton and hide all the data behind methods. Instead of coding Config.log_level they now say Config.log_level() or Config.getLogLevel(). This is better, because it means that your global data has a bit of intelligence behind it. If you decide to change the representation of\\n\\nlog levels, you can maintain compatibility by mapping between the new and old in the Config API. But you still have only the one set of configuration data.\\n\\n## **Global Data Includes External Resources**\\n\\nAny mutable external resource is global data. If your application uses a database, datastore, file system, service API, and so on, it risks falling into the globalization trap. Again, the solution is to make sure you always wrap these resources behind code that you control.\\n\\n**Tip 48** If It\\'s Important Enough to Be Global, Wrap It in an API\\n\\n## **Inheritance Adds Coupling**\\n\\nThe misuse of subclassing, where a class inherits state and behavior from another class, is so important that we discuss it in its own section, Topic 31, *Inheritance Tax*.\\n\\n## **Again, It\\'s All About Change**\\n\\nCoupled code is hard to change: alterations in one place can have secondary effects elsewhere in the code, and often in hard-to-find places that only come to light a month later in production.\\n\\nKeeping your code shy: having it only deal with things it directly knows about, will help keep your applications decoupled, and that will make them more amenable to change.\\n\\n## **Related Sections Include**\\n\\n- Topic 8, *The Essence of Good Design*\\n- Topic 9, *DRY—The Evils of Duplication*\\n- Topic 10, *Orthogonality*\\n- Topic 11, *Reversibility*\\n- Topic 29, *Juggling the Real World*\\n- Topic 30, *Transforming Programming*\\n- Topic 31, *Inheritance Tax*\\n- Topic 32, *Configuration*\\n- Topic 33, *Breaking Temporal Coupling*\\n- Topic 34, *Shared State Is Incorrect State*\\n- Topic 35, *Actors and Processes*\\n- Topic 36, *Blackboards*\\n- We discuss *Tell, Don\\'t Ask* in our 2003 Software Construction article *The Art of Enbugging*. [38]\\n\\n*Things don\\'t just happen; they are made to happen.*\\n\\n*John F. Kennedy*\\n\\nIn the old days, when your authors still had their boyish good looks, computers were not particularly flexible. We\\'d typically organize the way we interacted with them based on their limitations.\\n\\nToday, we expect more: computers have to integrate into *our* world, not the other way around. And our world is messy: things are constantly happening, stuff gets moved around, we change our minds, …. And the applications we write somehow have to work out what to do.\\n\\nThis section is all about writing these responsive applications.\\n\\nWe\\'ll start off with the concept of an *event*.\\n\\n## **Events**\\n\\nAn *event* represents the availability of information. It might come from the outside world: a user clicking a button, or a stock quote update. It might be internal: the result of a calculation is ready, a search finishes. It can even be something as trivial as fetching the next element in a list.\\n\\nWhatever the source, if we write applications that respond to events, and adjust what they do based on those events, those applications will work better in the real world. Their users will find them to be more interactive, and the applications themselves will make better use of resources.\\n\\nBut how can we write these kinds of applications? Without some kind of strategy, we\\'ll quickly find ourselves confused, and our applications will be a mess of tightly coupled code.\\n\\nLet\\'s look at four strategies that help.\\n\\n- 1. Finite State Machines\\n- 2. The Observer Pattern\\n- 3. Publish/Subscribe\\n- 4. Reactive Programming and Streams\\n\\n## **Finite State Machines**\\n\\nDave finds that he writes code using a Finite State Machine (FSM) just about every week. Quite often, the FSM implementation will be just a couple of lines of code, but those few lines help untangle a whole lot of potential mess.\\n\\nUsing an FSM is trivially easy, and yet many developers shy away from them. There seems to be a belief that they are difficult, or that they only apply if you\\'re working with hardware, or that you need to use some hardto-understand library. None of these are true.\\n\\n### **The Anatomy of a Pragmatic FSM**\\n\\nA state machine is basically just a specification of how to handle events. It consists of a set of states, one of which is the *current state*. For each state, we list the events that are significant to that state. For each of those events, we define the new current state of the system.\\n\\nFor example, we may be receiving multipart messages from a websocket. The first message is a header. This is followed by any number of data messages, followed by a trailing message. This could be represented as an FSM like this:\\n\\n![](_page_219_Figure_0.jpeg)\\n\\nWe start in the \"Initial state.\" If we receive a header message, we *transition* to the \"Reading message\" state. If we receive anything else while we\\'re in the initial state (the line labeled with an asterisk) we transition to the \"Error\" state and we\\'re done.\\n\\nWhile we\\'re in the \"Reading message\" state, we can accept either data messages, in which case we continue reading in the same state, or we can accept a trailer message, which transitions us to the \"Done\" state. Anything else causes a transition to the error state.\\n\\nThe neat thing about FSMs is that we can express them purely as data. Here\\'s a table representing our message parser:\\n\\n| State | Events |  |  |  |\\n| --- | --- | --- | --- | --- |\\n|  | Header | Data | Trailer | Other |\\n| Initial | Reading | Error | Error | Error |\\n| Reading | Error | Reading | Done | Error |\\n\\nThe rows in the table represent the states. To find out what to do when an event occurs, look up the row for the current state, scan along for the column representing the event, the contents of that cell are the new state.\\n\\nThe code that handles it is equally simple:\\n\\n```\\nevent/simple_fsm.rb\\n1:  TRANSITIONS = {\\n-  initial: { header: :reading},\\n-  reading: { data: :reading, trailer: :done},\\n-  }\\n```\\n\\n```\\n5: \\n -  state = :initial\\n - \\n -  while state != :done && state != :error\\n -  msg = get_next_message()\\n10:\\n     state = TRANSITIONS[state][msg.msg_type] || :error\\n -  end\\n```\\nThe code that implements the transitions between states is on line 10. It indexes the transition table using the current state, and then indexes the transitions for that state using the message type. If there is no matching new state, it sets the state to :error.\\n\\n## **Adding Actions**\\n\\nA pure FSM, such as the one we were just looking at, is an event stream parser. Its only output is the final state. We can beef it up by adding actions that are triggered on certain transitions.\\n\\nFor example, we might need to extract all of the strings in a source file. A string is text between quotes, but a backslash in a string escapes the next character, so \"Ignore \\\\\"quotes\\\\\"\" is a single string. Here\\'s an FSM that does this:\\n\\n![](_page_220_Figure_5.jpeg)\\n\\nThis time, each transition has two labels. The top one is the event that triggers it, and the bottom one is the action to take as we move between states.\\n\\nWe\\'ll express this in a table, as we did last time. However, in this case each entry in the table is a two-element list containing the next state and the name of an action:\\n\\n```\\nevent/strings_fsm.rb\\n   TRANSITIONS = {\\n    # current new state action to take\\n    #---------------------------------------------------------\\n    look_for_string: {\\n    \\'\"\\' => [ :in_string, :start_new_string ],\\n    :default => [ :look_for_string, :ignore ],\\n    },\\n    in_string: {\\n    \\'\"\\' => [ :look_for_string, :finish_current_string ],\\n    \\'\\\\\\\\\\' => [ :copy_next_char, :add_current_to_string ],\\n    :default => [ :in_string, :add_current_to_string ],\\n    },\\n    copy_next_char: {\\n    :default => [ :in_string, :add_current_to_string ],\\n    },\\n   }\\n```\\nWe\\'ve also added the ability to specify a default transition, taken if the event doesn\\'t match any of the other transitions for this state.\\n\\nNow let\\'s look at the code:\\n\\n```\\nevent/strings_fsm.rb\\n   state = :look_for_string\\n   result = []\\n   while ch = STDIN.getc\\n    state, action = TRANSITIONS[state][ch] || TRANSITIONS[state][ :default]\\n    case action\\n    when :ignore\\n```\\n\\n```\\n when :start_new_string\\n result = []\\n when :add_current_to_string\\n result << ch\\n when :finish_current_string\\n puts result.join\\n end\\nend\\n```\\nThis is similar to the previous example, in that we loop through the events (the characters in the input), triggering transitions. But it does more than the previous code. The result of each transition is both a new state and the name of an action. We use the action name to select the code to run before we go back around the loop.\\n\\nThis code is very basic, but it gets the job done. There are many other variants: the transition table could use anonymous functions or function pointers for the actions, you could wrap the code that implements the state machine in a separate class, with its own state, and so on.\\n\\nThere\\'s nothing to say that you have to process all the state transitions at the same time. If you\\'re going through the steps to sign up a user on your app, there\\'s likely to be a number of transitions as they enter their details, validate their email, agree to the 107 different legislated warnings that online apps must now give, and so on. Keeping the state in external storage, and using it to drive a state machine, is a great way to handle these kind of workflow requirements.\\n\\n#### **State Machines Are a Start**\\n\\nState machines are underused by developers, and we\\'d like to encourage you to look for opportunities to apply them. But they don\\'t solve all the problems associated with events. So let\\'s move on to some other ways of looking at the problems of juggling events.\\n\\n## **The Observer Pattern**\\n\\nIn the *observer pattern* we have a source of events, called the *observable* and a list of clients, the *observers*, who are interested in those events.\\n\\nAn observer registers its interest with the observable, typically by passing a reference to a function to be called. Subsequently, when the event occurs, the observable iterates down its list of observers and calls the function that each passed it. The event is given as a parameter to that call.\\n\\nHere\\'s a simple example in Ruby. The Terminator module is used to terminate the application. Before it does so, however, it notifies all its observers that the application is going to exit.[39] They might use this notification to tidy up temporary resources, commit data, and so on:\\n\\n```\\nevent/observer.rb\\n```\\n\\n```\\nmodule Terminator\\n CALLBACKS = []\\n def self.register(callback)\\n CALLBACKS << callback\\n end\\n def self.exit(exit_status)\\n CALLBACKS.each { |callback| callback.(exit_status) }\\n exit!(exit_status)\\n end\\nend\\nTerminator.register(-> (status) { puts \"callback 1 sees #{status }\" })\\nTerminator.register(-> (status) { puts \"callback 2 sees #{status }\" })\\nTerminator.exit(99)\\n$ ruby event/observer.rb\\ncallback 1 sees 99\\ncallback 2 sees 99\\n```\\nThere\\'s not much code involved in creating an observable: you push a function reference onto a list, and then call those functions when the event occurs. This is a good example of when *not* to use a library.\\n\\nThe observer/observable pattern has been used for decades, and it has served us well. It is particularly prevalent in user interface systems, where the callbacks are used to inform the application that some interaction has occurred.\\n\\nBut the observer pattern has a problem: because each of the observers has to register with the observable, it introduces coupling. In addition, because in the typical implementation the callbacks are handled inline by the observable, synchronously, it can introduce performance bottlenecks.\\n\\nThis is solved by the next strategy, Publish/Subscribe.\\n\\n## **Publish/Subscribe**\\n\\nPublish/Subscribe (pubsub) generalizes the observer pattern, at the same time solving the problems of coupling and performance.\\n\\nIn the pubsub model, we have *publishers* and *subscribers*. These are connected via channels. The channels are implemented in a separate body of code: sometimes a library, sometimes a process, and sometimes a distributed infrastructure. All this implementation detail is hidden from your code.\\n\\nEvery channel has a name. Subscribers register interest in one or more of these named channels, and publishers write events to them. Unlike the observer pattern, the communication between the publisher and subscriber is handled outside your code, and is potentially asynchronous.\\n\\nAlthough you could implement a very basic pubsub system yourself, you probably don\\'t want to. Most cloud service providers have pubsub offerings, allowing you to connect applications around the world. Every popular language will have at least one pubsub library.\\n\\nPubsub is a good technology for decoupling the handling of asynchronous events. It allows code to be added and replaced, potentially while the application is running, without altering existing code. The downside is that it can be hard to see what is going on in a system that uses pubsub heavily: you can\\'t look at a publisher and immediately see which subscribers are involved with a particular message.\\n\\nCompared to the observer pattern, pubsub is a great example of reducing coupling by abstracting up through a shared interface (the channel). However, it is still basically just a message passing system. Creating systems that respond to combinations of events will need more than this, so let\\'s look at ways we can add a time dimension to event processing.\\n\\n## **Reactive Programming, Streams, and Events**\\n\\nIf you\\'ve ever used a spreadsheet, then you\\'ll be familiar with *reactive programming*. If a cell contains a formula which refers to a second cell, then updating that second cell causes the first to update as well. The values *react* as the values they use change.\\n\\nThere are many frameworks that can help with this kind of data-level reactivity: in the realm of the browser React and Vue.js are current favorites (but, this being JavaScript, this information will be out-of-date before this book is even printed).\\n\\nIt\\'s clear that events can also be used to trigger reactions in code, but it isn\\'t necessarily easy to plumb them in. That\\'s where *streams* come in.\\n\\nStreams let us treat events as if they were a collection of data. It\\'s as if we had a list of events, which got longer when new events arrive. The beauty of that is that we can treat streams just like any other collection: we can manipulate, combine, filter, and do all the other data-ish things we know so well. We can even combine event streams and regular collections. And streams can be asynchronous, which means your code gets the opportunity to respond to events as they arrive.\\n\\nThe current *de facto* baseline for reactive event handling is defined on the site http://reactivex.io, which defines a language-agnostic set of principles and documents some common implementations. Here we\\'ll use the RxJs library for JavaScript.\\n\\nOur first example takes two streams and zips them together: the result is a new stream where each element contains one item from the first input stream and one item from the other. In this case, the first stream is simply a list of five animal names. The second stream is more interesting: it\\'s an interval timer which generates an event every 500ms. Because the streams are zipped together, a result is only generated when data is available on both, and so our result stream only emits a value every half second:\\n\\nevent/rx0/index.js\\n\\n```\\nimport * as Observable from \\'rxjs\\'\\nimport { logValues } from \"../rxcommon/logger.js\"\\nlet animals = Observable. of( \"ant\", \"bee\", \"cat\", \"dog\", \"elk\")\\nlet ticker = Observable.interval(500)\\nlet combined = Observable.zip(animals, ticker)\\ncombined.subscribe(next => logValues(JSON.stringify(next)))\\n```\\nThis code uses a simple logging function[40] which adds items to a list in the browser window. Each item is timestamped with the time in milliseconds since the program started to run. Here\\'s what it shows for our code:\\n\\n| 502 ms |\\n| --- |\\n| [\"ant\",0] |\\n| 1002 ms |\\n| [\"bee\",1] |\\n| 1502 ms |\\n| [\"cat\",2] |\\n| 2002 ms |\\n| [\"dog\",3] |\\n| 2502 ms |\\n| [\"elk\",4] |\\n\\nNotice the timestamps: we\\'re getting one event from the stream every 500ms. Each event contains a serial number (created by the interval observable) and the name of the next animal from the list. Watching it live in a browser, the log lines appear at every half second.\\n\\nEvent streams are normally populated as events occur, which implies that the observables that populate them can run in parallel. Here\\'s an example that fetches information about users from a remote site. For this we\\'ll use https://reqres.in, a public site that provides an open REST interface. As part of its API, we can fetch data on a particular (fake) user by performing a GET request to users/«id». Our code fetches the users with the IDs 3, 2, and 1:\\n\\n```\\nevent/rx1/index.js\\n```\\n\\n```\\nimport * as Observable from \\'rxjs\\'\\nimport { mergeMap } from \\'rxjs/operators\\'\\nimport { ajax } from \\'rxjs/ajax\\'\\nimport { logValues } from \"../rxcommon/logger.js\"\\nlet users = Observable. of(3, 2, 1)\\nlet result = users.pipe(\\n mergeMap((user) => ajax.getJSON( `https://reqres.in/api/users/${user} `))\\n)\\nresult.subscribe(\\n```\\n\\n```\\n resp => logValues(JSON.stringify(resp.data)),\\n err => console.error(JSON.stringify(err))\\n)\\n```\\nThe internal details of the code are not too important. What\\'s exciting is the result, shown in the following screenshot:\\n\\nLook at the timestamps: the three requests, or three separate streams, were processed in parallel, The first to come back, for id 2, took 82ms, and the next two came back 50 and 51ms later.\\n\\n## **Streams of Events Are Asynchronous Collections**\\n\\nIn the previous example, our list of user IDs (in the observable users) was static. But it doesn\\'t have to be. Perhaps we want to collect this information when people log in to our site. All we have to do is to generate an observable event containing their user ID when their session is created, and use that observable instead of the static one. We\\'d then be fetching details about the users as we received these IDs, and presumably storing them somewhere.\\n\\nThis is a very powerful abstraction: we no longer need to think about time as being something we have to manage. Event streams unify synchronous and asynchronous processing behind a common, convenient API.\\n\\n## **Events Are Ubiquitous**\\n\\nEvents are everywhere. Some are obvious: a button click, a timer expiring. Other are less so: someone logging in, a line in a file matching a pattern. But whatever their source, code that\\'s crafted around events can be more responsive and better decoupled than its more linear counterpart.\\n\\n## **Related Sections Include**\\n\\n- Topic 28, *Decoupling*\\n- Topic 36, *Blackboards*\\n\\n## **Exercises**\\n\\n## **Exercise 19** (possible answer)\\n\\nIn the FSM section we mentioned that you could move the generic state machine implementation into its own class. That class would probably be initialized by passing in a table of transitions and an initial state.\\n\\nTry implementing the string extractor that way.\\n\\n## **Exercise 20** (possible answer)\\n\\nWhich of these technologies (perhaps in combination) would be a good fit for the following situations:\\n\\n- If you receive three *network interface down* events within five minutes, notify the operations staff.\\n- If it is after sunset, and there is motion detected at the bottom of the stairs followed by motion detected at the top of the stairs, turn on the upstairs lights.\\n- You want to notify various reporting systems that an order was completed.\\n- In order to determine whether a customer qualifies for a car loan, the application needs to send requests to three backend services and wait for the responses.\\n\\n## **Topic 30** Transforming Programming\\n\\n*If you can\\'t describe what you are doing as a process, you don\\'t know what you\\'re doing.*\\n\\n*W. Edwards Deming, (attr)*\\n\\nAll programs transform data, converting an input into an output. And yet when we think about design, we rarely think about creating transformations. Instead we worry about classes and modules, data structures and algorithms, languages\\n\\nand frameworks.\\n\\nWe think that this focus on code often misses the point: we need to get back to thinking of programs as being something that transforms inputs into outputs. When we do, many of the details we previously worried about just evaporate. The structure becomes clearer, the error handling more consistent, and the coupling drops way down.\\n\\nTo start our investigation, let\\'s take the time machine back to the 1970s and ask a Unix programmer to write us a program that lists the five longest files in a directory tree, where longest means \"having the largest number of lines.\"\\n\\nYou might expect them to reach for an editor and start typing in C. But they wouldn\\'t, because they are thinking about this in terms of what we have (a directory tree) and what we want (a list of files). Then they\\'d go to a terminal and type something like:\\n\\n```\\n$ find . -type f | xargs wc -l | sort -n | tail -5\\n```\\nThis is a series of transformations:\\n\\n*find . -type f*\\n\\nWrite a list of all the files (-type f) in or below the current directory (.) to standard output.\\n\\n*xargs wc -l*\\n\\nRead lines from standard input and arrange for them all to be passed as arguments to the command wc -l. The wc program with the -l option counts the number of lines in each of its arguments and writes each result as \"count filename\" to standard output.\\n\\n*sort -n*\\n\\nSort standard input assuming each line starts with a number (-n), writing the result to standard output.\\n\\n#### *tail -5*\\n\\nRead standard input and write just the last five lines to standard output.\\n\\nRun this in our book\\'s directory and we get\\n\\n```\\n 470 ./test_to_build.pml\\n 487 ./dbc.pml\\n 719 ./domain_languages.pml\\n 727 ./dry.pml\\n9561 total\\n```\\nThat last line is the total number of lines in all the files (not just those shown), because that\\'s what wc does. We can strip it off by requesting one more line from tail, and then ignoring the last line:\\n\\n```\\n$ find . -type f | xargs wc -l | sort -n | tail -6 | head -5\\n 470 ./debug.pml\\n 470 ./test_to_build.pml\\n 487 ./dbc.pml\\n 719 ./domain_languages.pml\\n 727 ./dry.pml\\n```\\n![](_page_232_Figure_0.jpeg)\\n\\nLet\\'s look at this in terms of the data that flows between the individual steps. Our original requirement, \"top 5 files in terms of lines,\" becomes a series of transformations (also show in the figure).\\n\\ndirectory name\\n\\n```\\n→ list of files\\n```\\n- → list with line numbers\\n- → sorted list\\n- → highest five + total\\n- → highest five\\n\\nIt\\'s almost like an industrial assembly line: feed raw data in one end and the finished product (information) comes out the other.\\n\\nAnd we like to think about all code this way.\\n\\n**Tip 49** Programming Is About Code, But Programs Are About Data\\n\\n## **Finding Transformations**\\n\\nSometimes the easiest way to find the transformations is to start with the requirement and determine its inputs and outputs. Now you\\'ve defined the function representing the overall program. You can then find steps that lead you from input to output. This is a *top-down* approach.\\n\\nFor example, you want to create a website for folks playing word games that finds all the words that can be made from a set of letters. Your input here is a set of letters, and your output is a list of three-letter words, four-letter words, and so on:\\n\\n\"lvyin\" is transformed to → 3 => ivy, lin, nil, yin 4 => inly, liny, viny 5 => vinyl\\n\\n(Yes, they are all words, at least according to the macOS dictionary.)\\n\\nThe trick behind the overall application is simple: we have a dictionary which groups words by a *signature,* chosen so that all words containing the same letters will have the same signature. The simplest signature function is just the sorted list of letters in the word. We can then look up an input string by generating a signature for it, and then seeing which words (if any) in the dictionary have that same signature.\\n\\nThus the *anagram finder* breaks down into four separate transformations:\\n\\n| Step | Transformation | Sample data |\\n| --- | --- | --- |\\n| Step 0: | Initial input | \"ylvin\" |\\n| Step 1: | All combinations of three or more | vin, viy, vil, vny, vnl, vyl, iny, inl, iyl, nyl, |\\n|  | letters | viny, vinl, viyl, vnyl, inyl, vinyl |\\n| Step 2: | Signatures of the combinations | inv, ivy, ilv, nvy, lnv, lvy, iny, iln, ily, lny, invy, |\\n|  |  | ilnv, ilvy, lnvy, ilny, ilnvy |\\n| Step 3: | List of all dictionary words which | ivy, yin, nil, lin, viny, liny, inly, vinyl |\\n|  | match any of the signatures |  |\\n\\n| Step | Transformation | Sample data |\\n| --- | --- | --- |\\n| Step 4: | Words grouped by length | 3 => ivy, lin, nil, yin |\\n|  |  | 4 => inly, liny, viny |\\n|  |  | 5 => vinyl |\\n\\n#### **Transformations All the Way Down**\\n\\nLet\\'s start by looking at step 1, which takes a word and creates a list of all combinations of three or more letters. This step can itself be expressed as a list of transformations:\\n\\n| Step | Transformation | Sample data |\\n| --- | --- | --- |\\n| Step | Initial input | \"vinyl\" |\\n| 1.0: |  |  |\\n| Step | Convert to characters | v, i, n, y, l |\\n| 1.1: |  |  |\\n| Step | Get all subsets | [], [v], [i], … [v,i], [v,n], [v,y], … [v,i,n], [v,i,y], … |\\n| 1.2: |  | [v,n,y,l], [i,n,y,l], [v,i,n,y,l] |\\n| Step | Only those longer than three | [v,i,n], [v,i,y], … [i,n,y,l], [v,i,n,y,l] |\\n| 1.3: | characters |  |\\n| Step | Convert back to strings | [vin,viy, … inyl,vinyl] |\\n| 1.4: |  |  |\\n\\nWe\\'ve now reached the point where we can easily implement each transformation in code (using Elixir in this case):\\n\\n```\\nfunction-pipelines/anagrams/lib/anagrams.ex\\n```\\n\\n```\\ndefp all_subsets_longer_than_three_characters(word) do\\n word\\n |> String.codepoints()\\n |> Comb.subsets()\\n |> Stream.filter( fn subset -> length(subset) >= 3 end)\\n |> Stream.map(&List.to_string(&1))\\nend\\n```\\n#### **What\\'s with the |> Operator?**\\n\\nElixir, along with many other functional languages, has a pipeline operator, sometimes called a *forward pipe* or just a *pipe*. [41] All it does is take the\\n\\nvalue on its left and insert it as the first parameter of the function on its right, so\\n\\n*\"vinyl\"* |> String.codepoints |> Comb.subsets()\\n\\nis the same as writing\\n\\nComb.subsets(String.codepoints( *\"vinyl\"*))\\n\\n(Other languages may inject this piped value as the *last* parameter of the next function—it largely depends on the style of the built-in libraries.)\\n\\nYou might think that this is just syntactic sugar. But in a very real way the pipeline operator is a revolutionary opportunity to think differently. Using a pipeline means that you\\'re automatically thinking in terms of transforming data; each time you see |> you\\'re actually seeing a place where data is flowing between one transformation and the next.\\n\\nMany languages have something similar: Elm, and F# have |>, Clojure has -> and ->> (which work a little differently), R has %>%. Haskell both has pipe operators and makes it easy to declare new ones. As we write this, there\\'s talk of adding |> to JavaScript.\\n\\nIf your current language supports something similar, you\\'re in luck. If it doesn\\'t, see *Language X Doesn\\'t Have Pipelines*.\\n\\nAnyway, back to the code.\\n\\n#### **Keep on Transforming…**\\n\\nNow look at *Step 2* of the main program, where we convert the subsets into signatures. Again, it\\'s a simple transformation—a list of subsets becomes a list of signatures:\\n\\n| Step | Transformation | Sample data |\\n| --- | --- | --- |\\n| Step 2.0: | initial input | vin, viy, … inyl, vinyl |\\n| Step 2.1: | convert to signatures | inv, ivy … ilny, inlvy |\\n\\nThe Elixir code in the following listing is just as simple:\\n\\n```\\nfunction-pipelines/anagrams/lib/anagrams.ex\\n   defp as_unique_signatures(subsets) do\\n    subsets\\n    |> Stream.map(&Dictionary.signature_of/1)\\n   end\\n```\\nNow we transform that list of signatures: each signature gets mapped to the list of known words with the same signature, or nil if there are no such words. We then have to remove the nils and flatten the nested lists into a single level:\\n\\n```\\nfunction-pipelines/anagrams/lib/anagrams.ex\\n```\\n\\n```\\ndefp find_in_dictionary(signatures) do\\n signatures\\n |> Stream.map(&Dictionary.lookup_by_signature/1)\\n |> Stream.reject(&is_nil/1)\\n |> Stream.concat(&(&1))\\nend\\n```\\nStep 4, grouping the words by length, is another simple transformation, converting our list into a map where the keys are the lengths, and the values are all words with that length:\\n\\n```\\nfunction-pipelines/anagrams/lib/anagrams.ex\\n```\\n\\n```\\ndefp group_by_length(words) do\\n words\\n |> Enum.sort()\\n |> Enum.group_by(&String.length/1)\\nend\\n```\\n#### **Language X Doesn\\'t Have Pipelines**\\n\\nPipelines have been around for a long time, but only in niche languages. They\\'ve only moved into the mainstream recently, and many popular languages still don\\'t support the concept.\\n\\nThe good news is that thinking in transformations doesn\\'t require a particular language syntax: it\\'s more a philosophy of design. You still construct your code as transformations, but you write them as a series of assignments:\\n\\n```\\nconst content = File.read(file_name);\\nconst lines = find_matching_lines(content, pattern)\\nconst result = truncate_lines(lines)\\n```\\nIt\\'s a little more tedious, but it gets the job done.\\n\\n#### **Putting It All Together**\\n\\nWe\\'ve written each of the individual transformations. Now it\\'s time to string them all together into our main function:\\n\\n```\\nfunction-pipelines/anagrams/lib/anagrams.ex\\n```\\n\\n```\\n def anagrams_in(word) do\\n word\\n |> all_subsets_longer_than_three_characters()\\n |> as_unique_signatures()\\n |> find_in_dictionary()\\n |> group_by_length()\\nend\\n```\\nDoes it work? Let\\'s try it:\\n\\n```\\niex(1)> Anagrams.anagrams_in \"lyvin\"\\n%{\\n 3 => [\"ivy\", \"lin\", \"nil\", \"yin\"],\\n 4 => [\"inly\", \"liny\", \"viny\"],\\n 5 => [\"vinyl\"]\\n}\\n```\\n## **Why Is This So Great?**\\n\\nLet\\'s look at the body of the main function again:\\n\\n```\\nword\\n|> all_subsets_longer_than_three_characters()\\n|> as_unique_signatures()\\n|> find_in_dictionary()\\n|> group_by_length()\\n```\\nIt\\'s simply a chain of the transformations needed to meet our requirement, each taking input from the previous transformation and passing output to the next. That comes about as close to literate code as you can get.\\n\\nBut there\\'s something deeper, too. If your background is object-oriented programming, then your reflexes demand that you hide data, encapsulating it inside objects. These objects then chatter back and forth, changing each other\\'s state. This introduces a lot of coupling, and it is a big reason that OO systems can be hard to change.\\n\\n- **Tip 50** Don\\'t Hoard State; Pass It Around\\nIn the transformational model, we turn that on its head. Instead of little pools of data spread all over the system, think of data as a mighty river, a *flow*. Data becomes a peer to functionality: a pipeline is a sequence of code → data → code → data…. The data is no longer tied to a particular group of functions, as it is in a class definition. Instead it is free to represent the unfolding progress of our application as it transforms its inputs into its outputs. This means that we can greatly reduce coupling: a function can be used (and reused) anywhere its parameters match the output of some other function.\\n\\nYes, there is still a degree of coupling, but in our experience it\\'s more manageable than the OO-style of command and control. And, if you\\'re using a language with type checking, you\\'ll get compile-time warnings when you try to connect two incompatible things.\\n\\n## **What About Error Handling?**\\n\\nSo far our transforms have worked in a world where nothing goes wrong. How can we use them in the real world, though? If we can only build linear chains, how can we add all that conditional logic that we need for error checking?\\n\\nThere are many ways of doing this, but they all rely on a basic convention: we never pass raw values between transformations. Instead, we wrap them in a data structure (or type) which also tells us if the contained value is valid. In Haskell, for example, this wrapper is called Maybe. In F# and Scala it\\'s Option.\\n\\nHow you use this concept is language specific. In general, though, there are two basic ways of writing the code: you can handle checking for errors inside your transformations or outside them.\\n\\nElixir, which we\\'ve used so far, doesn\\'t have this support built in. For our purposes this is a good thing, as we get to show an implementation from the ground up. Something similar should work in most other languages.\\n\\n## **First, Choose a Representation**\\n\\nWe need a representation for our wrapper (the data structure that carries around a value or an error indication). You can use structures for this, but Elixir already has a pretty strong convention: functions tend to return a tuple containing either {:ok, value} or {:error, reason}. For example, File.open returns either :ok and an IO process or :error and a reason code:\\n\\n```\\niex(1)> File.open( \"/etc/passwd\")\\n{:ok, #PID<0.109.0>}\\niex(2)> File.open( \"/etc/wombat\")\\n{:error, :enoent}\\n```\\nWe\\'ll use the :ok/:error tuple as our wrapper when passing things through a pipeline.\\n\\n### **Then Handle It Inside Each Transformation**\\n\\nLet\\'s write a function that returns all the lines in a file that contain a given string, truncated to the first 20 characters. We want to write it as a transformation, so the input will be a file name and a string to match, and the output will be either an :ok tuple with a list of lines or an :error tuple with some kind of reason. The top-level function should look something like this:\\n\\n```\\nfunction-pipelines/anagrams/lib/grep.ex\\n   def find_all(file_name, pattern) do\\n    File.read(file_name)\\n    |> find_matching_lines(pattern)\\n    |> truncate_lines()\\n   end\\n```\\nThere\\'s no explicit error checking here, but if any step in the pipeline returns an error tuple then the pipeline will return that error without executing the functions that follow. [42] We do this using Elixir\\'s pattern matching:\\n\\n```\\nfunction-pipelines/anagrams/lib/grep.ex\\n   defp find_matching_lines({ :ok, content}, pattern) do\\n    content\\n    |> String.split( ~r/\\\\n/)\\n    |> Enum.filter(&String.match?(&1, pattern))\\n    |> ok_unless_empty()\\n   end\\n   defp find_matching_lines(error, _), do: error\\n   # ----------\\n   defp truncate_lines({ :ok, lines }) do\\n    lines\\n    |> Enum.map(&String.slice(&1, 0, 20))\\n    |> ok()\\n   end\\n   defp truncate_lines(error), do: error\\n   # ----------\\n```\\n\\n```\\ndefp ok_unless_empty([]), do: error( \"nothing found\")\\ndefp ok_unless_empty(result), do: ok(result)\\ndefp ok(result), do: { :ok, result }\\ndefp error(reason), do: { :error, reason }\\n```\\nHave a look at the function find_matching_lines. If its first parameter is an :ok tuple, it uses the content in that tuple to find lines matching the pattern. However, if the first parameter is *not* an :ok tuple, the second version of the function runs, which just returns that parameter. This way the function simply forwards an error down the pipeline. The same thing applies to truncate_lines.\\n\\nWe can play with this at the console:\\n\\n```\\niex> Grep.find_all \"/etc/passwd\", ~r/www/\\n{:ok, [\"_www: *:70:70:World W\", \"_wwwproxy: *:252:252:\"]}\\niex> Grep.find_all \"/etc/passwd\", ~r/wombat/\\n{:error, \"nothing found\"}\\niex> Grep.find_all \"/etc/koala\", ~r/www/\\n{:error, :enoent}\\n```\\nYou can see that an error anywhere in the pipeline immediately becomes the value of the pipeline.\\n\\n## **Or Handle It in the Pipeline**\\n\\nYou might be looking at the find_matching_lines and truncate_lines functions thinking that we\\'ve moved the burden of error handling into the transformations. You\\'d be right. In a language which uses pattern matching in function calls, such as Elixir, the effect is lessened, but it\\'s still ugly.\\n\\nIt would be nice if Elixir had a version of the pipeline operator |> that knew about the :ok/:error tuples and which short-circuited execution when an error occurred.[43] But the fact that it doesn\\'t allows us to add something similar, and in a way that is applicable to a number of other languages.\\n\\nThe problem we face is that when an error occurs we don\\'t want to run code further down the pipeline, and that we don\\'t want that code to know that this is happening. This means that we need to defer running pipeline functions until we know that previous steps in the pipeline were successful. To do this, we\\'ll need to change them from function *calls* into function *values* that can be called later. Here\\'s one implementation:\\n\\n```\\nfunction-pipelines/anagrams/lib/grep1.ex\\n```\\n\\n```\\ndefmodule Grep1 do\\n def and_then({ :ok, value }, func), do: func.(value)\\n def and_then(anything_else, _func), do: anything_else\\n def find_all(file_name, pattern) do\\n File.read(file_name)\\n |> and_then(&find_matching_lines(&1, pattern))\\n |> and_then(&truncate_lines(&1))\\n end\\n defp find_matching_lines(content, pattern) do\\n content\\n |> String.split( ~r/\\\\n/)\\n |> Enum.filter(&String.match?(&1, pattern))\\n |> ok_unless_empty()\\n end\\n defp truncate_lines(lines) do\\n lines\\n |> Enum.map(&String.slice(&1, 0, 20))\\n |> ok()\\n end\\n defp ok_unless_empty([]), do: error( \"nothing found\")\\n defp ok_unless_empty(result), do: ok(result)\\n defp ok(result), do: { :ok, result }\\n defp error(reason), do: { :error, reason }\\nend\\n```\\nThe and_then function is an example of a *bind* function: it takes a value wrapped in something, then applies a function to that value, returning a new wrapped value. Using the and_then function in the pipeline takes a little extra punctuation because Elixir needs to be told to convert function calls into function values, but that extra effort is offset by the fact that the transforming functions become simple: each just takes a value (and any extra parameters) and returns {:ok, new_value} or {:error, reason}.\\n\\n## **Transformations Transform Programming**\\n\\nThinking of code as a series of (nested) transformations can be a liberating approach to programming. It takes a while to get used to, but once you\\'ve developed the habit you\\'ll find your code becomes cleaner, your functions shorter, and your designs flatter.\\n\\nGive it a try.\\n\\n## **Related Sections Include**\\n\\n- Topic 8, *The Essence of Good Design*\\n- Topic 17, *Shell Games*\\n- Topic 26, *How to Balance Resources*\\n- Topic 28, *Decoupling*\\n- Topic 35, *Actors and Processes*\\n\\n## **Exercises**\\n\\n## **Exercise 21** (possible answer)\\n\\nCan you express the following requirements as a top-level transformation? That is, for each, identify the input and the output.\\n\\n- 1. Shipping and sales tax are added to an order\\n- 2. Your application loads configuration information from a named file\\n- 3. Someone logs in to a web application\\n\\n#### **Exercise 22** (possible answer)\\n\\nYou\\'ve identified the need to validate and convert an input field from a string into an integer between 18 and 150. The overall transformation is described by\\n\\n```\\nfield contents as string\\n → [validate & convert]\\n → {:ok, value} | {:error, reason}\\n```\\nWrite the individual transformations that make up *validate & convert*.\\n\\n**Exercise 23** (possible answer)\\n\\n```\\nIn Language X Doesn\\'t Have Pipelines we wrote:\\n```\\n\\n```\\nconst content = File.read(file_name);\\nconst lines = find_matching_lines(content, pattern)\\nconst result = truncate_lines(lines)\\n```\\nMany people write OO code by chaining together method calls, and might be tempted to write this as something like:\\n\\n```\\nconst result = content_of(file_name)\\n .find_matching_lines(pattern)\\n .truncate_lines()\\n```\\nWhat\\'s the difference between these two pieces of code? Which do you think we prefer?\\n\\n*Joe Armstrong*\\n\\n| You wanted a banana but what you got was a gorilla |\\n| --- |\\n| holding the banana and the entire jungle. |\\n\\nDo you program in an object-oriented language? Do you use inheritance?\\n\\nIf so, stop! It probably isn\\'t what you want to do.\\n\\nLet\\'s see why.\\n\\n#### **Some Background**\\n\\nInheritance first appeared in Simula 67 in 1969. It was an elegant solution to the problem of queuing multiple types of events on the same list. The Simula approach was to use something called *prefix classes*. You could write something like this:\\n\\n```\\nlink CLASS car;\\n ... implementation of car\\nlink CLASS bicycle;\\n ... implementation of bicycle\\n```\\nThe link is a prefix class that adds the functionality of linked lists. This lets you add both cars and bicycles to the list of things waiting at (say) a traffic light. In current terminology, link would be a parent class.\\n\\nThe mental model used by Simula programmers was that the instance data and implementation of class link was prepended to the implementation of classes car and bicycle. The link part was almost viewed as being a *container* that carried around cars and bicycles. This gave them a form of polymorphism: cars and bicycles both implemented the link interface because they both contained the link code.\\n\\nAfter Simula came Smalltalk. Alan Kay, one of the creators of Smalltalk, describes in a 2019 Quora answer[44] *why* Smalltalk has inheritance:\\n\\nSo when I designed Smalltalk-72—and it was a lark for fun while thinking about Smalltalk-71—I thought it would be fun to use its Lisp-like dynamics to do experiments with \"differential programming\" (meaning: various ways to accomplish \"this is like that except\").\\n\\nThis is subclassing purely for behavior.\\n\\nThese two styles of inheritance (which actually had a fair amount in common) developed over the following decades. The Simula approach, which suggested inheritance was a way of combining types, continued in languages such as C++ and Java. The Smalltalk school, where inheritance was a dynamic organization of behaviors, was seen in languages such as Ruby and JavaScript.\\n\\nSo, now we\\'re faced with a generation of OO developers who use inheritance for one of two reasons: they don\\'t like typing, or they like types.\\n\\nThose who don\\'t like typing save their fingers by using inheritance to add common functionality from a base class into child classes: class User and class Product are both subclasses of ActiveRecord::Base.\\n\\nThose who like types use inheritance to express the relationship between classes: a Car is-a-kind-of Vehicle.\\n\\nUnfortunately both kinds of inheritance have problems.\\n\\n#### **Problems Using Inheritance to Share Code**\\n\\nInheritance *is* coupling. Not only is the child class coupled to the parent, the parent\\'s parent, and so on, but the code that *uses* the child is also coupled to all the ancestors. Here\\'s an example:\\n\\n```\\nclass Vehicle\\n def initialize\\n @speed = 0\\n end\\n def stop\\n @speed = 0\\n end\\n def move_at(speed)\\n @speed = speed\\n end\\nend\\nclass Car < Vehicle\\n def info\\n \"I\\'m car driving at #{@speed }\"\\n end\\nend\\n# top-level code\\nmy_ride = Car.new\\nmy_ride.move_at(30)\\n```\\nWhen the top-level calls my_car.move_at, the method being invoked is in Vehicle, the parent of Car.\\n\\nNow the developer in charge of Vehicle changes the API, so move_at becomes set_velocity, and the instance variable @speed becomes @velocity.\\n\\nAn API change is expected to break clients of Vehicle class. But the top-level is not: as far as it is concerned it is using a Car. What the Car class does in terms of implementation is not the concern of the top-level code, but it still breaks.\\n\\nSimilarly the name of an instance variable is purely an internal implementation detail, but when Vehicle changes it also (silently) breaks Car.\\n\\nSo much coupling.\\n\\n#### **Problems Using Inheritance to Build Types**\\n\\nSome folks view inheritance as a way of defining new types. Their favorite design diagram shows class hierarchies. They view problems the way Victorian gentleman scientists viewed nature, as something to be broken down into categories.\\n\\nUnfortunately, these diagrams soon grow into wall-covering monstrosities, layer-upon-layer added in order to express the smallest nuance of\\n\\n![](_page_246_Figure_13.jpeg)\\n\\ndifferentiation between classes. This added complexity can make the application more brittle, as changes can ripple up and down many layers.\\n\\nEven worse, though, is the multiple inheritance issue. A Car may be a kind of Vehicle, but it can also be a kind of Asset, InsuredItem, LoanCollateral and so on. Modeling this correctly would need multiple inheritance.\\n\\nC++ gave multiple inheritance a bad name in the 1990s because of some questionable disambiguation semantics. As a result, many current OO languages don\\'t offer it. So, even if you\\'re happy with complex type trees, you won\\'t be able to model your domain accurately anyway.\\n\\n#### **Tip 51** Don\\'t Pay Inheritance Tax\\n\\n#### **The Alternatives Are Better**\\n\\nLet us suggest three techniques that mean you should never need to use inheritance again:\\n\\n- Interfaces and protocols\\n- Delegation\\n- Mixins and traits\\n\\n#### **Interfaces and Protocols**\\n\\nMost OO languages allow you to specify that a class implements one or more sets of behaviors. You could say, for example, that a Car class implements the Drivable behavior and the Locatable behavior. The syntax used for doing this varies: in Java, it might look like this:\\n\\n```\\npublic class Car implements Drivable, Locatable {\\n // Code for class Car. This code must include\\n // the functionality of both Drivable\\n // and Locatable\\n}\\n```\\nDrivable and Locatable are what Java calls *interfaces*; other languages call them *protocols*, and some call them *traits* (although this is not what we\\'ll be calling a trait later).\\n\\nInterfaces are defined like this:\\n\\n```\\npublic interface Drivable {\\n double getSpeed();\\n void stop();\\n}\\npublic interface Locatable() {\\n Coordinate getLocation();\\n boolean locationIsValid();\\n```\\n![](_page_247_Figure_16.jpeg)\\n\\n}\\n\\nThese declarations create no code: they simply say that any class that implements Drivable must implement the two methods getSpeed and stop, and a class that\\'s Locatable must implement getLocation and locationIsValid. This means that our previous class definition of Car will only be valid if it includes all four of these methods.\\n\\nWhat makes interfaces and protocols so powerful is that we can use them as types, and any class that implements the appropriate interface will be compatible with that type. If Car and Phone both implement Locatable, we could store both in a list of locatable items:\\n\\n```\\nList<Locatable> items = new ArrayList<>();\\nitems.add( new Car(...));\\nitems.add( new Phone(...));\\nitems.add( new Car(...));\\n// ...\\n```\\nWe can then process that list, safe in the knowledge that every item has getLocation and locationIsValid:\\n\\n```\\nvoid printLocation(Locatable item) {\\n if (item.locationIsValid() {\\n print(item.getLocation().asString());\\n}\\n// ...\\n```\\nitems.forEach(printLocation);\\n\\n#### **Tip 52** Prefer Interfaces to Express Polymorphism\\n\\nInterfaces and protocols give us polymorphism without inheritance.\\n\\n#### **Delegation**\\n\\nInheritance encourages developers to create classes whose objects have large numbers of methods. If a parent class has 20 methods, and the subclass wants to make use of just two of them, its objects will still have the other 18 just lying around and callable. The class has lost control of its interface. This is a common problem—many persistence and UI frameworks insist that application components subclass some supplied base class:\\n\\n```\\nclass Account < PersistenceBaseClass\\nend\\n```\\nThe Account class now carries all of the persistence class\\'s API around with it. Instead, imagine an alternative using delegation, as in the following example:\\n\\n```\\nclass Account\\n def initialize(. . .)\\n @repo = Persister.for(self)\\n end\\n def save\\n @repo.save()\\n```\\n **end end**\\n\\nWe now expose *none* of the framework API to the clients of our Account class: that coupling is now broken. But there\\'s more. Now that we\\'re no longer constrained by the API of the framework we\\'re using, we\\'re free to create the API we need. Yes, we could do that before, but we always ran the risk that the interface *we* wrote can be bypassed, and the persistence API used instead. Now we control everything.\\n\\n![](_page_249_Picture_2.jpeg)\\n\\nIn fact, we can take this a step further. Why should an Account have to know how to persist itself? Isn\\'t its job to know and enforce the account business rules?\\n\\n```\\nclass Account\\n # nothing but account stuff\\nend\\nclass AccountRecord\\n # wraps an account with the ability\\n # to be fetched and stored\\nend\\n```\\nNow we\\'re really decoupled, but it has come at a cost. We\\'re having to write more code, and typically some of it will be boilerplate: it\\'s likely that all our record classes will need a find method, for example.\\n\\nFortunately, that\\'s what mixins and traits do for us.\\n\\n#### **Mixins, Traits, Categories, Protocol Extensions, …**\\n\\nAs an industry, we love to give things names. Quite often we\\'ll give the same thing many names. More is better, right?\\n\\nThat\\'s what we\\'re dealing with when we look at mixins. The basic idea is simple: we want to be able to extend classes and objects with new functionality without using inheritance. So we create a set of these functions, give that set a name, and then somehow extend a class or object with them. At that point, you\\'ve created a new class or object that combines the capabilities of the original and all its mixins. In most cases, you\\'ll be able to make this extension even if you don\\'t have access to the source code of the class you\\'re extending.\\n\\nNow the implementation and name of this feature varies between languages. We\\'ll tend to call them *mixins* here, but we really want you to think of this as a language-agnostic feature. The important thing is the capability that all these implementations have: merging functionality between *existing things* and *new things*.\\n\\nAs an example, let\\'s go back to our AccountRecord example. As we left it, an AccountRecord needed to know about both accounts and about our persistence framework. It also needed to delegate all the methods in the persistence layer that it wanted to expose to the outside world.\\n\\nMixins give us an alternative. First, we could write a mixin that implements (for example) two of three of the standard finder methods. We could then add them into AccountRecord as a mixin. And, as we write new classes for persisted things, we can add the mixin to them, too:\\n\\n```\\nmixin CommonFinders {\\n def find(id) { ... }\\n def findAll() { ... }\\nend\\nclass AccountRecord extends BasicRecord with CommonFinders\\nclass OrderRecord extends BasicRecord with CommonFinders\\n```\\nWe can take this a lot further. For example, we all know our business objects need validation code to prevent bad data from infiltrating our calculations. But exactly what do we mean by *validation*?\\n\\nIf we take an account, for example, there are probably many different layers of validation that could be applied:\\n\\n- Validating that a hashed password matches one entered by the user\\n- Validating form data entered by the user when an account is created\\n- Validating form data entered by an admin person updating the user details\\n- Validating data added to the account by other system components\\n- Validating data for consistency before it is persisted\\n\\nA common (and we believe less-than-ideal) approach is to bundle all the validations into a single class (the business object/persistence object) and then add flags to control which fire in which circumstances.\\n\\nWe think a better way is to use mixins to create specialized classes for appropriate situations:\\n\\n```\\nclass AccountForCustomer extends Account\\n with AccountValidations,AccountCustomerValidations\\nclass AccountForAdmin extends Account\\n with AccountValidations,AccountAdminValidations\\n```\\nHere, both derived classes include validations common to all account objects. The customer variant also includes validations appropriate for the customer-facing APIs, while the admin variant contained (the presumably less restrictive) admin validations.\\n\\nNow, by passing instances of AccountForCustomer or AccountForAdmin back and forth, our code *automatically* ensures the correct validation is applied.\\n\\n![](_page_250_Picture_13.jpeg)\\n\\n#### **Tip 54** Use Mixins to Share Functionality\\n\\n#### **Inheritance Is Rarely the Answer**\\n\\nWe\\'ve had a quick look at three alternatives to traditional class inheritance:\\n\\n- Interfaces and protocols\\n- Delegation\\n- Mixins and traits\\n\\nEach of these methods may be better for you in different circumstances, depending on whether your goal is sharing type information, adding functionality, or sharing methods. As with anything in programming, aim to use the technique that best expresses your intent.\\n\\nAnd try not to drag the whole jungle along for the ride.\\n\\n#### **Related Sections Include**\\n\\n- Topic 8, *The Essence of Good Design*\\n- Topic 10, *Orthogonality*\\n- Topic 28, *Decoupling*\\n\\n#### **Challenges**\\n\\n- The next time you find yourself subclassing, take a minute to examine the options. Can you achieve what you want with interfaces, delegation, and/or mixins? Can you reduce coupling by doing so?\\n## **Topic 32** Configuration\\n\\n*Let all your things have their places; let each part of your business have its time.*\\n\\n> *Benjamin Franklin, Thirteen Virtues, autobiography*\\n\\nWhen code relies on values that may change after the application has gone live, keep those values external to the app. When your application will run in different environments, and potentially for different customers, keep the\\n\\nenvironment- and customer-specific values outside the app. In this way, you\\'re parameterizing your application; the code adapts to the places it runs.\\n\\n**Tip 55** Parameterize Your App Using External Configuration\\n\\nCommon things you will probably want to put in configuration data include:\\n\\n- Credentials for external services (database, third party APIs, and so on)\\n- Logging levels and destinations\\n- Port, IP address, machine, and cluster names the app uses\\n- Environment-specific validation parameters\\n- Externally set parameters, such as tax rates\\n- Site-specific formatting details\\n- License keys\\n\\nBasically, look for anything that you know will have to change that you can express outside your main body of code, and slap it into some configuration bucket.\\n\\n## **Static Configuration**\\n\\nMany frameworks, and quite a few custom applications, keep configuration in either flat files or database tables. If the information is in flat files, the trend is to use some off-the-shelf plain-text format. Currently YAML and JSON are popular for this. Sometimes applications written in scripting languages use special purpose source-code files, dedicated to containing just configuration. If the information is structured, and is likely to be changed by the customer (sales tax rates, for example), it might be better to store it in a database table. And, of course, you can use both, splitting the configuration information according to use.\\n\\nWhatever form you use, the configuration is read into your application as a data structure, normally when the application starts. Commonly, this data structure is made global, the thinking being that this makes it easier for any part of the code to get to the values it holds.\\n\\nWe prefer that you don\\'t do that. Instead, wrap the configuration information behind a (thin) API. This decouples your code from the details of the representation of configuration.\\n\\n## **Configuration-As-A-Service**\\n\\nWhile static configuration is common, we currently favor a different approach. We still want configuration data kept external to the application, but rather than in a flat file or database, we\\'d like to see it stored behind a service API. This has a number of benefits:\\n\\n- Multiple applications can share configuration information, with authentication and access control limiting what each can see\\n- Configuration changes can be made globally\\n- The configuration data can be maintained via a specialized UI\\n- The configuration data becomes dynamic\\n\\nThat last point, that configuration should be dynamic, is critical as we move toward highly available applications. The idea that we should have to stop and restart an application to change a single parameter is hopelessly out of touch with modern realities. Using a configuration service, components of the application could register for notifications of updates to parameters they use, and the service could send them messages containing new values if and when they are changed.\\n\\nWhatever form it takes, configuration data drives the runtime behavior of an application. When configuration values change, there\\'s no need to rebuild the code.\\n\\n## **Don\\'t Write Dodo-Code**\\n\\nWithout external configuration, your code is not as adaptable or flexible as it could be. Is this a bad thing? Well, out here in the real world, species that don\\'t adapt die.\\n\\nThe dodo didn\\'t adapt to the presence of humans and their livestock on the island of Mauritius, and quickly became extinct.[45] It was the first documented extinction of a species at the hand of man.\\n\\nDon\\'t let your project (or your career) go the way of the dodo.\\n\\n![](_page_254_Picture_6.jpeg)\\n\\n## **Related Sections Include**\\n\\n- Topic 9, *DRY—The Evils of Duplication*\\n- Topic 14, *Domain Languages*\\n- Topic 16, *The Power of Plain Text*\\n- Topic 28, *Decoupling*\\n\\n#### **Don\\'t Overdo It**\\n\\nIn the first edition of this book, we suggested using configuration instead of code in a similar fashion, but apparently should have been a little more specific in our instructions. Any advice can be taken to extremes or used inappropriately, so here are a few cautions:\\n\\nDon\\'t overdo it. One early client of ours decided that every single field in their application should be configurable. As a result, it took weeks to make even the smallest change, as you had to implement both the field and all the admin code to save and edit it. They had some *40,000* configuration variables and a coding nightmare on their hands.\\n\\nDon\\'t push decisions to configuration out of laziness. If there\\'s genuine debate about whether a feature should work this way or that, or if it should be the users\\' choice, try it out one way and get feedback on whether the decision was a good one.\\n\\n#### **Footnotes**\\n\\n- [37] So it\\'s not really a law. It\\'s more like The Jolly Good Idea of Demeter.\\n- [38] https://media.pragprog.com/articles/jan_03_enbug.pdf\\n- [39] Yes, we know that Ruby already has this capability with its at_exit function.\\n- [40] https://media.pragprog.com/titles/tpp20/code/event/rxcommon/logger.js\\n- [41] It seems that the first use of the characters |> as a pipe dates to 1994, in a discussion about the language Isabelle/ML, archived at https://blogs.msdn.microsoft.com/dsyme/2011/05/17/archeological-semiotics-the-birth-of-thepipeline-symbol-1994/\\n- [42] We\\'ve taken a liberty here. Technically we do execute the following functions. We just don\\'t execute the code in them.\\n- [43] In fact you could add such an operator to Elixir using its macro facility; an example of this is the Monad library in hex. You could also use Elixir\\'s with construct, but then you lose much of the\\n\\nsense of writing transformations that you get with pipelines.\\n\\n- [44] https://www.quora.com/What-does-Alan-Kay-think-about-inheritance-in-object-orientedprogramming\\n- [45] It didn\\'t help that the settlers beat the placid (read: *stupid*) birds to death with clubs for sport.\\n\\nCopyright © 2020 Pearson Education, Inc.\\n\\n# Chapter 6\\n\\n# **Concurrency**\\n\\nJust so we\\'re all on the same page, let\\'s start with some definitions:\\n\\n*Concurrency* is when the execution of two or more pieces of code act as if they run at the same time. *Parallelism* is when they *do* run at the same time.\\n\\nTo have concurrency, you need to run code in an environment that can switch execution between different parts of your code when it is running. This is often implemented using things such as fibers, threads, and processes.\\n\\nTo have parallelism, you need hardware that can do two things at once. This might be multiple cores in a CPU, multiple CPUs in a computer, or multiple computers connected together.\\n\\n## **Everything Is Concurrent**\\n\\nIt\\'s almost impossible to write code in a decent-sized system that doesn\\'t have concurrent aspects to it. They may be explicit, or they may be buried inside a library. Concurrency is a requirement if you want your application to be able to deal with the real world, where things are asynchronous: users are interacting, data is being fetched, external services are being called, all at the same time. If you force this process to be serial, with one thing happening, then the next, and so on, your system feels sluggish and you\\'re probably not taking full advantage of the power of the hardware on which it runs.\\n\\nIn this chapter we\\'ll look at concurrency and parallelism.\\n\\nDevelopers often talk about coupling between chunks of code. They\\'re referring to dependencies, and how those dependencies make things hard to change. But there\\'s another form of coupling. *Temporal coupling* happens when your code imposes a sequence on things that is not required to solve the problem at hand. Do you depend on the \"tick\" coming before the \"tock\"? Not if you want to stay flexible. Does your code access multiple back-end services sequentially, one after the other? Not if you want to keep your customers. In Topic 33, *Breaking Temporal Coupling*, we\\'ll look at ways of identifying this kind of temporal coupling.\\n\\nWhy is writing concurrent and parallel code so difficult? One reason is that we learned to program using sequential systems, and our languages have features that are relatively safe when used sequentially but become a liability once two things can happen at the same time. One of the biggest culprits here is *shared state*. This doesn\\'t just mean global variables: any time two or more chunks of code hold references to the same piece of mutable data, you have shared state. And Topic 34, *Shared State Is Incorrect State*. The section describes a number of workarounds for this, but ultimately they\\'re all error prone.\\n\\nIf that makes you feel sad, *nil desperandum!* There are better ways to construct concurrent applications. One of these is using the *actor model*, where independent processes, which share no data, communicate over channels using defined, simple, semantics. We talk about both the theory and practice of this approach in Topic 35, *Actors and Processes*.\\n\\nFinally, we\\'ll look at Topic 36, *Blackboards*. These are systems which act like a combination of an object store and a smart publish/subscribe broker. In their original form, they never really took off. But today we\\'re seeing more and more implementations of middleware layers with blackboard-like semantics. Used correctly, these types of systems offer a serious amount of decoupling.\\n\\nConcurrent and parallel code used to be exotic. Now it is required.\\n\\n## **Topic 33** Breaking Temporal Coupling\\n\\n\"What is *temporal coupling* all about?\", you may ask. It\\'s about time.\\n\\nTime is an often ignored aspect of software architectures. The only time that preoccupies us is the time on the schedule, the time left until we ship but this is not what we\\'re talking about here. Instead, we are talking about the role of time as a design element of the software itself. There are two aspects of time that are important to us: concurrency (things happening at the same time) and ordering (the relative positions of things in time).\\n\\nWe don\\'t usually approach programming with either of these aspects in mind. When people first sit down to design an architecture or write a program, things tend to be linear. That\\'s the way most people think—*do this* and then always *do that*. But thinking this way leads to *temporal coupling*: coupling in time. Method A must always be called before method B; only one report can be run at a time; you must wait for the screen to redraw before the button click is received. Tick must happen before tock.\\n\\nThis approach is not very flexible, and not very realistic.\\n\\nWe need to allow for concurrency and to think about decoupling any time or order dependencies. In doing so, we can gain flexibility and reduce any time-based dependencies in many areas of development: workflow analysis, architecture, design, and deployment. The result will be systems that are easier to reason about, that potentially respond faster and more reliably.\\n\\n## **Looking for Concurrency**\\n\\nOn many projects, we need to model and analyze the application workflows as part of the design. We\\'d like to find out what *can* happen at the same\\n\\ntime, and what *must* happen in a strict order. One way to do this is to capture the workflow using a notation such as the *activity diagram*. [46]\\n\\n#### **Tip 56** Analyze Workflow to Improve Concurrency\\n\\nAn activity diagram consists of a set of actions drawn as rounded boxes. The arrow leaving an action leads to either another action (which can start once the first action completes) or to a thick line called a *synchronization bar*. Once *all* the actions leading into a synchronization bar are complete, you can then proceed along any arrows leaving the bar. An action with no arrows leading into it can be started at any time.\\n\\nYou can use activity diagrams to maximize parallelism by identifying activities that *could be* performed in parallel, but aren\\'t.\\n\\nFor instance, we may be writing the software for a robotic piña colada maker. We\\'re told that the steps are:\\n\\n- 1. Open blender 2. Open piña colada mix 3. Put mix in blender 4. Measure 1/2 cup white rum 5. Pour in rum 6. Add 2 cups of ice\\n- 1. Close blender\\n- 2. Liquefy for 1 minute\\n- 3. Open blender\\n- 4. Get glasses\\n- 5. Get pink umbrellas\\n- 6. Serve\\n\\nHowever, a bartender would lose their job if they followed these steps, one by one, in order. Even though they describe these actions serially, many of them could be performed in parallel. We\\'ll use the following activity diagram to capture and reason about potential concurrency.\\n\\n![](_page_262_Figure_0.jpeg)\\n\\nIt can be eye-opening to see where the dependencies really exist. In this instance, the top-level tasks (1, 2, 4, 10, and 11) can all happen concurrently, up front. Tasks 3, 5, and 6 can happen in parallel later. If you were in a piña colada-making contest, these optimizations may make all the difference.\\n\\n#### **Faster Formatting**\\n\\nThis book is written in plain text. To build the version to be printed, or an ebook, or whatever, that text is fed through a pipeline of processors. Some look for particular constructs (bibliography citations, index entries, special markup for tips, and so on). Other processors operate on the document as a whole.\\n\\nMany of the processors in the pipeline have to access external information (reading files, writing files, piping through external programs). All this relatively slow speed work gives us the opportunity to exploit concurrency: in fact each step in the pipeline executes concurrently, reading from the previous step and writing to the next.\\n\\nIn addition, some parts of the process are relatively processor intensive. One of these is the conversion of mathematical formulae. For various historical reasons each equation can take up to 500ms to convert. To speed things up, we take advantage of parallelism. Because each formula is independent of the others, we convert each in its own parallel process and collect the results back into the book as they become available.\\n\\nAs a result, the book builds much, much faster on multicore machines.\\n\\n(And, yes, we did indeed discover a number of concurrency errors in our pipeline along the way….)\\n\\n## **Opportunities for Concurrency**\\n\\nActivity diagrams show the potential areas of concurrency, but have nothing to say about whether these areas are worth exploiting. For example, in the piña colada example, a bartender would need five hands to be able to run all the potential initial tasks at once.\\n\\nAnd that\\'s where the design part comes in. When we look at the activities, we realize that number 8, liquify, will take a minute. During that time, our bartender can get the glasses and umbrellas (activities 10 and 11) and probably still have time to serve another customer.\\n\\nAnd that\\'s what we\\'re looking for when we\\'re designing for concurrency. We\\'re hoping to find activities that take time, but not time in our code. Querying a database, accessing an external service, waiting for user input: all these things would normally stall our program until they complete. And these are all opportunities to do something more productive than the CPU equivalent of twiddling one\\'s thumbs.\\n\\n## **Opportunities for Parallelism**\\n\\nRemember the distinction: concurrency is a software mechanism, and parallelism is a hardware concern. If we have multiple processors, either locally or remotely, then if we can split work out among them we can reduce the overall time things take.\\n\\nThe ideal things to split this way are pieces of work that are relatively independent—where each can proceed without waiting for anything from the others. A common pattern is to take a large piece of work, split it into independent chunks, process each in parallel, then combine the results.\\n\\nAn interesting example of this in practice is the way the compiler for the Elixir language works. When it starts, it splits the project it is building into modules, and compiles each in parallel. Sometimes a module depends on another, in which case its compilation pauses until the results of the other module\\'s build become available. When the top-level module completes, it means that all dependencies have been compiled. The result is a speedy compilation that takes advantage of all the cores available.\\n\\n## **Identifying Opportunities Is the Easy Part**\\n\\nBack to your applications. We\\'ve identified places where it will benefit from concurrency and parallelism. Now for the tricky part: how can we implement it safely. That\\'s the topic of the rest of the chapter.\\n\\n## **Related Sections Include**\\n\\n- Topic 10, *Orthogonality*\\n- Topic 26, *How to Balance Resources*\\n- Topic 28, *Decoupling*\\n- Topic 36, *Blackboards*\\n\\n## **Challenges**\\n\\n- How many tasks do you perform in parallel when you get ready for work in the morning? Could you express this in a UML activity diagram? Can you find some way to get ready more quickly by increasing concurrency?\\nYou\\'re in your favorite diner. You finish your main course, and ask your server if there\\'s any apple pie left. He looks over his shoulder, sees one piece in the display case, and says yes. You order it and sigh contentedly.\\n\\nMeanwhile, on the other side of the restaurant, another customer asks their server the same question. She also looks, confirms there\\'s a piece, and that customer orders.\\n\\nOne of the customers is going to be disappointed.\\n\\nSwap the display case for a joint bank account, and turn the waitstaff into point-of-sale devices. You and your partner both decide to buy a new phone at the same time, but there\\'s only enough in the account for one. Someone —the bank, the store, or you—is going to be very unhappy.\\n\\n- \\n- **Tip 57** Shared State Is Incorrect State\\n\\nThe problem is the shared state. Each server in the restaurant looked into the display case without regard for the other. Each point-of-sale device looked at an account balance without regard for the other.\\n\\n## **Nonatomic Updates**\\n\\nLet\\'s look at our diner example as if it were code:\\n\\n![](_page_267_Figure_0.jpeg)\\n\\nThe two waiters operate concurrently (and, in real life, in parallel). Let\\'s look at their code:\\n\\n```\\nif display_case.pie_count > 0\\n promise_pie_to_customer()\\n display_case.take_pie()\\n give_pie_to_customer()\\nend\\n```\\nWaiter 1 gets the current pie count, and finds that it is one. He promises the pie to the customer. But at that point, waiter 2 runs. She also sees the pie count is one and makes the same promise to her customer. One of the two then grabs the last piece of pie, and the other waiter enters some kind of error state (which probably involves much grovelling).\\n\\nThe problem here is not that two processes can write to the same memory. The problem is that neither process can guarantee that its view of that memory is consistent. Effectively, when a waiter executes display_case.pie_count(), they copy the value from the display case into their own memory. If the value in the display case changes, their memory (which they are using to make decisions) is now out of date.\\n\\nThis is all because the fetching and then updating the pie count is not an atomic operation: the underlying value can change in the middle.\\n\\nSo how can we make it atomic?\\n\\n## **Semaphores and Other Forms of Mutual Exclusion**\\n\\nA semaphore is simply a *thing* that only one person can own at a time. You can create a semaphore and then use it to control access to some other resource. In our example, we could create a semaphore to control access to the pie case, and adopt the convention that anyone who wants to update the pie case contents can only do so if they are holding that semaphore.\\n\\nSay the diner decides to fix the pie problem with a physical semaphore. They place a plastic Leprechaun on the pie case. Before any waiter can sell a pie, they have to be holding the Leprechaun in their hand. Once their order has been completed (which means delivering the pie to the table) they can return the Leprechaun to its place guarding the treasure of the pies, ready to mediate the next order.\\n\\nLet\\'s look at this in code. Classically, the operation to grab the semaphore was called *P*, and the operation to release it was called *V*. [47] Today we use terms such as *lock/unlock*, *claim/release*, and so on.\\n\\n```\\ncase_semaphore.lock()\\nif display_case.pie_count > 0\\n promise_pie_to_customer()\\n display_case.take_pie()\\n give_pie_to_customer()\\nend\\ncase_semaphore.unlock()\\n```\\nThis code assumes that a semaphore has already been created and stored in the variable case_semaphore.\\n\\nLet\\'s assume both waiters execute the code at the same time. They both try to lock the semaphore, but only one succeeds. The one that gets the semaphore continues to run as normal. The one that doesn\\'t get the\\n\\nsemaphore is suspended until the semaphore becomes available (the waiter waits…). When the first waiter completes the order they unlock the semaphore and the second waiter continues running. They now see there\\'s no pie in the case, and apologize to the customer.\\n\\nThere are some problems with this approach. Probably the most significant is that it only works because everyone who accesses the pie case agrees on the convention of using the semaphore. If someone forgets (that is, some developer writes code that doesn\\'t follow the convention) then we\\'re back in chaos.\\n\\n## **Make the Resource Transactional**\\n\\nThe current design is poor because it delegates responsibility for protecting access to the pie case to the people who use it. Let\\'s change it to centralize that control. To do this, we have to change the API so that waiters can check the count and also take a slice of pie in a single call:\\n\\n```\\nslice = display_case.get_pie_if_available()\\nif slice\\n give_pie_to_customer()\\nend\\n```\\nTo make this work, we need to write a method that runs as part of the display case itself:\\n\\n![](_page_269_Picture_6.jpeg)\\n\\nThis code illustrates a common misconception. We\\'ve moved the resource access into a central place, but our method can still be called from multiple concurrent threads, so we still need to protect it with a semaphore:\\n\\n```\\ndef get_pie_if_available()\\n @case_semaphore.lock()\\n if @slices.size > 0\\n update_sales_data( :pie)\\n return @slices.shift\\n else\\n false\\n end\\n @case_semaphore.unlock()\\nend\\n```\\nEven this code might not be correct. If update_sales_data raises an exception, the semaphore will never get unlocked, and all future access to the pie case will hang indefinitely. We need to handle this:\\n\\n```\\ndef get_pie_if_available()\\n @case_semaphore.lock()\\n try {\\n if @slices.size > 0\\n update_sales_data( :pie)\\n return @slices.shift\\n else\\n false\\n end\\n }\\n ensure {\\n @case_semaphore.unlock()\\n }\\nend\\n```\\nBecause this is such a common mistake, many languages provide libraries that handle this for you:\\n\\n```\\ndef get_pie_if_available()\\n @case_semaphore.protect() {\\n if @slices.size > 0\\n update_sales_data( :pie)\\n return @slices.shift\\n else\\n```\\n\\n```\\n false\\n end\\n }\\nend\\n```\\n## **Multiple Resource Transactions**\\n\\nOur diner just installed an ice cream freezer. If a customer orders pie *à la mode*, the waiter will need to check that both pie *and* ice cream are available.\\n\\nWe could change the waiter code to something like:\\n\\n```\\nslice = display_case.get_pie_if_available()\\nscoop = freezer.get_ice_cream_if_available()\\nif slice && scoop\\n give_order_to_customer()\\nend\\n```\\nThis won\\'t work, though. What happens if we claim a slice of pie, but when we try to get a scoop of ice cream we find out there isn\\'t any? We\\'re now left holding some pie that we can\\'t do anything with (because our customer *must* have ice cream). And the fact we\\'re holding the pie means it isn\\'t in the case, so it isn\\'t available to some other customer who (being a purist) doesn\\'t want ice cream with it.\\n\\nWe could fix this by adding a method to the case that lets us return a slice of pie. We\\'ll need to add exception handling to ensure we don\\'t keep resources if something fails:\\n\\n```\\nslice = display_case.get_pie_if_available()\\nif slice\\n try {\\n scoop = freezer.get_ice_cream_if_available()\\n if scoop\\n try {\\n give_order_to_customer()\\n```\\n\\n```\\n }\\n rescue {\\n freezer.give_back(scoop)\\n }\\n end\\n }\\n rescue {\\n display_case.give_back(slice)\\n }\\nend\\n```\\nAgain, this is less than ideal. The code is now really ugly: working out what it actually does is difficult: the business logic is buried in all the housekeeping.\\n\\nPreviously we fixed this by moving the resource handling code into the resource itself. Here, though, we have two resources. Should we put the code in the display case or the freezer?\\n\\nWe think the answer is \"no\" to both options. The pragmatic approach would be to say that \"apple pie à la mode\" is its own resource. We\\'d move this code into a new module, and then the client could just say \"get me apple pie with ice cream\" and it either succeeds or fails.\\n\\nOf course, in the real world there are likely to be many composite dishes like this, and you wouldn\\'t want to write new modules for each. Instead, you\\'d probably want some kind of menu item which contained references to its components, and then have a generic get_menu_item method that does the resource dance with each.\\n\\n## **Non-Transactional Updates**\\n\\nA lot of attention is given to shared memory as a source of concurrency problems, but in fact the problems can pop up *anywhere* where your application code shares mutable resources: files, databases, external services, and so on. Whenever two or more instances of your code can\\n\\naccess some resource at the same time, you\\'re looking at a potential problem.\\n\\nSometimes, the resource isn\\'t all that obvious. While writing this edition of the book we updated the toolchain to do more work in parallel using threads. This caused the build to fail, but in bizarre ways and random places. A common thread through all the errors was that files or directories could not be found, even though they were really in exactly the right place.\\n\\nWe tracked this down to a couple of places in the code which temporarily changed the current directory. In the nonparallel version, the fact that this code restored the directory back was good enough. But in the parallel version, one thread would change the directory and then, while in that directory, another thread would start running. That thread would expect to be in the original directory, but because the current directory is shared between threads, that wasn\\'t the case.\\n\\nThe nature of this problem prompts another tip:\\n\\n## **Tip 58** Random Failures Are Often Concurrency Issues\\n\\n## **Other Kinds of Exclusive Access**\\n\\nMost languages have library support for some kind of exclusive access to shared resources. They may call it mutexes (for mutual exclusion), monitors, or semaphores. These are all implemented as libraries.\\n\\nHowever, some languages have concurrency support built into the language itself. Rust, for example, enforces the concept of data ownership; only one variable or parameter can hold a reference to any particular piece of mutable data at a time.\\n\\nYou could also argue that functional languages, with their tendency to make all data immutable, make concurrency simpler. However, they still face the same challenges, because at some point they are forced to step into the real, mutable world.\\n\\n## **Doctor, It Hurts…**\\n\\nIf you take nothing else away from this section, take this: concurrency in a shared resource environment is difficult, and managing it yourself is fraught with challenges.\\n\\nWhich is why we\\'re recommending the punchline to the old joke:\\n\\nDoctor, it hurts when I do this.\\n\\nThen don\\'t do that.\\n\\nThe next couple of sections suggest alternative ways of getting the benefits of concurrency without the pain.\\n\\n## **Related Sections Include**\\n\\n- Topic 10, *Orthogonality*\\n- Topic 28, *Decoupling*\\n- Topic 38, *Programming by Coincidence*\\n\\n## **Topic 35** Actors and Processes\\n\\n*Without writers, stories would not be written, Without actors, stories could not be brought to life.*\\n\\n*Angie-Marie Delsante*\\n\\nActors and processes offer interesting ways of implementing concurrency without the burden of synchronizing access to shared memory.\\n\\nBefore we get into them, however, we need to define what we mean. And this\\n\\nis going to sound academic. Never fear, we\\'ll be working through it all in a short while.\\n\\n- An *actor* is an independent virtual processor with its own local (and private) state. Each actor has a mailbox. When a message appears in the mailbox and the actor is idle, it kicks into life and processes the message. When it finishes processing, it processes another message in the mailbox, or, if the mailbox is empty, it goes back to sleep.\\nWhen processing a message, an actor can create other actors, send messages to other actors that it knows about, and create a new state that will become the current state when the next message is processed.\\n\\n- A *process* is typically a more general-purpose virtual processor, often implemented by the operating system to facilitate concurrency. Processes can be constrained (by convention) to behave like actors, and that\\'s the type of process we mean here.\\n## **Actors Can Only Be Concurrent**\\n\\nThere are a few things that you *won\\'t* find in the definition of actors:\\n\\n- There\\'s no single *thing* that\\'s in control. Nothing schedules what happens next, or orchestrates the transfer of information from the raw\\ndata to the final output.\\n\\n- The *only* state in the system is held in messages and in the local state of each actor. Messages cannot be examined except by being read by their recipient, and local state is inaccessible outside the actor.\\n- All messages are one way—there\\'s no concept of replying. If you want an actor to return a response, you include your own mailbox address in the message you send it, and it will (eventually) send the response as just another message to that mailbox.\\n- An actor processes each message to completion, and only processes one message at a time.\\n\\nAs a result, actors execute concurrently, asynchronously, and share nothing. If you had enough physical processors, you could run an actor on each. If you have a single processor, then some runtime can handle the switching of context between them. Either way, the code running in the actors is the same.\\n\\n| Tip 59 | Use Actors For Concurrency Without Shared |\\n| --- | --- |\\n|  | State |\\n\\n## **A Simple Actor**\\n\\nLet\\'s implement our diner using actors. In this case, we\\'ll have three (the customer, the waiter, and the pie case).\\n\\nThe overall message flow will look like this:\\n\\n- We (as some kind of external, God-like being) tell the customer that they are hungry\\n- In response, they\\'ll ask the waiter for pie\\n- The waiter will ask the pie case to get some pie to the customer\\n- If the pie case has a slice available, it will send it to the customer, and also notify the waiter to add it to the bill\\n- If there is no pie, the case tells the waiter, and the waiter apologizes to the customer\\n\\nWe\\'ve chosen to implement the code in JavaScript using the Nact library. [48] We\\'ve added a little wrapper to this that lets us write actors as simple objects, where the keys are the message types that it receives and the values are functions to run when that particular message is received. (Most actor systems have a similar kind of structure, but the details depend on the host language.)\\n\\nLet\\'s start with the customer. The customer can receive three messages:\\n\\n- You\\'re hungry (sent by the external context)\\n- There\\'s pie on the table (sent by the pie case)\\n- Sorry, there\\'s no pie (sent by the waiter)\\n\\nHere\\'s the code:\\n\\n```\\nconcurrency/actors/index.js\\n```\\n\\n```\\nconst customerActor = {\\n \\'hungry for pie\\': (msg, ctx, state) => {\\n return dispatch(state.waiter,\\n { type: \"order\", customer: ctx.self, wants: \\'pie\\' })\\n },\\n \\'put on table\\': (msg, ctx, _state) =>\\n console.log( `${ctx.self.name} sees \"${msg.food} \" appear on the table`),\\n \\'no pie left\\': (_msg, ctx, _state) =>\\n console.log( `${ctx.self.name} sulks…`)\\n}\\n```\\nThe interesting case is when we receive a \\'\\'hungry for pie\\'\" message, where we then send a message off to the waiter. (We\\'ll see how the customer knows about the waiter actor shortly.)\\n\\nHere\\'s the waiter\\'s code:\\n\\n```\\nconcurrency/actors/index.js\\n   const waiterActor = {\\n    \"order\": (msg, ctx, state) => {\\n    if (msg.wants == \"pie\") {\\n    dispatch(state.pieCase,\\n    { type: \"get slice\", customer: msg.customer, waiter: ctx.self\\n   })\\n    }\\n    else {\\n    console.dir( `Don\\'t know how to order ${msg.wants} `);\\n    }\\n    },\\n    \"add to order\": (msg, ctx) =>\\n    console.log( `Waiter adds ${msg.food} to ${msg.customer.name} \\'s order`),\\n    \"error\": (msg, ctx) => {\\n    dispatch(msg.customer, { type: \\'no pie left\\', msg: msg.msg });\\n    console.log( `\\\\nThe waiter apologizes to ${msg.customer.name} : ${msg.msg} \\n   `)\\n    }\\n   };\\n```\\nWhen it receives the \\'order\\' message from the customer, it checks to see if the request is for pie. If so, it sends a request to the pie case, passing references both to itself and the customer.\\n\\nThe pie case has state: an array of all the slices of pie it holds. (Again, we see how that gets set up shortly.) When it receives a \\'get slice\\' message from the waiter, it sees if it has any slices left. If it does, it passes the slice to the customer, tells the waiter to update the order, and finally returns an updated state, containing one less slice. Here\\'s the code:\\n\\nconcurrency/actors/index.js\\n\\n```\\nconst pieCaseActor = {\\n \\'get slice\\': (msg, context, state) => {\\n if (state.slices.length == 0) {\\n dispatch(msg.waiter,\\n { type: \\'error\\', msg: \"no pie left\", customer: msg.customer\\n})\\n return state\\n }\\n else {\\n var slice = state.slices.shift() + \" pie slice\";\\n dispatch(msg.customer,\\n { type: \\'put on table\\', food: slice });\\n dispatch(msg.waiter,\\n { type: \\'add to order\\', food: slice, customer: msg.customer\\n});\\n return state;\\n }\\n }\\n}\\n```\\nAlthough you\\'ll often find that actors are started dynamically by other actors, in our case we\\'ll keep it simple and start our actors manually. We will also pass each some initial state:\\n\\n- The pie case gets the initial list of pie slices it contains\\n- We\\'ll give the waiter a reference to the pie case\\n- We\\'ll give the customers a reference to the waiter\\n\\n```\\nconcurrency/actors/index.js\\n   const actorSystem = start();\\n   let pieCase = start_actor(\\n    actorSystem,\\n    \\'pie-case\\',\\n    pieCaseActor,\\n    { slices: [ \"apple\", \"peach\", \"cherry\"] });\\n   let waiter = start_actor(\\n    actorSystem,\\n```\\n\\n```\\n \\'waiter\\',\\n waiterActor,\\n { pieCase: pieCase });\\nlet c1 = start_actor(actorSystem, \\'customer1\\',\\n customerActor, { waiter: waiter });\\nlet c2 = start_actor(actorSystem, \\'customer2\\',\\n customerActor, { waiter: waiter });\\n```\\nAnd finally we kick it off. Our customers are greedy. Customer 1 asks for three slices of pie, and customer 2 asks for two:\\n\\n```\\nconcurrency/actors/index.js\\n```\\n\\n```\\ndispatch(c1, { type: \\'hungry for pie\\', waiter: waiter });\\ndispatch(c2, { type: \\'hungry for pie\\', waiter: waiter });\\ndispatch(c1, { type: \\'hungry for pie\\', waiter: waiter });\\ndispatch(c2, { type: \\'hungry for pie\\', waiter: waiter });\\ndispatch(c1, { type: \\'hungry for pie\\', waiter: waiter });\\nsleep(500)\\n .then(() => {\\n stop(actorSystem);\\n })\\n```\\nWhen we run it, we can see the actors communicating.[49] The order you see may well be different:\\n\\n```\\n$ node index.js\\ncustomer1 sees \"apple pie slice\" appear on the table\\ncustomer2 sees \"peach pie slice\" appear on the table\\nWaiter adds apple pie slice to customer1\\'s order\\nWaiter adds peach pie slice to customer2\\'s order\\ncustomer1 sees \"cherry pie slice\" appear on the table\\nWaiter adds cherry pie slice to customer1\\'s order\\nThe waiter apologizes to customer1: no pie left\\ncustomer1 sulks…\\nThe waiter apologizes to customer2: no pie left\\ncustomer2 sulks…\\n```\\n### **No Explicit Concurrency**\\n\\nIn the actor model, there\\'s no need to write any code to handle concurrency, as there is no shared state. There\\'s also no need to code in explicit end-toend \"do this, do that\" logic, as the actors work it out for themselves based on the messages they receive.\\n\\nThere\\'s also no mention of the underlying architecture. This set of components work equally well on a single processor, on multiple cores, or on multiple networked machines.\\n\\n## **Erlang Sets the Stage**\\n\\nThe Erlang language and runtime are great examples of an actor implementation (even though the inventors of Erlang hadn\\'t read the original Actor\\'s paper). Erlang calls actors *processes*, but they aren\\'t regular operating system processes. Instead, just like the actors we\\'ve been discussing, Erlang processes are lightweight (you can run millions of them on a single machine), and they communicate by sending messages. Each is isolated from the others, so there is no sharing of state.\\n\\nIn addition, the Erlang runtime implements a *supervision* system, which manages the lifetimes of processes, potentially restarting a process or set of processes in case of failure. And Erlang also offers hot-code loading: you can replace code in a running system without stopping that system. And the Erlang system runs some of the world\\'s most reliable code, often citing nine nines availability.\\n\\nBut Erlang (and it\\'s progeny Elixir) aren\\'t unique—there are actor implementations for most languages. Consider using them for your concurrent implementations.\\n\\n## **Related Sections Include**\\n\\n- Topic 28, *Decoupling*\\n- Topic 30, *Transforming Programming*\\n- Topic 36, *Blackboards*\\n\\n## **Challenges**\\n\\n- Do you currently have code that uses mutual exclusion to protect shared data. Why not try a prototype of the same code written using actors?\\n- The actor code for the diner only supports ordering slices of pie. Extend it to let customers order pie à la mode, with separate agents managing the pie slices and the scoops of ice cream. Arrange things so that it handles the situation where one or the other runs out.\\n\\n**Topic 36** Blackboards\\n\\n|\\n|  |\\n\\nConsider how detectives might use a *blackboard* to coordinate and solve a murder investigation. The chief inspector starts off by setting up a large blackboard in the conference room. On\\n\\nit, she writes a single question:\\n\\nH. Dumpty (Male, Egg): Accident? Murder?\\n\\nDid Humpty really fall, or was he pushed? Each detective may make contributions to this potential murder mystery by adding facts, statements from witnesses, any forensic evidence that might arise, and so on. As the data accumulates, a detective might notice a connection and post that observation or speculation as well. This process continues, across all shifts, with many different people and agents, until the case is closed. A sample blackboard is shown in the figure.\\n\\n![](_page_284_Figure_0.jpeg)\\n\\nSome key features of the blackboard approach are:\\n\\n- None of the detectives needs to know of the existence of any other detective—they watch the board for new information, and add their findings.\\n- The detectives may be trained in different disciplines, may have different levels of education and expertise, and may not even work in the same precinct. They share a desire to solve the case, but that\\'s all.\\n- Different detectives may come and go during the course of the process, and may work different shifts.\\n- There are no restrictions on what may be placed on the blackboard. It may be pictures, sentences, physical evidence, and so on.\\n\\nThis is a form of *laissez faire* concurrency. The detectives are independent processes, agents, actors, and so on. Some store facts on the blackboard. Others take facts off the board, maybe combining or processing them, and add more information to the board. Gradually the board helps them come to a conclusion.\\n\\nComputer-based blackboard systems were originally used in artificial intelligence applications where the problems to be solved were large and complex—speech recognition, knowledge-based reasoning systems, and so on.\\n\\nOne of the first blackboard systems was David Gelernter\\'s Linda. It stored facts as typed tuples. Applications could write new tuples into Linda, and query for existing tuples using a form of pattern matching.\\n\\nLater came distributed blackboard-like systems such as JavaSpaces and T Spaces. With these systems, you can store active Java objects—not just data—on the blackboard, and retrieve them by partial matching of fields (via templates and wildcards) or by subtypes. For example, suppose you had a type Author, which is a subtype of Person. You could search a blackboard containing Person objects by using an Author template with a lastName value of \"Shakespeare.\\'\\' You\\'d get Bill Shakespeare the author, but not Fred Shakespeare the gardener.\\n\\nThese systems never really took off, we believe, in part, because the need for the kind of concurrent cooperative processing hadn\\'t yet developed.\\n\\n## **A Blackboard in Action**\\n\\nSuppose we are writing a program to accept and process mortgage or loan applications. The laws that govern this area are odiously complex, with federal, state, and local governments all having their say. The lender must prove they have disclosed certain things, and must ask for certain information—but must *not* ask certain other questions, and so on, and so on. Beyond the miasma of applicable law, we also have the following problems to contend with:\\n\\n- Responses can arrive in any order. For instance, queries for a credit check or title search may take a substantial amount of time, while items such as name and address may be available immediately.\\n- Data gathering may be done by different people, distributed across different offices, in different time zones.\\n- Some data gathering may be done automatically by other systems. This data may arrive asynchronously as well.\\n- Nonetheless, certain data may still be dependent on other data. For instance, you may not be able to start the title search for a car until you get proof of ownership or insurance.\\n- The arrival of new data may raise new questions and policies. Suppose the credit check comes back with a less than glowing report; now you need these five extra forms and perhaps a blood sample.\\n\\nYou can try to handle every possible combination and circumstance using a workflow system. Many such systems exist, but they can be complex and programmer intensive. As regulations change, the workflow must be reorganized: people may have to change their procedures and hard-wired code may have to be rewritten.\\n\\nA blackboard, in combination with a rules engine that encapsulates the legal requirements, is an elegant solution to the difficulties found here. Order of data arrival is irrelevant: when a fact is posted it can trigger the appropriate rules. Feedback is easily handled as well: the output of any set of rules can post to the blackboard and cause the triggering of yet more applicable rules.\\n\\n## **Messaging Systems Can Be Like Blackboards**\\n\\nAs we\\'re writing this second edition, many applications are constructed using small, decoupled services, all communicating via some form of messaging system. These messaging systems (such as Kafka and NATS) do far more than simply send data from A to B. In particular, they offer persistence (in the form of an event log) and the ability to retrieve messages through a form of pattern matching. This means you can use them both as a blackboard system and/or as a platform on which you can run a bunch of actors.\\n\\n## **But It\\'s Not That Simple…**\\n\\nThe actor and/or blackboard and/or microservice approach to architecture removes a whole class of potential concurrency problems from your applications. But that benefit comes at a cost. These approaches are harder to reason about, because a lot of the action is indirect. You\\'ll find it helps to keep a central repository of message formats and/or APIs, particularly if the repository can generate the code and documentation for you. You\\'ll also need good tooling to be able to trace messages and facts as they progress through the system. (A useful technique is to add a unique *trace id* when a particular business function is initiated and then propagate it to all the actors involved. You\\'ll then be able to reconstruct what happens from the log files.)\\n\\nFinally, these kinds of system can be more troublesome to deploy and manage, as there are more moving parts. To some extent this is offset by the fact that the system is more granular, and can be updated by replacing individual actors, and not the whole system.\\n\\n## **Related Sections Include**\\n\\n- Topic 28, *Decoupling*\\n- Topic 29, *Juggling the Real World*\\n- Topic 33, *Breaking Temporal Coupling*\\n- Topic 35, *Actors and Processes*\\n\\n## **Exercises**\\n\\n#### **Exercise 24** (possible answer)\\n\\nWould a blackboard-style system be appropriate for the following applications? Why, or why not?\\n\\n*Image processing.* You\\'d like to have a number of parallel processes grab chunks of an image, process them, and put the completed chunk back.\\n\\n*Group calendaring.* You\\'ve got people scattered across the globe, in different time zones, and speaking different languages, trying to schedule a meeting.\\n\\n*Network monitoring tool.* The system gathers performance statistics and collects trouble reports, which agents use to look for trouble in the system.\\n\\n## **Challenges**\\n\\n- Do you use blackboard systems in the real world—the message board by the refrigerator, or the big whiteboard at work? What makes them effective? Are messages ever posted with a consistent format? Does it matter?\\n#### **Footnotes**\\n\\n- [46] Although UML has gradually faded, many of its individual diagrams still exist in one form or another, including the very useful *activity diagram*. For more information on all of the UML diagram types, see *UML Distilled: A Brief Guide to the Standard Object Modeling Language* [Fow04].\\n- [47] The names P and V come from the initial letters of Dutch words. However there is some discussion about exactly which words. The inventor of the technique, Edsger Dijkstra, has\\n\\nsuggested both *passering* and *prolaag* for P, and *vrijgave* and possibly *verhogen* for V.\\n\\n- [48] https://github.com/ncthbrt/nact\\n- [49] In order to run this code you\\'ll also need our wrapper functions, which are not shown here. You can download them from https://media.pragprog.com/titles/tpp20/code/concurrency/actors/index.js\\n\\nCopyright © 2020 Pearson Education, Inc.\\n\\n# Chapter 7\\n\\n# **While You Are Coding**\\n\\nConventional wisdom says that once a project is in the coding phase, the work is mostly mechanical, transcribing the design into executable statements. We think that this attitude is the single biggest reason that software projects fail, and many systems end up ugly, inefficient, poorly structured, unmaintainable, or just plain wrong.\\n\\nCoding is not mechanical. If it were, all the CASE tools that people pinned their hopes on way back in the early 1980s would have replaced programmers long ago. There are decisions to be made every minute decisions that require careful thought and judgment if the resulting program is to enjoy a long, accurate, and productive life.\\n\\nNot all decisions are even conscious. You can better harness your instincts and nonconscious thoughts when you Topic 37, *Listen to Your Lizard Brain*. We\\'ll see how to listen more carefully and look at ways of actively responding to these sometimes niggling thoughts.\\n\\nBut listening to your instincts doesn\\'t mean you can just fly on autopilot. Developers who don\\'t actively think about their code are programming by coincidence—the code might work, but there\\'s no particular reason why. In Topic 38, *Programming by Coincidence*, we advocate a more positive involvement with the coding process.\\n\\nWhile most of the code we write executes quickly, we occasionally develop algorithms that have the potential to bog down even the fastest processors. In Topic 39, *Algorithm Speed*, we discuss ways to estimate the speed of code, and we give some tips on how to spot potential problems before they happen.\\n\\nPragmatic Programmers think critically about all code, including our own. We constantly see room for improvement in our programs and our designs. In Topic 40, *Refactoring*, we look at techniques that help us fix up existing code continuously as we go.\\n\\nTesting is not about finding bugs, it\\'s about getting feedback on your code: aspects of design, the API, coupling, and so on. That means that the major benefits of testing happen when you think about and write the tests, not just when you run them. We\\'ll explore this idea in Topic 41, *Test to Code*.\\n\\nBut of course when you test your own code, you might bring your own biases to the task. In Topic 42, *Property-Based Testing* we\\'ll see how to have the computer do some wide-ranging testing for you and how to handle the inevitable bugs that come up.\\n\\nIt\\'s critical that you write code that is readable and easy to reason about. It\\'s a harsh world out there, filled with bad actors who are actively trying to break into your system and cause harm. We\\'ll discuss some very basic techniques and approaches to help you Topic 43, *Stay Safe Out There*.\\n\\nFinally, one of the hardest things in software development is Topic 44, *Naming Things*. We have to name a lot of things, and in many ways the names we choose define the reality we create. You need to stay aware of any potential semantic drift while you are coding.\\n\\nMost of us can drive a car largely on autopilot; we don\\'t explicitly command our foot to press a pedal, or our arm to turn the wheel—we just think \"slow down and turn right.\" However, good, safe drivers are\\n\\nconstantly reviewing the situation, checking for potential problems, and putting themselves into good positions in case the unexpected happens. The same is true of coding—it may be largely routine, but keeping your wits about you could well prevent a disaster.\\n\\n*Only human beings can look directly at something, have all the information they need to make an accurate prediction, perhaps even momentarily make the accurate prediction, and then say that it isn\\'t so.*\\n\\n*Gavin de Becker, The Gift of Fear*\\n\\nGavin de Becker\\'s life\\'s work is helping people to protect themselves. His book, *The Gift of Fear: And Other Survival Signals That Protect Us from Violence* [de 98], encapsulates his message. One of the key themes running through the book is that as sophisticated humans we have learned to ignore our more animal side; our\\n\\ninstincts, our lizard brain. He claims that most people who are attacked in the street are aware of feeling uncomfortable or nervous before the attack. These people just tell themselves they\\'re being silly. Then the figure emerges from the dark doorway….\\n\\nInstincts are simply a response to patterns packed into our nonconscious brain. Some are innate, others are learned through repetition. As you gain experience as a programmer, your brain is laying down layers of tacit knowledge: things that work, things that don\\'t work, the probable causes of a type of error, all the things you notice throughout your days. This is the part of your brain that hits the *save file* key when you stop to chat with someone, even when you don\\'t realize that you\\'re doing it.\\n\\nWhatever their source, instincts share one thing: they have no words. Instincts make you feel, not think. And so when an instinct is triggered, you don\\'t see a flashing lightbulb with a banner wrapped around it. Instead, you get nervous, or queasy, or feel like this is just too much work.\\n\\nThe trick is first to notice it is happening, and then to work out why. Let\\'s look first at a couple of common situations in which your inner lizard is\\n\\ntrying to tell you something. Then we\\'ll discuss how you can let that instinctive brain out of its protective wrapper.\\n\\n## **Fear of the Blank Page**\\n\\nEveryone fears the empty screen, the lonely blinking cursor surrounded by a whole bunch of nothing. Starting a new project (or even a new module in an existing project) can be an unnerving experience. Many of us would prefer to put off making the initial commitment of starting.\\n\\nWe think that there are two problems that cause this, and that both have the same solution.\\n\\nOne problem is that your lizard brain is trying to tell you something; there\\'s some kind of doubt lurking just below the surface of perception. And that\\'s important.\\n\\nAs a developer, you\\'ve been trying things and seeing which worked and which didn\\'t. You\\'ve been accumulating experience and wisdom. When you feel a nagging doubt, or experience some reluctance when faced with a task, it might be that experience trying to speak to you. Heed it. You may not be able to put your finger on exactly what\\'s wrong, but give it time and your doubts will probably crystallize into something more solid, something you can address. Let your instincts contribute to your performance.\\n\\nThe other problem is a little more prosaic: you might simply be afraid that you\\'ll make a mistake.\\n\\nAnd that\\'s a reasonable fear. We developers put a lot of ourselves into our code; we can take errors in that code as reflections on our competence. Perhaps there\\'s an element of *imposter syndrome*, too; we may think that this project is beyond us. We can\\'t see our way through to the end; we\\'ll get so far and then be forced to admit that we\\'re lost.\\n\\n## **Fighting Yourself**\\n\\nSometimes code just flies from your brain into the editor: ideas become bits with seemingly no effort.\\n\\nOther days, coding feels like walking uphill in mud. Taking each step requires tremendous effort, and every three steps you slide back two.\\n\\nBut, being a professional, you soldier on, taking step after muddy step: you have a job to do. Unfortunately, that\\'s probably the exact opposite of what you should do.\\n\\nYour code is trying to tell you something. It\\'s saying that this is harder than it should be. Maybe the structure or design is wrong, maybe you\\'re solving the wrong problem, or maybe you\\'re just creating an ant farm\\'s worth of bugs. Whatever the reason, your lizard brain is sensing feedback from the code, and it\\'s desperately trying to get you to listen.\\n\\n## **How to Talk Lizard**\\n\\nWe talk a lot about listening to your instincts, to your nonconscious, lizard brain. The techniques are always the same.\\n\\n![](_page_295_Picture_6.jpeg)\\n\\n- **Tip 61** Listen to Your Inner Lizard\\nFirst, stop what you\\'re doing. Give yourself a little time and space to let your brain organize itself. Stop thinking about the code, and do something that is fairly mindless for a while, away from a keyboard. Take a walk, have lunch, chat with someone. Maybe sleep on it. Let the ideas percolate up through the layers of your brain on their own: you can\\'t force it. Eventually they may bubble up to the conscious level, and you have one of those *a ha!* moments.\\n\\nIf that\\'s not working, try externalizing the issue. Make doodles about the code you\\'re writing, or explain it to a coworker (preferably one who isn\\'t a programmer), or to your rubber duck. Expose different parts of your brain\\n\\nto the issue, and see if any of them have a better handle on the thing that\\'s troubling you. We\\'ve lost track of the number of conversations we\\'ve had where one of us was explaining a problem to the other and suddenly went \"Oh! Of course!\" and broke off to fix it.\\n\\nBut maybe you\\'ve tried these things, and you\\'re still stuck. It\\'s time for action. We need to tell your brain that what you\\'re about to do doesn\\'t matter. And we do that by prototyping.\\n\\n## **It\\'s Playtime!**\\n\\nAndy and Dave have both spent hours looking at empty editor buffers. We\\'ll type in some code, then look at the ceiling, then get yet another drink, then type in some more code, then go read a funny story about a cat with two tails, then type some more code, then do select-all/delete and start again. And again. And again.\\n\\nAnd over the years we\\'ve found a brain hack that seems to work. Tell yourself you need to prototype something. If you\\'re facing a blank screen, then look for some aspect of the project that you want to explore. Maybe you\\'re using a new framework, and want to see how it does data binding. Or maybe it\\'s a new algorithm, and you want to explore how it works on edge cases. Or maybe you want to try a couple of different styles of user interaction.\\n\\nIf you\\'re working on existing code and it\\'s pushing back, then stash it away somewhere and prototype up something similar instead.\\n\\nDo the following.\\n\\n- 1. Write \"I\\'m prototyping\" on a sticky note, and stick it on the side of your screen.\\n- 2. Remind yourself that prototypes are meant to fail. And remind yourself that prototypes get thrown away, even if they don\\'t fail. There is no\\n\\ndownside to doing this.\\n\\n- 3. In your empty editor buffer, create a comment describing in one sentence what you want to learn or do.\\n- 4. Start coding.\\n\\nIf you start to have doubts, look at the sticky note.\\n\\nIf, in the middle of coding, that nagging doubt suddenly crystallizes into a solid concern, then address it.\\n\\nIf you get to the end of the experiment and you still feel uneasy, start again with the walk and the talk and the time off.\\n\\nBut, in our experience, at some point during the first prototype you will be surprised to find yourself humming along with your music, enjoying the feeling of creating code. The nervousness will have evaporated, replaced by a feeling of urgency: let\\'s get this done!\\n\\nAt this stage, you know what to do. Delete all the prototype code, throw away the sticky note, and fill that empty editor buffer with bright, shiny new code.\\n\\n## **Not Just** *Your* **Code**\\n\\nA large part of our job is dealing with existing code, often written by other people. Those people will have different instincts to you, and so the decisions they made will be different. Not necessarily worse; just different.\\n\\nYou can read their code mechanically, slogging through it making notes on stuff that seems important. It\\'s a chore, but it works.\\n\\nOr you can try an experiment. When you spot things done in a way that seems strange, jot it down. Continue doing this, and look for patterns. If you can see what drove them to write code that way, you may find that the job\\n\\nof understanding it becomes a lot easier. You\\'ll be able consciously to apply the patterns that they applied tacitly.\\n\\nAnd you might just learn something new along the way.\\n\\n## **Not Just Code**\\n\\nLearning to listen to your gut when coding is an important skill to foster. But it applies to the bigger picture are well. Sometimes a design just feels wrong, or some requirement makes you feel uneasy. Stop and analyze these feelings. If you\\'re in a supportive environment, express them out loud. Explore them. The chances are that there\\'s something lurking in that dark doorway. Listen to your instincts and avoid the problem before it jumps out at you.\\n\\n## **Related Sections Include**\\n\\n- Topic 13, *Prototypes and Post-it Notes*\\n- Topic 22, *Engineering Daybooks*\\n- Topic 46, *Solving Impossible Puzzles*\\n\\n## **Challenges**\\n\\n- Is there something you know you should do, but have put off because it feels a little scary, or difficult? Apply the techniques in this section. Time box it to an hour, maybe two, and promise yourself that when the bell rings you\\'ll delete what you did. What did you learn?\\n## **Topic 38** Programming by Coincidence\\n\\nDo you ever watch old black-and-white war movies? The weary soldier advances cautiously out of the brush. There\\'s a clearing ahead: are there any land mines, or is it safe to cross? There aren\\'t any indications that it\\'s a minefield—no signs, barbed wire, or craters. The soldier pokes the ground ahead of him with his bayonet and winces, expecting an explosion. There isn\\'t one. So he proceeds painstakingly through the field for a while, prodding and poking as he goes. Eventually, convinced that the field is safe, he straightens up and marches proudly forward, only to be blown to pieces.\\n\\nThe soldier\\'s initial probes for mines revealed nothing, but this was merely lucky. He was led to a false conclusion—with disastrous results.\\n\\nAs developers, we also work in minefields. There are hundreds of traps waiting to catch us each day. Remembering the soldier\\'s tale, we should be wary of drawing false conclusions. We should avoid programming by coincidence—relying on luck and accidental successes—in favor of *programming deliberately*.\\n\\n## **How to Program by Coincidence**\\n\\nSuppose Fred is given a programming assignment. Fred types in some code, tries it, and it seems to work. Fred types in some more code, tries it, and it still seems to work. After several weeks of coding this way, the program suddenly stops working, and after hours of trying to fix it, he still doesn\\'t know why. Fred may well spend a significant amount of time chasing this piece of code around without ever being able to fix it. No matter what he does, it just doesn\\'t ever seem to work right.\\n\\nFred doesn\\'t know why the code is failing because *he didn\\'t know why it worked in the first place*. It seemed to work, given the limited \"testing\\'\\' that Fred did, but that was just a coincidence. Buoyed by false confidence, Fred charged ahead into oblivion. Now, most intelligent people may know someone like Fred, but *we* know better. We don\\'t rely on coincidences—do we?\\n\\nSometimes we might. Sometimes it can be pretty easy to confuse a happy coincidence with a purposeful plan. Let\\'s look at a few examples.\\n\\n## **Accidents of Implementation**\\n\\nAccidents of implementation are things that happen simply because that\\'s the way the code is currently written. You end up relying on undocumented error or boundary conditions.\\n\\nSuppose you call a routine with bad data. The routine responds in a particular way, and you code based on that response. But the author didn\\'t intend for the routine to work that way—it was never even considered. When the routine gets \"fixed,\\'\\' your code may break. In the most extreme case, the routine you called may not even be designed to do what you want, but it *seems* to work okay. Calling things in the wrong order, or in the wrong context, is a related problem.\\n\\nHere it looks like Fred is desperately trying to get something out on the screen using some particular GUI rendering framework:\\n\\n```\\npaint();\\ninvalidate();\\nvalidate();\\nrevalidate();\\nrepaint();\\npaintImmediately();\\n```\\nBut these routines were never designed to be called this way; although they seem to work, that\\'s really just a coincidence.\\n\\nTo add insult to injury, when the scene finally does get drawn, Fred won\\'t try to go back and take out the spurious calls. \"It works now, better leave\\n\\nwell enough alone….\"\\n\\nIt\\'s easy to be fooled by this line of thought. Why should you take the risk of messing with something that\\'s working? Well, we can think of several reasons:\\n\\n- It may not really be working—it might just look like it is.\\n- The boundary condition you rely on may be just an accident. In different circumstances (a different screen resolution, more CPU cores), it might behave differently.\\n- Undocumented behavior may change with the next release of the library.\\n- Additional and unnecessary calls make your code slower.\\n- Additional calls increase the risk of introducing new bugs of their own.\\n\\nFor code you write that others will call, the basic principles of good modularization and of hiding implementation behind small, welldocumented interfaces can all help. A well-specified contract (see Topic 23, *Design by Contract*) can help eliminate misunderstandings.\\n\\nFor routines you call, rely only on documented behavior. If you can\\'t, for whatever reason, then document your assumption well.\\n\\n## **Close Enough Isn\\'t**\\n\\nWe once worked on a large project that reported on data fed from a very large number of hardware data collection units out in the field. These units spanned states and time zones, and for various logistical and historical reasons, each unit was set to local time.[50] As a result of conflicting time zone interpretations and inconsistencies in Daylight Savings Time policies, results were almost always wrong, but only off by one. The developers on the project had gotten into the habit of just adding one or subtracting one to get the correct answer, reasoning that it was *only* off by one in this one situation. And then the next function would see the value as off by the one the other way, and change it back.\\n\\nBut the fact that it was \"only\" off by one some of the time was a coincidence, masking a deeper and more fundamental flaw. Without a proper model of time handling, the entire large code base had devolved over time to an untenable mass of +1 and -1 statements. Ultimately, none of it was correct and the project was scrapped.\\n\\n### **Phantom Patterns**\\n\\nHuman beings are designed to see patterns and causes, even when it\\'s just a coincidence. For example, Russian leaders always alternate between being bald and hairy: a bald (or obviously balding) state leader of Russia has succeeded a non-bald (\"hairy\") one, and vice versa, for nearly 200 years.[51]\\n\\nBut while you wouldn\\'t write code that depended on the next Russian leader being bald or hairy, in some domains we think that way all the time. Gamblers imagine patterns in lottery numbers, dice games, or roulette, when in fact these are statistically independent events. In finance, stock and bond trading are similarly rife with coincidence instead of actual, discernible patterns.\\n\\nA log file that shows an intermittent error every 1,000 requests may be a difficult-to-diagnose race condition, or may be a plain old bug. Tests that seem to pass on your machine but not on the server might indicate a difference between the two environments, or maybe it\\'s just a coincidence.\\n\\nDon\\'t assume it, prove it.\\n\\n### **Accidents of Context**\\n\\nYou can have \"accidents of context\" as well. Suppose you are writing a utility module. Just because you are currently coding for a GUI environment, does the module have to rely on a GUI being present? Are you relying on English-speaking users? Literate users? What else are you relying on that isn\\'t guaranteed?\\n\\nAre you relying on the current directory being writable? On certain environment variables or configuration files being present? On the time on the server being accurate—within what tolerance? Are you relying on network availability and speed?\\n\\nWhen you copied code from the first answer you found on the net, are you sure your context is the same? Or are you building \"cargo cult\" code, merely imitating form without content?[52]\\n\\nFinding an answer that happens to fit is not the same as the right answer.\\n\\n![](_page_303_Picture_4.jpeg)\\n\\n- **Tip 62** Don\\'t Program by Coincidence\\n### **Implicit Assumptions**\\n\\nCoincidences can mislead at all levels—from generating requirements through to testing. Testing is particularly fraught with false causalities and coincidental outcomes. It\\'s easy to assume that *X* causes *Y*, but as we said in Topic 20, *Debugging*: don\\'t assume it, prove it.\\n\\nAt all levels, people operate with many assumptions in mind—but these assumptions are rarely documented and are often in conflict between different developers. Assumptions that aren\\'t based on well-established facts are the bane of all projects.\\n\\n## **How to Program Deliberately**\\n\\nWe want to spend less time churning out code, catch and fix errors as early in the development cycle as possible, and create fewer errors to begin with. It helps if we can program deliberately:\\n\\n- Always be aware of what you are doing. Fred let things get slowly out of hand, until he ended up boiled, like the frog here.\\n- Can you explain the code, in detail, to a more junior programmer? If not, perhaps you are relying on coincidences.\\n- Don\\'t code in the dark. Build an application you don\\'t fully grasp, or use a technology you don\\'t understand, and you\\'ll likely be bitten by coincidences. If you\\'re not sure why it works, you won\\'t know why it fails.\\n- Proceed from a plan, whether that plan is in your head, on the back of a cocktail napkin, or on a whiteboard.\\n- Rely only on reliable things. Don\\'t depend on assumptions. If you can\\'t tell if something is reliable, assume the worst.\\n- Document your assumptions. Topic 23, *Design by Contract*, can help clarify your assumptions in your own mind, as well as help communicate them to others.\\n- Don\\'t just test your code, but test your assumptions as well. Don\\'t guess; actually try it. Write an assertion to test your assumptions (see Topic 25, *Assertive Programming*). If your assertion is right, you have improved the documentation in your code. If you discover your assumption is wrong, then count yourself lucky.\\n- Prioritize your effort. Spend time on the important aspects; more than likely, these are the hard parts. If you don\\'t have fundamentals or infrastructure correct, brilliant bells and whistles will be irrelevant.\\n- Don\\'t be a slave to history. Don\\'t let existing code dictate future code. All code can be replaced if it is no longer appropriate. Even within one program, don\\'t let what you\\'ve already done constrain what you do next—be ready to refactor (see Topic 40, *Refactoring*). This decision\\n\\nmay impact the project schedule. The assumption is that the impact will be less than the cost of *not* making the change.[53]\\n\\nSo next time something seems to work, but you don\\'t know why, make sure it isn\\'t just a coincidence.\\n\\n## **Related Sections Include**\\n\\n- Topic 4, *Stone Soup and Boiled Frogs*\\n- Topic 9, *DRY—The Evils of Duplication*\\n- Topic 23, *Design by Contract*\\n- Topic 34, *Shared State Is Incorrect State*\\n- Topic 43, *Stay Safe Out There*\\n\\n## **Exercises**\\n\\n### **Exercise 25** (possible answer)\\n\\nA data feed from a vendor gives you an array of tuples representing keyvalue pairs. The key of DepositAccount will hold a string of the account number in the corresponding value:\\n\\n```\\n[\\n ...\\n { :DepositAccount, \"564-904-143-00\"}\\n ...\\n]\\n```\\nIt worked perfectly in test on the 4-core developer laptops and on the 12 core build machine, but on the production servers running in containers, you keep getting the wrong account numbers. What\\'s going on?\\n\\n## **Exercise 26** (possible answer)\\n\\nYou\\'re coding an autodialer for voice alerts, and have to manage a database of contact information. The ITU specifies that phone numbers should be no longer than 15 digits, so you store the contact\\'s phone number in a numeric\\n\\nfield guaranteed to hold at least 15 digits. You\\'ve tested in thoroughly throughout North America and everything seems fine, but suddenly you\\'re getting a rash of complaints from other parts of the world. Why?\\n\\n**Exercise 27** (possible answer)\\n\\nYou have written an app that scales up common recipes for a cruise ship dining room that seats 5,000. But you\\'re getting complaints that the conversions aren\\'t precise. You check, and the code uses the conversion formula of 16 cups to a gallon. That\\'s right, isn\\'t it?\\n\\nIn Topic 15, *Estimating*, we talked about estimating things such as how long it takes to walk across town, or how long a project will take to finish. However, there is another kind of estimating that Pragmatic Programmers use almost daily: estimating the resources that algorithms use—time, processor, memory, and so on.\\n\\nThis kind of estimating is often crucial. Given a choice between two ways of doing something, which do you pick? You know how long your program runs with 1,000 records, but how will it scale to 1,000,000? What parts of the code need optimizing?\\n\\nIt turns out that these questions can often be answered using common sense, some analysis, and a way of writing approximations called the *Big-O* notation.\\n\\n## **What Do We Mean by Estimating Algorithms?**\\n\\nMost nontrivial algorithms handle some kind of variable input—sorting strings, inverting an matrix, or decrypting a message with an -bit key. Normally, the size of this input will affect the algorithm: the larger the input, the longer the running time or the more memory used.\\n\\nIf the relationship were always linear (so that the time increased in direct proportion to the value of ), this section wouldn\\'t be important. However, most significant algorithms are not linear. The good news is that many are sublinear. A binary search, for example, doesn\\'t need to look at every candidate when finding a match. The bad news is that other algorithms are considerably worse than linear; runtimes or memory requirements increase\\n\\nfar faster than . An algorithm that takes a minute to process ten items may take a lifetime to process 100.\\n\\nWe find that whenever we write anything containing loops or recursive calls, we subconsciously check the runtime and memory requirements. This is rarely a formal process, but rather a quick confirmation that what we\\'re doing is sensible in the circumstances. However, we sometimes *do* find ourselves performing a more detailed analysis. That\\'s when Big-O notation comes in handy.\\n\\n## **Big-O Notation**\\n\\nThe Big-O notation, written , is a mathematical way of dealing with approximations. When we write that a particular sort routine sorts records in time, we are simply saying that the worst-case time taken will vary as the square of . Double the number of records, and the time will increase roughly fourfold. Think of the as meaning *on the order of.*\\n\\nThe notation puts an upper bound on the value of the thing we\\'re measuring (time, memory, and so on). If we say a function takes time, then we know that the upper bound of the time it takes will not grow faster than . Sometimes we come up with fairly complex functions, but because the highest-order term will dominate the value as increases, the convention is to remove all low-order terms, and not to bother showing any constant multiplying factors:\\n\\n$\\\\begin{array}{c}\\\\includegraphics[height=36.135pt]{28.eps}\\\\end{array}$\\n\\nThis is actually a feature of the notation—one algorithm may be 1,000 times faster than another algorithm, but you won\\'t know it from the notation. Big-O is never going to give you actual numbers for time or\\n\\nmemory or whatever: it simply tells you how these values will change as the input changes.\\n\\nFigure 3, *Runtimes of various algorithms* shows several common notations you\\'ll come across, along with a graph comparing running times of algorithms in each category. Clearly, things quickly start getting out of hand once we get over .\\n\\nFor example, suppose you\\'ve got a routine that takes one second to process 100 records. How long will it take to process 1,000? If your code is , then it will still take one second. If it\\'s , then you\\'ll probably be waiting about three seconds. will show a linear increase to ten seconds, while an will take some 33 seconds. If you\\'re unlucky enough to have an routine, then sit back for 100 seconds while it does its stuff. And if you\\'re using an exponential algorithm , you might want to make a cup of coffee—your routine should finish in about years. Let us know how the universe ends.\\n\\nThe notation doesn\\'t apply just to time; you can use it to represent any other resources used by an algorithm. For example, it is often useful to be able to model memory consumption (see the exercises for an example).\\n\\n| Constant (access element in array, simple statements) |\\n| --- |\\n| Logarithmic (binary search). The base of the logarithm |\\n| doesn\\'t matter, so this is equivalent . |\\n| Linear (sequential search) |\\n| Worse than linear, but not much worse. (Average runtime |\\n| of quicksort, heapsort) |\\n| Square law (selection and insertion sorts) |\\n| Cubic (multiplication of two matrices) |\\n\\n![](_page_310_Figure_0.jpeg)\\n\\n## **Common Sense Estimation**\\n\\nYou can estimate the order of many basic algorithms using common sense.\\n\\n*Simple loops*\\n\\nIf a simple loop runs from to , then the algorithm is likely to be —time increases linearly with . Examples include exhaustive searches, finding the maximum value in an array, and generating checksums.\\n\\n#### *Nested loops*\\n\\nIf you nest a loop inside another, then your algorithm becomes , where and are the two loops\\' limits. This commonly occurs in simple sorting algorithms, such as bubble sort, where the outer loop scans each element in the array in turn, and the inner loop works out where to place that element in the sorted result. Such sorting algorithms tend to be .\\n\\n#### *Binary chop*\\n\\nIf your algorithm halves the set of things it considers each time around the loop, then it is likely to be logarithmic, . A binary search of a sorted list, traversing a binary tree, and finding the first set bit in a machine word can all be .\\n\\n*Divide and conquer*\\n\\nAlgorithms that partition their input work on the two halves independently, and then combine the result can be . The classic example is quicksort, which works by partitioning the data into two halves and recursively sorting each. Although technically , because its behavior degrades when it is fed sorted input, the average runtime of quicksort is .\\n\\n#### *Combinatoric*\\n\\nWhenever algorithms start looking at the permutations of things, their running times may get out of hand. This is because permutations involve factorials (there are permutations of the digits from 1 to 5). Time a combinatoric algorithm for five elements: it will take six times longer to run it for six, and 42 times longer for seven. Examples include algorithms for many of the acknowledged *hard* problems—the traveling salesman problem, optimally packing things into a container, partitioning a set of numbers so that each set has the same total, and so on. Often, heuristics are used\\n\\nto reduce the running times of these types of algorithms in particular problem domains.\\n\\n## **Algorithm Speed in Practice**\\n\\nIt\\'s unlikely that you\\'ll spend much time during your career writing sort routines. The ones in the libraries available to you will probably outperform anything you may write without substantial effort. However, the basic kinds of algorithms we\\'ve described earlier pop up time and time again. Whenever you find yourself writing a simple loop, you know that you have an algorithm. If that loop contains an inner loop, then you\\'re looking at . You should be asking yourself how large these values can get. If the numbers are bounded, then you\\'ll know how long the code will take to run. If the numbers depend on external factors (such as the number of records in an overnight batch run, or the number of names in a list of people), then you might want to stop and consider the effect that large values may have on your running time or memory consumption.\\n\\n## **Tip 63** Estimate the Order of Your Algorithms\\n\\nThere are some approaches you can take to address potential problems. If you have an algorithm that is , try to find a divide-and-conquer approach that will take you down to .\\n\\nIf you\\'re not sure how long your code will take, or how much memory it will use, try running it, varying the input record count or whatever is likely to impact the runtime. Then plot the results. You should soon get a good idea of the shape of the curve. Is it curving upward, a straight line, or flattening off as the input size increases? Three or four points should give you an idea.\\n\\nAlso consider just what you\\'re doing in the code itself. A simple loop may well perform better than a complex, one for smaller values of , particularly if the algorithm has an expensive inner loop.\\n\\nIn the middle of all this theory, don\\'t forget that there are practical considerations as well. Runtime may look like it increases linearly for small input sets. But feed the code millions of records and suddenly the time degrades as the system starts to thrash. If you test a sort routine with random input keys, you may be surprised the first time it encounters ordered input. Try to cover both the theoretical and practical bases. After all this estimating, the only timing that counts is the speed of your code, running in the production environment, with real data. This leads to our next tip.\\n\\n**Tip 64** Test Your Estimates\\n\\nIf it\\'s tricky getting accurate timings, use *code profilers* to count the number of times the different steps in your algorithm get executed, and plot these figures against the size of the input.\\n\\n### **Best Isn\\'t Always Best**\\n\\nYou also need to be pragmatic about choosing appropriate algorithms—the fastest one is not always the best for the job. Given a small input set, a straightforward insertion sort will perform just as well as a quicksort, and will take you less time to write and debug. You also need to be careful if the algorithm you choose has a high setup cost. For small input sets, this setup may dwarf the running time and make the algorithm inappropriate.\\n\\nAlso be wary of *premature optimization*. It\\'s always a good idea to make sure an algorithm really is a bottleneck before investing your precious time trying to improve it.\\n\\n## **Related Sections Include**\\n\\n- Topic 15, *Estimating*\\n## **Challenges**\\n\\n- Every developer should have a feel for how algorithms are designed and analyzed. Robert Sedgewick has written a series of accessible books on the subject (*Algorithms* [SW11]*An Introduction to the Analysis of Algorithms* [SF13] and others). We recommend adding one of his books to your collection, and making a point of reading it.\\n- For those who like more detail than Sedgewick provides, read Donald Knuth\\'s definitive *Art of Computer Programming* books, which analyze a wide range of algorithms.\\n\\t- *The Art of Computer Programming, Volume 1: Fundamental Algorithms* [Knu98]\\n\\t- *The Art of Computer Programming, Volume 2: Seminumerical Algorithms* [Knu98a]\\n\\t- *The Art of Computer Programming, Volume 3: Sorting and Searching* [Knu98b]\\n\\t- *The Art of Computer Programming, Volume 4A: Combinatorial Algorithms, Part 1* [Knu11].\\n- In the first exercise that follows we look at sorting arrays of long integers. What is the impact if the keys are more complex, and the overhead of key comparison is high? Does the key structure affect the efficiency of the sort algorithms, or is the fastest sort always fastest?\\n\\n## **Exercises**\\n\\n## **Exercise 28** (possible answer)\\n\\nWe coded a set of simple sort routines[54] in Rust. Run them on various machines available to you. Do your figures follow the expected curves? What can you deduce about the relative speeds of your machines? What are the effects of various compiler optimization settings?\\n\\n**Exercise 29** (possible answer)\\n\\nIn *Common Sense Estimation*, we claimed that a binary chop is . Can you prove this?\\n\\n**Exercise 30** (possible answer)\\n\\nIn Figure 3, *Runtimes of various algorithms*, we claimed that is the same as (or indeed logarithms to any base). Can you explain why?\\n\\n**Topic 40** Refactoring\\n\\n*Change and decay in all around I see...*\\n\\n*H. F. Lyte, Abide With Me*\\n\\nAs a program evolves, it will become necessary to rethink earlier decisions and rework portions of the code. This process is perfectly natural. Code needs to evolve; it\\'s not a static thing.\\n\\nUnfortunately, the most common metaphor for software development is building construction. Bertrand Meyer\\'s classic work *Object-Oriented Software Construction* [Mey97] uses the term \"Software Construction,\" and even your humble authors edited the *Software Construction* column for IEEE Software in the early 2000s.[55]\\n\\nBut using construction as the guiding metaphor implies the following steps:\\n\\n- 1. An architect draws up blueprints.\\n- 2. Contractors dig the foundation, build the superstructure, wire and plumb, and apply finishing touches.\\n- 3. The tenants move in and live happily ever after, calling building maintenance to fix any problems.\\n\\nWell, software doesn\\'t quite work that way. Rather than construction, software is more like *gardening*—it is more organic than concrete. You plant many things in a garden according to an initial plan and conditions. Some thrive, others are destined to end up as compost. You may move plantings relative to each other to take advantage of the interplay of light and shadow, wind and rain. Overgrown plants get split or pruned, and colors that clash may get moved to more aesthetically pleasing locations. You pull weeds, and you fertilize plantings that are in need of some extra\\n\\nhelp. You constantly monitor the health of the garden, and make adjustments (to the soil, the plants, the layout) as needed.\\n\\nBusiness people are comfortable with the metaphor of building construction: it is more scientific than gardening, it\\'s repeatable, there\\'s a rigid reporting hierarchy for management, and so on. But we\\'re not building skyscrapers—we aren\\'t as constrained by the boundaries of physics and the real world.\\n\\nThe gardening metaphor is much closer to the realities of software development. Perhaps a certain routine has grown too large, or is trying to accomplish too much—it needs to be split into two. Things that don\\'t work out as planned need to be weeded or pruned.\\n\\nRewriting, reworking, and re-architecting code is collectively known as *restructuring*. But there\\'s a subset of that activity that has become practiced as *refactoring*.\\n\\n*Refactoring* [Fow19] is defined by Martin Fowler as a:\\n\\ndisciplined technique for restructuring an existing body of code, altering its internal structure without changing its external behavior.\\n\\nThe critical parts of this definition are that:\\n\\n- 1. The activity is disciplined, not a free-for-all\\n- 2. External behavior does not change; this is not the time to add features\\n\\nRefactoring is not intended to be a special, high-ceremony, once-in-a-while activity, like plowing under the whole garden in order to replant. Instead, refactoring is a day-to-day activity, taking low-risk small steps, more like weeding and raking. Instead of a free-for-all, wholesale rewrite of the codebase, it\\'s a targeted, precision approach to help keep the code easy to change.\\n\\nIn order to guarantee that the external behavior hasn\\'t changed, you need good, automated unit testing that validates the behavior of the code.\\n\\n## **When Should You Refactor?**\\n\\nYou refactor when you\\'ve learned something; when you understand something better than you did last year, yesterday, or even just ten minutes ago.\\n\\nPerhaps you\\'ve come across a stumbling block because the code doesn\\'t quite fit anymore, or you notice two things that should really be merged, or anything else at all strikes you as being \"wrong,\" *don\\'t hesitate to change it*. There\\'s no time like the present. Any number of things may cause code to qualify for refactoring:\\n\\n#### *Duplication*\\n\\nYou\\'ve discovered a violation of the DRY principle.\\n\\n#### *Nonorthogonal design*\\n\\nYou\\'ve discovered something that could be made more orthogonal.\\n\\n### *Outdated knowledge*\\n\\nThings change, requirements drift, and your knowledge of the problem increases. Code needs to keep up.\\n\\n### *Usage*\\n\\nAs the system gets used by real people under real circumstances, you realize some features are now more important than previously thought, and \"must have\" features perhaps weren\\'t.\\n\\n### *Performance*\\n\\nYou need to move functionality from one area of the system to another to improve performance.\\n\\n### *The Tests Pass*\\n\\nYes. Seriously. We did say that refactoring should be a small scale activity, backed up by good tests. So when you\\'ve added a small amount of code, and that one extra test passes, you now have a great opportunity to dive in and tidy up what you just wrote.\\n\\nRefactoring your code—moving functionality around and updating earlier decisions—is really an exercise in *pain management*. Let\\'s face it, changing source code around can be pretty painful: it was working, maybe it\\'s better to leave well enough alone. Many developers are reluctant to go in and reopen a piece of code just because it isn\\'t quite right.\\n\\n## **Real-World Complications**\\n\\nSo you go to your teammates or client and say, \"This code works, but I need another week to completely refactor it.\"\\n\\nWe can\\'t print their reply.\\n\\nTime pressure is often used as an excuse for not refactoring. But this excuse just doesn\\'t hold up: fail to refactor now, and there\\'ll be a far greater time investment to fix the problem down the road—when there are more dependencies to reckon with. Will there be more time available then? Nope.\\n\\nYou might want to explain this principle to others by using a medical analogy: think of the code that needs refactoring as \"a growth.\" Removing it requires invasive surgery. You can go in now, and take it out while it is still small. Or, you could wait while it grows and spreads—but removing it then will be both more expensive and more dangerous. Wait even longer, and you may lose the patient entirely.\\n\\n## **Tip 65** Refactor Early, Refactor Often\\n\\nCollateral damage in code can be just as deadly over time (see Topic 3, *Software Entropy*). Refactoring, as with most things, is easier to do while the issues are small, as an ongoing activity while coding. You shouldn\\'t need \"a week to refactor\" a piece of code—that\\'s a full-on rewrite. If that level of disruption is necessary, then you might well not be able to do it immediately. Instead, make sure that it gets placed on the schedule. Make sure that users of the affected code *know* that it is scheduled to be rewritten and how this might affect them.\\n\\n## **How Do You Refactor?**\\n\\nRefactoring started out in the Smalltalk community, and had just started to gain a wider audience when we wrote the first edition of this book, probably thanks to the first major book on refactoring (*Refactoring: Improving the Design of Existing Code* [Fow19], now in its second edition).\\n\\nAt its heart, refactoring is redesign. Anything that you or others on your team designed can be redesigned in light of new facts, deeper understandings, changing requirements, and so on. But if you proceed to rip up vast quantities of code with wild abandon, you may find yourself in a worse position than when you started.\\n\\nClearly, refactoring is an activity that needs to be undertaken slowly, deliberately, and carefully. Martin Fowler offers the following simple tips on how to refactor without doing more harm than good:[56]\\n\\n- 1. Don\\'t try to refactor and add functionality at the same time.\\n- 2. Make sure you have good tests before you begin refactoring. Run the tests as often as possible. That way you will know quickly if your changes have broken anything.\\n- 3. Take short, deliberate steps: move a field from one class to another, split a method, rename a variable. Refactoring often involves making many localized changes that result in a larger-scale change. If you keep your steps small, and test after each step, you will avoid prolonged debugging.[57]\\n\\n#### **Automatic Refactoring**\\n\\nBack in the first edition we noted that, \"this technology has yet to appear outside of the Smalltalk world, but this is likely to change….\" And indeed, it did, as automatic refactoring is available in many IDEs and for most mainstream languages.\\n\\nThese IDEs can rename variables and methods, split a long routine into smaller ones, automatically propagating the required changes, drag and drop to assist you in moving code, and so on.\\n\\nWe\\'ll talk more about testing at this level in Topic 41, *Test to Code*, and larger-scale testing in *Ruthless and Continuous Testing*, but Mr. Fowler\\'s point of maintaining good regression tests is the key to refactoring safely.\\n\\nIf you have to go beyond refactoring and end up changing external behavior or interfaces, then it can help to deliberately break the build: old clients of this code should fail to compile. That way you\\'ll know what needs updating. Next time you see a piece of code that isn\\'t quite as it should be, fix it. Manage the pain: if it hurts now, but is going to hurt even more later, you might as well get it over with. Remember the lessons of Topic 3, *Software Entropy*: don\\'t live with broken windows.\\n\\n## **Related Sections Include**\\n\\n- Topic 3, *Software Entropy*\\n- Topic 9, *DRY—The Evils of Duplication*\\n- Topic 12, *Tracer Bullets*\\n- Topic 27, *Don\\'t Outrun Your Headlights*\\n- Topic 44, *Naming Things*\\n- Topic 48, *The Essence of Agility*\\n\\n**Topic 41** Test to Code\\n\\nThe first edition of this book was written in more primitive times, when most developers wrote no tests—why bother, they thought, the world was going to end in the year 2000 anyway.\\n\\nIn that book, we had a section on how to build code that was easy to test. It was a sneaky way of convincing developers to actually write tests.\\n\\nThese are more enlightened times. If there are any developers still not writing tests, they at least know that they should be.\\n\\nBut there\\'s still a problem. When we ask developers *why* they write tests, they look at us as if we just asked if they still coded using punched cards and they\\'d say \"to make sure the code works,\" with an unspoken \"you dummy\" at the end. And we think that\\'s wrong.\\n\\nSo what do *we* think is important about testing? And how do we think you should go about it?\\n\\nLet\\'s start with the bold statement:\\n\\n![](_page_322_Picture_7.jpeg)\\n\\n## **Tip 66** Testing Is Not About Finding Bugs\\n\\nWe believe that the major benefits of testing happen when you think about and write the tests, not when you run them.\\n\\n## **Thinking About Tests**\\n\\nIt\\'s a Monday morning and you settle in to start work on some new code. You have to write something that queries the database to return a list of\\n\\npeople who watch more than 10 videos a week on your \"world\\'s funniest dishwashing videos\" site.\\n\\nYou fire up your editor, and start by writing the function that performs the query:\\n\\n```\\ndef return_avid_viewers do\\n # ... hmmm ...\\nend\\n```\\nStop! How do you know that what you\\'re about to do is a good thing?\\n\\nThe answer is that you can\\'t know that. No one can. But thinking about tests can make it more likely. Here\\'s how that works.\\n\\nStart by imagining that you\\'d finished writing the function and now had to test it. How would you do that? Well, you\\'d want to use some test data, which probably means you want to work in a database you control. Now some frameworks can handle that for you, running tests against a test database, but in our case that means we should be passing the database instance into our function rather than using some global one, as that allows us to change it while testing:\\n\\n```\\ndef return_avid_users(db) do\\n```\\nThen we have to think about how we\\'d populate that test data. The requirement asks for a \"list of people who watch more than 10 videos a week.\" So we look at the database schema for fields that might help. We find two likely fields in a table of who-watched-what: opened_video and completed_video. To write our test data, we need to know which field to use. But we don\\'t know what the requirement means, and our business contact is out. Let\\'s just cheat and pass in the name of the field (which will allow us to test what we have, and potentially change it later):\\n\\n```\\ndef return_avid_users(db, qualifying_field_name) do\\n```\\nWe started by thinking about our tests, and without writing a line of code, we\\'ve already made two discoveries and used them to change the API of our method.\\n\\n## **Tests Drive Coding**\\n\\nIn the previous example, thinking about testing made us reduce coupling in our code (by passing in a database connection rather than using a global one) and increase flexibility (by making the name of the field we test a parameter). Thinking about writing a test for our method made us look at it from the outside, as if we were a client of the code, and not its author.\\n\\n## **Tip 67** A Test Is the First User of Your Code\\n\\nWe think this is probably the biggest benefit offered by testing: testing is vital feedback that guides your coding.\\n\\nA function or method that is tightly coupled to other code is hard to test, because you have to set up all that environment before you can even run your method. So making your stuff testable also reduces its coupling.\\n\\nAnd before you can test something, you have to understand it. That sounds silly, but in reality we\\'ve all launched into a piece of code based on a nebulous understanding of what we had to do. We assure ourselves that we\\'ll work it out as we go along. Oh, and we\\'ll add all the code to support the boundary conditions later, too. Oh, and the error handling. And the code ends up five times longer than it should because it\\'s full of conditional logic and special cases. But shine the light of a test on that code, and things become clearer. If you think about testing boundary conditions and how that will work *before* you start coding, you may well find the patterns in the logic that\\'ll simplify the function. If you think about the error conditions you\\'ll need to test, you\\'ll structure your function accordingly.\\n\\n### **Test-Driven Development**\\n\\nThere\\'s a school of programming that says that, given all the benefits of thinking about tests up front, why not go ahead and write them up front too? They practice something called *test-driven development* or *TDD*. You\\'ll also see this called *test-first development*. [58]\\n\\nThe basic cycle of TDD is:\\n\\n- 1. Decide on a small piece of functionality you want to add.\\n- 2. Write a test that will pass once that functionality is implemented.\\n- 3. Run all tests. Verify that the only failure is the one you just wrote.\\n- 4. Write the smallest amount of code needed to get the test to pass, and verify that the tests now run cleanly.\\n- 5. Refactor your code: see if there is a way to improve on what you just wrote (the test or the function). Make sure the tests still pass when you\\'re done.\\n\\nThe idea is that this cycle should be very short: a matter of minutes, so that you\\'re constantly writing tests and then getting them to work.\\n\\nWe see a major benefit in TDD for people just starting out with testing. If you follow the TDD workflow, you\\'ll guarantee that you always have tests for your code. And that means you\\'ll always be thinking about your tests.\\n\\nHowever, we\\'ve also seen people become slaves to TDD. This manifests itself in a number of ways:\\n\\n- They spend inordinate amounts of time ensuring that they always have 100% test coverage.\\n- They have lots of redundant tests. For example, before writing a class for the first time, many TDD adherents will first write a failing test\\n\\nthat simply references the class\\'s name. It fails, then they write an empty class definition and it passes. But now you have a test that does absolutely nothing; the next test you write will also reference the class, and so it makes the first unnecessary. There\\'s more stuff to change if the class name changes later. And this is just a trivial example.\\n\\n- Their designs tend to start at the bottom and work their way up. (See *Bottom-Up vs. Top-Down vs. The Way You Should Do It*.)\\n#### **Bottom-Up vs. Top-Down vs. The Way You Should Do It**\\n\\nBack when computing was young and carefree, there were two schools of design: top-down and bottom-up. The top-down folks said you should start with the overall problem you\\'re trying to solve and break it into a small number of pieces. Then break each of these into smaller pieces, and so on, until you end up with pieces small enough to express in code.\\n\\nThe bottom-up folks build code like you\\'d build a house. They start at the bottom, producing a layer of code that gives them some abstractions that are closer to the problem they are trying to solve. Then they add another layer, with higher-level abstractions. They keep on until the final layer is an abstraction that solves the problem. \"Make it so….\"\\n\\nNeither school actually works, because both ignore one of the most important aspects of software development: we don\\'t know what we\\'re doing when we start. The top-down folks assume they can express the whole requirement up front: they can\\'t. The bottom-up folks assume they can build a list of abstractions which will take them eventually to a single toplevel solution, but how can they decide on the functionality of layers when they don\\'t know where they are heading?\\n\\n#### **Tip 68** Build End-to-End, Not Top-Down or Bottom Up\\n\\nWe strongly believe that the only way to build software is incrementally. Build small pieces of end-to-end functionality, learning about the problem as you go. Apply this learning as you continue to flesh out the code, involve the customer at each step, and have them guide the process.\\n\\nBy all means practice TDD. But, if you do, don\\'t forget to stop every now and then and look at the big picture. It is easy to become seduced by the\\n\\ngreen \"tests passed\" message, writing lots of code that doesn\\'t actually get you closer to a solution.\\n\\n## **TDD: You Need to Know Where You\\'re Going**\\n\\nThe old joke asks \"How do you eat an elephant?\" The punchline: \"One bite at a time.\" And this idea is often touted as a benefit of TDD. When you can\\'t comprehend the whole problem, take small steps, one test at a time. However, this approach can mislead you, encouraging you to focus on and endlessly polish the easy problems while ignoring the real reason you\\'re coding. An interesting example of this happened in 2006, when Ron Jeffries, a leading figure in the agility movement, started a series of blog posts which documented his test-driven coding of a Sudoko solver. [59] After five posts, he\\'d refined the representation of the underlying board, refactoring a number of times until he was happy with the object model. But then he abandoned the project. It\\'s interesting to read the blog posts in order, and watch how a clever person can get sidetracked by the minutia, reinforced by the glow of passing tests.\\n\\nAs a contrast, Peter Norvig describes an alternative approach[60] which feels very different in character: rather than being driven by tests, he starts with a basic understanding of how these kinds of problems are traditionally solved (using constraint propagation), and then focuses on refining his algorithm. He addresses board representation in a dozen lines of code that flow directly from his discussion of notation.\\n\\nTests can definitely help drive development. But, as with every drive, unless you have a destination in mind, you can end up going in circles.\\n\\n## **Back to the Code**\\n\\nComponent-based development has long been a lofty goal of software development.[61] The idea is that generic software components should be available and combined just as easily as common integrated circuits (ICs) are combined. But this works only if the components you are using are\\n\\nknown to be reliable, and if you have common voltages, interconnect standards, timing, and so on.\\n\\nChips are designed to be tested—not just at the factory, not just when they are installed, but also in the field when they are deployed. More complex chips and systems may have a full Built-In Self Test (BIST) feature that runs some base-level diagnostics internally, or a Test Access Mechanism (TAM) that provides a test harness that allows the external environment to provide stimuli and collect responses from the chip.\\n\\nWe can do the same thing in software. Like our hardware colleagues, we need to build testability into the software from the very beginning, and test each piece thoroughly before trying to wire them together.\\n\\n## **Unit Testing**\\n\\nChip-level testing for hardware is roughly equivalent to *unit testing* in software—testing done on each module, in isolation, to verify its behavior. We can get a better feeling for how a module will react in the big wide world once we have tested it throughly under controlled (even contrived) conditions.\\n\\nA software unit test is code that exercises a module. Typically, the unit test will establish some kind of artificial environment, then invoke routines in the module being tested. It then checks the results that are returned, either against known values or against the results from previous runs of the same test (regression testing).\\n\\nLater, when we assemble our \"software ICs\" into a complete system, we\\'ll have confidence that the individual parts work as expected, and then we can use the same unit test facilities to test the system as a whole. We talk about this large-scale checking of the system in *Ruthless and Continuous Testing*.\\n\\nBefore we get that far, however, we need to decide what to test at the unit level. Historically, programmers threw a few random bits of data at the\\n\\ncode, looked at the print statements, and called it tested. We can do much better.\\n\\n## **Testing Against Contract**\\n\\nWe like to think of unit testing as *testing against contract* (see Topic 23, *Design by Contract*). We want to write test cases that ensure that a given unit honors its contract. This will tell us two things: whether the code meets the contract, and whether the contract means what we think it means. We want to test that the module delivers the functionality it promises, over a wide range of test cases and boundary conditions.\\n\\nWhat does this mean in practice? Let\\'s start with a simple, numerical example: a square root routine. Its documented contract is simple:\\n\\n```\\npre-conditions:\\n argument >= 0;\\npost-conditions:\\n ((result * result) - argument).abs <= epsilon*argument;\\n```\\nThis tells us what to test:\\n\\n- Pass in a negative argument and ensure that it is rejected.\\n- Pass in an argument of zero to ensure that it is accepted (this is the boundary value).\\n- Pass in values between zero and the maximum expressible argument and verify that the difference between the square of the result and the original argument is less than some small fraction of the argument (epsilon).\\n\\nArmed with this contract, and assuming that our routine does its own preand postcondition checking, we can write a basic test script to exercise the square root function.\\n\\nThen we can call this routine to test our square root function:\\n\\n```\\nassertWithinEpsilon(my_sqrt(0), 0)\\nassertWithinEpsilon(my_sqrt(2.0), 1.4142135624)\\nassertWithinEpsilon(my_sqrt(64.0), 8.0)\\nassertWithinEpsilon(my_sqrt(1.0e7), 3162.2776602)\\nassertRaisesException fn => my_sqrt(-4.0) end\\n```\\nThis is a pretty simple test; in the real world, any nontrivial module is likely to be dependent on a number of other modules, so how do we go about testing the combination?\\n\\nSuppose we have a module A that uses a DataFeed and a LinearRegression. In order, we would test:\\n\\n- 1. DataFeed\\'s contract, in full\\n- 2. LinearRegression\\'s contract, in full\\n- 3. A\\'s contract, which relies on the other contracts but does not directly expose them\\n\\nThis style of testing requires you to test subcomponents of a module first. Once the subcomponents have been verified, then the module itself can be tested.\\n\\nIf DataFeed and LinearRegression\\'s tests passed, but A\\'s test failed, we can be pretty sure that the problem is in A, or in A\\'s *use* of one of those subcomponents. This technique is a great way to reduce debugging effort: we can quickly concentrate on the likely source of the problem within module A, and not waste time reexamining its subcomponents.\\n\\nWhy do we go to all this trouble? Above all, we want to avoid creating a \"time bomb\"—something that sits around unnoticed and blows up at an awkward moment later in the project. By emphasizing testing against contract, we can try to avoid as many of those downstream disasters as possible.\\n\\n## **Ad Hoc Testing**\\n\\nNot to be confused with \"odd hack,\" *ad-hoc* testing is when we run poke at our code manually. This may be as simple as a console.log(), or a piece of code entered interactively in a debugger, IDE environment, or REPL.\\n\\nAt the end of the debugging session, you need to formalize this ad hoc test. If the code broke once, it is likely to break again. Don\\'t just throw away the test you created; add it to the existing unit test arsenal.\\n\\n## **Build a Test Window**\\n\\nEven the best sets of tests are unlikely to find all the bugs; there\\'s something about the damp, warm conditions of a production environment that seems to bring them out of the woodwork.\\n\\nThis means you\\'ll often need to test a piece of software once it has been deployed—with real-world data flowing though its veins. Unlike a circuit board or chip, we don\\'t have *test pins* in software, but we *can* provide various views into the internal state of a module, without using the debugger (which may be inconvenient or impossible in a production application).\\n\\nLog files containing trace messages are one such mechanism. Log messages should be in a regular, consistent format; you may want to parse them automatically to deduce processing time or logic paths that the program took. Poorly or inconsistently formatted diagnostics are just so much \"spew\"—they are difficult to read and impractical to parse.\\n\\nAnother mechanism for getting inside running code is the \"hot-key\" sequence or magic URL. When this particular combination of keys is pressed, or the URL is accessed, a diagnostic control window pops up with status messages and so on. This isn\\'t something you normally would reveal to end users, but it can be very handy for the help desk.\\n\\nMore generally, you could use a *feature switch* to enable extra diagnostics for a particular user or class of users.\\n\\n#### **A Confession**\\n\\nI (Dave) have been known to tell people that I no longer write tests. Partly I do it to shake the faith of those who have turned testing into a religion. And partly I say it because it is (somewhat) true.\\n\\nI\\'ve been coding for 45 years, and writing automated tests for more than 30 of them. Thinking about testing is built in to the way I approach coding. It felt comfortable. And my personality insists that when something starts to feel comfortable I should move on to something else.\\n\\nIn this case I decided to stop writing tests for a couple of months and see what it did to my code. To my surprise, the answer was \"not a lot.\" So I spent some time working out why.\\n\\nI *believe* the answer is that (for me) most of the benefit of testing comes from thinking about the tests and their impact on the code. And, after doing it for so long, I could do that thinking without actually writing tests. My code was still testable; it just wasn\\'t tested.\\n\\nBut that ignores the fact that tests are also a way of communicating with other developers, so I now *do* write tests on code shared with others or that relies on the peculiarities of external dependencies.\\n\\nAndy says I shouldn\\'t include this sidebar. He worries it will tempt inexperienced developers not to test. Here\\'s my compromise:\\n\\nShould you write tests? Yes. But after you\\'ve been doing it for 30 years, feel free to experiment a little to see where the benefit lies for you.\\n\\n## **A Culture of Testing**\\n\\nAll software you write *will* be tested—if not by you and your team, then by the eventual users—so you might as well plan on testing it thoroughly. A little forethought can go a long way toward minimizing maintenance costs and help-desk calls.\\n\\nYou really only have a few choices:\\n\\n- Test First\\n- Test During\\n- Test Never\\n\\nTest First, including Test-Driven Design, is probably your best choice in most circumstances, as it ensures that testing happens. But sometimes that\\'s not as convenient or useful, so Test During coding can be a good fallback, where you write some code, fiddle with it, write the tests for it, then move on to the next bit. The worst choice is often called \"Test Later,\" but who are you kidding? \"Test Later\" really means \"Test Never.\"\\n\\nA culture of testing means all the tests pass all the time. Ignore a spew of tests that \"always fail\" makes it easier to ignore *all* the tests, and the vicious spiral begins (see Topic 3, *Software Entropy*).\\n\\nTreat test code with the same care as any production code. Keep it decoupled, clean, and robust. Don\\'t rely on unreliable things (see Topic 38, *Programming by Coincidence*) like the absolute position of widgets in a GUI system, or exact timestamps in a server log, or the exact wording of error messages. Testing for these sorts of things will result in fragile tests.\\n\\n## **Tip 70** Test Your Software, or Your Users Will\\n\\nMake no mistake, testing is part of programming. It\\'s not something left to other departments or staff.\\n\\nTesting, design, coding—it\\'s all programming.\\n\\n## **Related Sections Include**\\n\\n- Topic 27, *Don\\'t Outrun Your Headlights*\\n- Topic 51, *Pragmatic Starter Kit*\\n\\n## **Topic 42** Property-Based Testing\\n\\n*Доверяй, но проверяй (Trust, but verify) Russian proverb* We recommend writing unit tests for your functions. You do that by thinking about typical things that might be a problem, based on your knowledge of the thing you\\'re testing.\\n\\nThere\\'s a small but potentially significant problem lurking in that paragraph, though. If you write the original code and you write the tests, is it possible that an incorrect assumption could be expressed in both? The code passes the tests, because it does what it is supposed to based on your understanding.\\n\\nOne way around this is to have different people write tests and the code under test, but we don\\'t like this: as we said in Topic 41, *Test to Code*, one of the biggest benefits of thinking about tests is the way it informs the code you write. You lose that when the work of testing is split from the coding.\\n\\nInstead, we favor an alternative, where the computer, which doesn\\'t share your preconceptions, does some testing for you.\\n\\n## **Contracts, Invariants, and Properties**\\n\\nIn Topic 23, *Design by Contract*, we talked about the idea that code has *contracts* that it meets: you meet the conditions when you feed it input, and it will make certain guarantees about the outputs it produces.\\n\\nThere are also code *invariants*, things that remain true about some piece of state when it\\'s passed through a function. For example, if you sort a list, the result will have the same number of elements as the original—the length is invariant.\\n\\nOnce we work out our contracts and invariants (which we\\'re going to lump together and call *properties*) we can use them to automate our testing. What we end up doing is called *property-based testing*.\\n\\n| Tip 71 | Use Property-Based Tests to Validate Your |\\n| --- | --- |\\n|  | Assumptions |\\n\\nAs an artificial example, we can build some tests for our sorted list. We\\'ve already established one property: the sorted list is the same size as the original. We can also state that no element in the result can be greater than the one that follows it.\\n\\nWe can now express that in code. Most languages have some kind of property-based testing framework. This example is in Python, and uses the Hypothesis tool and pytest, but the principles are pretty universal.\\n\\nHere is the full source of the tests:\\n\\n```\\nproptest/sort.py\\n   from hypothesis import given\\n   import hypothesis.strategies as some\\n   @given(some.lists(some.integers()))\\n   def test_list_size_is_invariant_across_sorting(a_list):\\n    original_length = len(a_list)\\n    a_list.sort()\\n    assert len(a_list) == original_length\\n   @given(some.lists(some.text()))\\n   def test_sorted_result_is_ordered(a_list):\\n    a_list.sort()\\n    for i in range(len(a_list) - 1):\\n    assert a_list[i] <= a_list[i + 1]\\n```\\nHere\\'s what happens when we run it:\\n\\n```\\n$ pytest sort.py\\n======================= test session starts ========================\\n...\\nplugins: hypothesis-4.14.0\\nsort.py .. [100%]\\n```\\n===================== 2 passed in 0.95 seconds =====================\\n\\nNot much drama there. But, behind the scenes, Hypothesis ran both of our tests one hundred times, passing in a different list each time. The lists will have varying lengths, and will have different contents. It\\'s as if we\\'d cooked up 200 individual tests with 200 random lists.\\n\\n## **Test Data Generation**\\n\\nLike most property-based testing libraries, Hypothesis gives you a minilanguage for describing the data it should generate. The language is based around calls to functions in the hypothesis.strategies module, which we aliased as some, just because it reads better.\\n\\nIf we wrote:\\n\\n```\\n@given(some.integers())\\n```\\nOur test function would run multiple times. Each time, it would be passed a different integer. If instead we wrote the following:\\n\\n```\\n@given(some.integers(min_value=5, max_value=10).map( lambda x: x * 2))\\n```\\nthen we\\'d get the even numbers between 10 and 20.\\n\\nYou can also compose types, so that\\n\\n```\\n@given(some.lists(some.integers(min_value=1), max_size=100))\\n```\\nwill be lists of natural numbers that are at most 100 elements long.\\n\\nThis isn\\'t supposed to be a tutorial on any particular framework, so we\\'ll skip a bunch of cool details and instead look at a real-world example.\\n\\n## **Finding Bad Assumptions**\\n\\nWe\\'re writing a simple order processing and stock control system (because there\\'s always room for one more). It models the stock levels with a Warehouse object. We can query a warehouse to see if something is in stock, remove things from stock, and get the current stock levels.\\n\\nHere\\'s the code:\\n\\n```\\nproptest/stock.py\\n   class Warehouse:\\n    def __init__(self, stock):\\n    self.stock = stock\\n    def in_stock(self, item_name):\\n    return (item_name in self.stock) and (self.stock[item_name] > 0)\\n    def take_from_stock(self, item_name, quantity):\\n    if quantity <= self.stock[item_name]:\\n    self.stock[item_name] -= quantity\\n    else:\\n    raise Exception( \"Oversold {}\".format(item_name))\\n    def stock_count(self, item_name):\\n    return self.stock[item_name]\\n```\\nWe wrote a basic unit test, which passes:\\n\\nproptest/stock.py\\n\\n```\\ndef test_warehouse():\\n wh = Warehouse({ \"shoes\": 10, \"hats\": 2, \"umbrellas\": 0})\\n assert wh.in_stock( \"shoes\")\\n assert wh.in_stock( \"hats\")\\n assert not wh.in_stock( \"umbrellas\")\\n wh.take_from_stock( \"shoes\", 2)\\n```\\n\\n```\\n assert wh.in_stock( \"shoes\")\\n wh.take_from_stock( \"hats\", 2)\\n assert not wh.in_stock( \"hats\")\\n```\\nThen we wrote a function that processes a request to order items from the warehouse. It returns a tuple where the first element is either \"ok\" or \"not available\", followed by the item and requested quantity. We also wrote some tests, and they pass:\\n\\n```\\nproptest/stock.py\\n```\\n\\n```\\ndef order(warehouse, item, quantity):\\n if warehouse.in_stock(item):\\n warehouse.take_from_stock(item, quantity)\\n return ( \"ok\", item, quantity )\\n else:\\n return ( \"not available\", item, quantity )\\n```\\n\\n```\\nproptest/stock.py\\n```\\n\\n```\\ndef test_order_in_stock():\\n wh = Warehouse({ \"shoes\": 10, \"hats\": 2, \"umbrellas\": 0})\\n status, item, quantity = order(wh, \"hats\", 1)\\n assert status == \"ok\"\\n assert item == \"hats\"\\n assert quantity == 1\\n assert wh.stock_count( \"hats\") == 1\\ndef test_order_not_in_stock():\\n wh = Warehouse({ \"shoes\": 10, \"hats\": 2, \"umbrellas\": 0})\\n status, item, quantity = order(wh, \"umbrellas\", 1)\\n assert status == \"not available\"\\n assert item == \"umbrellas\"\\n assert quantity == 1\\n assert wh.stock_count( \"umbrellas\") == 0\\ndef test_order_unknown_item():\\n wh = Warehouse({ \"shoes\": 10, \"hats\": 2, \"umbrellas\": 0})\\n status, item, quantity = order(wh, \"bagel\", 1)\\n assert status == \"not available\"\\n assert item == \"bagel\"\\n```\\n\\n```\\n assert quantity == 1\\n```\\nOn the surface, everything looks fine. But before we ship the code, let\\'s add some property tests.\\n\\nOne thing we know is that stock cannot appear and disappear across our transaction. This means that if we take some items from the warehouse, the number we took plus the number currently in the warehouse should be the same as the number originally in the warehouse. In the following test, we run our test with the item parameter chosen randomly from \"hat\" or \"shoe\" and the quantity chosen from 1 to 4:\\n\\n```\\nproptest/stock.py\\n```\\n\\n```\\n@given(item = some.sampled_from([ \"shoes\", \"hats\"]),\\n quantity = some.integers(min_value=1, max_value=4))\\ndef test_stock_level_plus_quantity_equals_original_stock_level(item,\\nquantity):\\n wh = Warehouse({ \"shoes\": 10, \"hats\": 2, \"umbrellas\": 0})\\n initial_stock_level = wh.stock_count(item)\\n (status, item, quantity) = order(wh, item, quantity)\\n if status == \"ok\":\\n assert wh.stock_count(item) + quantity == initial_stock_level\\n```\\nLet\\'s run it:\\n\\n```\\n$ pytest stock.py\\n. . .\\nstock.py:72:\\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\\nstock.py:76: in test_stock_level_plus_quantity_equals_original_stock_level\\n (status, item, quantity) = order(wh, item, quantity)\\nstock.py:40: in order\\n warehouse.take_from_stock(item, quantity)\\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\\nself = <stock.Warehouse object at 0x10cf97cf8>, item_name = \\'hats\\'\\nquantity = 3\\n def take_from_stock(self, item_name, quantity):\\n```\\n\\n```\\n if quantity <= self.stock[item_name]:\\n self.stock[item_name] -= quantity\\n else:\\n> raise Exception(\"Oversold {}\".format(item_name))\\nE Exception: Oversold hats\\nstock.py:16: Exception\\n---------------------------- Hypothesis ----------------------------\\nFalsifying example:\\n test_stock_level_plus_quantity_equals_original_stock_level(\\n item=\\'hats\\', quantity=3)\\n```\\nIt blew up in warehouse.take_from_stock: we tried to remove three hats from the warehouse, but it only has two in stock.\\n\\nOur property testing found a faulty assumption: our in_stock function only checks that there\\'s at least one of the given item in stock. Instead we need to make sure we have enough to fill the order:\\n\\n```\\nproptest/stock1.py\\n```\\n\\n```\\ndef in_stock(self, item_name, quantity):\\n» return (item_name in self.stock) and (self.stock[item_name] >= quantity)\\n```\\nAnd we change the order function, too:\\n\\n```\\nproptest/stock1.py\\n```\\n\\n```\\ndef order(warehouse, item, quantity):\\n» if warehouse.in_stock(item, quantity):\\n   warehouse.take_from_stock(item, quantity)\\n   return ( \"ok\", item, quantity )\\n   else:\\n   return ( \"not available\", item, quantity )\\n```\\nAnd now our property test passes.\\n\\n## **Property-Based Tests Often Surprise You**\\n\\nIn the previous example, we used a property-based test to check that stock levels were adjusted properly. The test found a bug, but it wasn\\'t to do with stock level adjustment. Instead, it found a bug in our in_stock function.\\n\\nThis is both the power and the frustration of property-based testing. It\\'s powerful because you set up some rules for generating inputs, set up some assertions for validating output, and then just let it rip. You never quite know what will happen. The test may pass. An assertion may fail. Or the code may fail totally because it couldn\\'t handle the inputs it was given.\\n\\nThe frustration is that it can be tricky to pin down what failed.\\n\\nOur suggestion is that when a property-based test fails, find out what parameters it was passing to the test function, and then use those values to create a separate, regular, unit test. That unit test does two things for you. First, it lets you focus in on the problem without all the additional calls being made into your code by the property-based testing framework. Second, that unit test acts as a *regression test*. Because property-based tests generate random values that get passed to your test, there\\'s no guarantee that the same values will be used the next time you run tests. Having a unit test that forces those values to be used ensures that this bug won\\'t slip through.\\n\\n## **Property-Based Tests Also Help Your Design**\\n\\nWhen we talked about unit testing, we said that one of the major benefits was the way it made you think about your code: a unit test is the first client of your API.\\n\\nThe same is true of property-based tests, but in a slightly different way. They make you think about your code in terms of invariants and contracts; you think about what must not change, and what must be true. This extra insight has a magical effect on your code, removing edge cases and highlighting functions that leave data in an inconsistent state.\\n\\nWe believe that property-based testing is complementary to unit testing: they address different concerns, and each brings its own benefits. If you\\'re not currently using them, give them a go.\\n\\n## **Related Sections Include**\\n\\n- Topic 23, *Design by Contract*\\n- Topic 25, *Assertive Programming*\\n- Topic 45, *The Requirements Pit*\\n\\n## **Exercises**\\n\\n### **Exercise 31** (possible answer)\\n\\nLook back at the warehouse example. Are there any other properties that you can test?\\n\\n## **Exercise 32** (possible answer)\\n\\nYour company ships machinery. Each machine comes in a crate, and each crate is rectangular. The crates vary in size. Your job is to write some code to pack as many crates as possible in a single layer that fits in the delivery truck. The output of your code is a list of all the crates. For each crate, the list gives the location in the truck, along with the width and height. What properties of the output could be tested?\\n\\n## **Challenges**\\n\\nThink about the code you\\'re currently working on. What are the properties: the contracts and invariants? Can you use property-based testing framework to verify these automatically?\\n\\n*Good fences make good neighbors.*\\n\\n*Robert Frost, Mending Wall*\\n\\nIn the first edition\\'s discussion of code coupling we made a bold and naive statement: \"we don\\'t need to be as paranoid as spies or dissidents.\" We were wrong. In fact, you *do* need to be\\n\\nthat paranoid, every day.\\n\\nAs we write this, the daily news is filled with stories of devastating data breaches, hijacked systems, and cyberfraud. Hundreds of millions of records stolen at once, billions and billions of dollars in losses and remediation and these numbers are growing rapidly each year. In the vast majority of cases, it\\'s not because the attackers were terribly clever, or even vaguely competent.\\n\\nIt\\'s because the developers were careless.\\n\\n## **The Other 90%**\\n\\nWhen coding, you may go through several cycles of \"it works!\" and \"why isn\\'t that working?\" with the occasional \"there\\'s no way that could have happened…\"[62] After several hills and bumps on this uphill climb, it\\'s easy to say to yourself, \"phew, it all works!\" and proclaim the code done. Of course, it\\'s not done yet. You\\'re 90% done, but now you have the *other* 90% to consider.\\n\\nThe next thing you have to do is analyze the code for ways it can go wrong and add those to your test suite. You\\'ll consider things such as passing in bad parameters, leaking or unavailable resources; that sort of thing.\\n\\nIn the good old days, this evaluation of internal errors may have been sufficient. But today that\\'s only the beginning, because in addition to errors from internal causes, you need to consider how an external actor could deliberately screw up the system. But perhaps you protest, \"Oh, no one will care about this code, it\\'s not important, no one even knows about this server…\" It\\'s a big world out there, and most of it is connected. Whether it\\'s a bored kid on the other side of the planet, state-sponsored terrorism, criminal gangs, corporate espionage, or even a vengeful ex, they are out there and aiming for you. The survival time of an unpatched, outdated system on the open net is measured in minutes—or even less.\\n\\nSecurity through obscurity just doesn\\'t work.\\n\\n## **Security Basic Principles**\\n\\nPragmatic Programmers have a healthy amount of paranoia. We know we have faults and limitations, and that external attackers will seize on *any* opening we leave to compromise our systems. Your particular development and deployment environments will have their own security-centric needs, but there are a handful of basic principles that you should always bear in mind:\\n\\n- 1. Minimize Attack Surface Area\\n- 2. Principle of Least Privilege\\n- 3. Secure Defaults\\n- 4. Encrypt Sensitive Data\\n- 5. Maintain Security Updates\\n\\nLet\\'s take a look at each of these.\\n\\n## **Minimize Attack Surface Area**\\n\\nThe *attack surface area* of a system is the sum of all access points where an attacker can enter data, extract data, or invoke execution of a service. Here are a few examples:\\n\\n#### *Code complexity leads to attack vectors*\\n\\nCode complexity makes the attack surface larger, with more opportunities for unanticipated side effects. Think of complex code as making the surface area more porous and open to infection. Once again, simple, smaller code is better. Less code means fewer bugs, fewer opportunities for a crippling security hole. Simpler, tighter, less complex code is easier to reason about, easier to spot potential weaknesses.\\n\\n#### *Input data is an attack vector*\\n\\nNever trust data from an external entity, always sanitize it before passing it on to a database, view rendering, or other processing.[63] Some languages can help with this. In Ruby, for example, variables holding external input are *tainted*, which limits what operations can be performed on them. For example, this code apparently uses the wc utility to report on the number of characters in a file whose name is supplied at runtime:\\n\\n```\\nsafety/taint.rb\\n```\\n\\n```\\nputs \"Enter a file name to count: \"\\nname = gets\\nsystem( \"wc -c #{name }\")\\n```\\nA nefarious user could do damage like this:\\n\\n```\\nEnter a file name to count:\\ntest.dat; rm -rf /\\n```\\nHowever, setting the SAFE level to 1 will taint external data, which means it can\\'t be used in dangerous contexts:\\n\\n```\\n» $SAFE = 1\\n  puts \"Enter a file name to count: \"\\n  name = gets\\n  system( \"wc -c #{name }\")\\n```\\nNow when we run it, we get caught red-handed:\\n\\n```\\n$ ruby taint.rb\\nEnter a file name to count:\\ntest.dat; rm -rf /\\ncode/safety/taint.rb:5:in `system\\': Insecure operation - system\\n(SecurityError)\\n from code/safety/taint.rb:5:in `main\\n```\\n#### *Unauthenticated services are an attack vector*\\n\\nBy their very nature, any user anywhere in the world can call unauthenticated services, so barring any other handling or limiting you\\'ve immediately created an opportunity for a *denial-of-service* attack at the very least. Quite a few of highly public data breaches recently were caused by developers accidentally putting data in unauthenticated, publicly readable data stores in the cloud.\\n\\n#### *Authenticated services are an attack vector*\\n\\nKeep the number of authorized users at an absolute minimum. Cull unused, old, or outdated users and services. Many net-enabled devices have been found to contain simple default passwords or unused, unprotected administrative accounts. If an account with deployment credentials is compromised, your entire product is compromised.\\n\\n#### *Output data is an attack vector*\\n\\nThere\\'s a (possibly apocryphal) story about a system that dutifully reported the error message Password is used by another user. Don\\'t give away information. Make sure that the data you report is appropriate for the authorization of that user. Truncate or obfuscate potentially risky information such as Social Security or other government ID numbers.\\n\\n#### *Debugging info is an attack vector*\\n\\nThere\\'s nothing as heartwarming as seeing a full stack trace with data on your local ATM machine, an airport kiosk, or crashing web page. Information designed to make debugging easier can make breaking in\\n\\neasier as well. Make sure any \"test window\" (discussed here) and runtime exception reporting is protected from spying eyes.[64]\\n\\n## **Tip 72** Keep It Simple and Minimize Attack Surfaces\\n\\n## **Principle of Least Privilege**\\n\\nAnother key principle is to use the *least* amount of privilege for the *shortest* time you can get away with. In other words, don\\'t automatically grab the highest permission level, such as root or Administrator. If that high level *is* needed, take it, do the minimum amount of work, and relinquish your permission quickly to reduce the risk. This principle dates back to the early 1970s:\\n\\nEvery program and every privileged user of the system should operate using the least amount of privilege necessary to complete the job.— Jerome Saltzer, Communications of the ACM, 1974.\\n\\nTake the login program on Unix-derived systems. It initially executes with root privileges. As soon as it finishes authenticating the correct user, though, it drops the high level privilege to that of the user.\\n\\nThis doesn\\'t just apply to operating system privilege levels. Does your application implement different levels of access? Is it a blunt tool, such as \"administrator\" vs. \"user?\" If so, consider something more finely grained, where your sensitive resources are partitioned into different categories, and individual users have permissions for only certain of those categories.\\n\\nThis technique follows the same sort of idea as minimizing surface area reducing the scope of attack vectors, both by time and by privilege level. In this case, less is indeed more.\\n\\n### **Secure Defaults**\\n\\nThe default settings on your app, or for your users on your site, should be the *most* secure values. These might not be the most user-friendly or convenient values, but it\\'s better to let each individual decide for themselves the trade-offs between security and convenience.\\n\\nFor example, the default for password entry might be to hide the password as entered, replacing each character with an asterisk. If you\\'re entering a password in a crowded public place, or projected before a large audience, that\\'s a sensible default. But some users might want to see the password spelled out, perhaps for accessibility. If there\\'s little risk someone is looking over their shoulder, that\\'s a reasonable choice for them.\\n\\n## **Encrypt Sensitive Data**\\n\\nDon\\'t leave personally identifiable information, financial data, passwords, or other credentials in plain text, whether in a database or some other external file. If the data gets exposed, encryption offers an additional level of safety.\\n\\nIn Topic 19, *Version Control* we strongly recommend putting everything needed for the project under version control. Well, *almost* everything. Here\\'s one major exception to that rule:\\n\\nDon\\'t check in secrets, API keys, SSH keys, encryption passwords or other credentials alongside your source code in version control.\\n\\nKeys and secrets need to be managed separately, generally via config files or environment variables as part of build and deployment.\\n\\n#### **Password Antipatterns**\\n\\nOne of the fundamental problems with security is that oftentimes good security runs counter to common sense or common practice. For example, you might think that strict password requirements would increase security for your application or site. You\\'d be wrong.\\n\\nStrict password policies will actually *lower* your security. Here\\'s a short list of very bad ideas, along with some recommendations from the NIST: [65]\\n\\n- Do not restrict password length to less than 64 characters. NIST recommends 256 as a good maximum length.\\n- Do not truncate the user\\'s chosen password.\\n- Do not restrict special characters such as []();&%$# or /. See the note about Bobby Tables earlier in this section. If special characters in your password will compromise your system, you have bigger problems. The NIST says to accept all printing ASCII characters, space, and Unicode.\\n- Do not provide password hints to unauthenticated users, or prompt for specific types of information (e.g., \"what was the name of your first pet?\").\\n- Do not disable the paste function in the browser. Crippling the functionality of the browser and password managers does not make your system more secure, in fact it drives users to create simpler, shorter passwords that are much easier to compromise. Both the NIST in the US and the National Cyber Security Centre in the UK specifically require verifiers to allow paste functionality for this reason.\\n- Do not impose other composition rules. For example, do not mandate any particular mix of upper and lower case, numerics, or special characters, or prohibit repeating characters, and so on.\\n- Do not arbitrarily require users to change their passwords after some length of time. Only do this for a valid reason (e.g., if there has been a breach).\\n\\nYou want to encourage long, random passwords with a high degree of entropy. Putting artificial constraints limits entropy and encourages bad password habits, leaving your user\\'s accounts vulnerable to takeover.\\n\\n#### **Maintain Security Updates**\\n\\nUpdating computer systems can be a huge pain. You need that security patch, but as a side effect it breaks some portion of your application. You could decide to wait, and defer the update until later. That\\'s a terrible idea, because now your system is vulnerable to a known exploit.\\n\\n## **Tip 73** Apply Security Patches Quickly\\n\\nThis tip affects every net-connected device, including phones, cars, appliances, personal laptops, developer machines, build machines, production servers, and cloud images. Everything. And if you think that this doesn\\'t really matter, just remember that the largest data breaches in history (so far) were caused by systems that were behind on their updates.\\n\\nDon\\'t let it happen to you.\\n\\n## **Common Sense vs. Crypto**\\n\\nIt\\'s important to keep in mind that common sense may fail you when it comes to matters of cryptography. The first and most important rule when it comes to crypto is *never do it yourself.*[66] Even for something as simple as passwords, common practices are wrongheaded (see the sidebar *Password Antipatterns*). Once you get into the world of crypto, even the tiniest, most insignificant-looking error can compromise everything: your clever new, home-made encryption algorithm can probably be broken by an expert in minutes. You don\\'t want to do encryption yourself.\\n\\nAs we\\'ve said elsewhere, rely only on reliable things: well-vetted, thoroughly examined, well-maintained, frequently updated, preferably open source libraries and frameworks.\\n\\nBeyond simple encryption tasks, take a hard look at other security-related features of your site or application. Take authentication, for instance.\\n\\nIn order to implement your own login with password or biometric authentication, you need to understand how hashes and salts work, how crackers use things like Rainbow tables, why you shouldn\\'t use MD5 or\\n\\nSHA1, and a host of other concerns. And even if you get all that right, at the end of the day you\\'re still responsible for holding onto the data and keeping it secure, subject to whatever new legislation and legal obligations come up.\\n\\nOr, you could take the Pragmatic approach and let someone else worry about it and use a third-party authentication provider. This may be an off-the-shelf service you run in-house, or it could be a third party in the cloud. Authentication services are often available from email, phone, or social media providers, which may or may not be appropriate for your application. In any case, these folks spend all their days keeping their systems secure, and they\\'re better at it than you are.\\n\\nStay safe out there.\\n\\n## **Related Sections Include**\\n\\n- Topic 23, *Design by Contract*\\n- Topic 24, *Dead Programs Tell No Lies*\\n- Topic 25, *Assertive Programming*\\n- Topic 38, *Programming by Coincidence*\\n- Topic 45, *The Requirements Pit*\\n\\n## **Topic 44** Naming Things\\n\\n*The beginning of wisdom is to call things by their proper name.*\\n\\n*Confucius*\\n\\nWhat\\'s in a name? When we\\'re programming, the answer is \"everything!\"\\n\\nWe create names for applications, subsystems, modules, functions,\\n\\nvariables—we\\'re constantly creating new things and bestowing names on them. And those names are very, very important, because they reveal a lot about your intent and belief.\\n\\nWe believe that things should be named according to the role they play in your code. This means that, whenever you create something, you need to pause and think \"what is my motivation to create this?\"\\n\\nThis is a powerful question, because it takes you out of the immediate problem-solving mindset and makes you look at the bigger picture. When you consider the role of a variable or function, you\\'re thinking about what is special about it, about what it can do, and what it interacts with. Often, we find ourselves realizing that what we were about to do made no sense, all because we couldn\\'t come up with an appropriate name.\\n\\nThere\\'s some science behind the idea that names are deeply meaningful. It turns out that the brain can read and understand words really fast: faster than many other activities. This means that words have a certain priority when we try to make sense of something. This can be demonstrated using the Stroop effect.[67]\\n\\nLook at the following panel. It has a list of color names or shades, and each is shown in a color or shade. But the names and colors don\\'t necessarily\\n\\nmatch. Here\\'s part one of the challenge—say aloud the name of each color as written:[68]\\n\\n![](_page_353_Picture_1.jpeg)\\n\\nNow repeat this, but instead say aloud the color used to draw the word. Harder, eh? It\\'s easy to be fluent when reading, but way harder when trying to recognize colors.\\n\\nYour brain treats written words as something to be respected. We need to make sure the names we use live up to this.\\n\\nLet\\'s look at a couple of examples:\\n\\n- We\\'re authenticating people who access our site that sells jewelry made from old graphics cards:\\nlet user = authenticate(credentials)\\n\\nThe variable is user because it\\'s *always* user. But why? It means nothing. How about customer, or buyer? That way we get constant reminders as we code of what this person is trying to do, and what that means to us.\\n\\n- We have an instance method that discounts an order:\\n\\n```\\npublic void deductPercent( double amount)\\n // ...\\n```\\nTwo things here. First, deductPercent is *what it does* and not *why it does it*. Then the name of the parameter amount is at best misleading: is it an\\n\\nabsolute amount, a percentage?\\n\\nPerhaps this would be better:\\n\\n```\\npublic void applyDiscount(Percentage discount)\\n // ...\\n```\\nThe method name now makes its intent clear. We\\'ve also changed the parameter from a double to a Percentage, a type we\\'ve defined. We don\\'t know about you, but when dealing with percentages we never know if the value is supposed to be between 0 and 100 or 0.0 and 1.0. Using a type documents what the function expects.\\n\\n- We have a module that does interesting things with Fibonacci numbers. One of those things is to calculate the number in the sequence. Stop and think what you\\'d call this function.\\nMost people we ask would call it fib. Seems reasonable, but remember it will normally be called in the context of its module, so the call would be Fib.fib(n). How about calling it of or nth instead:\\n\\n> Fib.of(0) *# => 0* Fib.nth(20) *# => 4181*\\n\\nWhen naming things, you\\'re constantly looking for ways of clarifying what you mean, and that act of clarification will lead you to a better understanding of your code *as you write it.*\\n\\nHowever, not all names have to be candidates for a literary prize.\\n\\n#### **The Exception That Proves the Rule**\\n\\nWhile we strive for clarity in code, branding is a different matter entirely.\\n\\nThere\\'s a well-established tradition that projects and project teams should have obscure, \"clever\" names. Names of Pokémon, Marvel superheroes, cute mammals, *Lord of the Rings* characters, you name it.\\n\\nLiterally.\\n\\n## **Honor the Culture**\\n\\nMost introductory computer texts will admonish you never to use single letter variables such as i, j, or k. [69]\\n\\nWe think they\\'re wrong. Sort of.\\n\\nIn fact, it depends on the culture of that particular programming language or environment. In the C programming language, i, j, and k are traditionally used as loop increment variables, s is used for a character string, and so on. If you program in that environment, that\\'s what you are used to seeing and it would be jarring (and hence wrong) to violate that norm. On the other hand, using that convention in a different environment where it\\'s *not* expected is just as wrong. You\\'d never do something heinous like this Clojure example which assigns a string to variable i:\\n\\n```\\n( let [i \"Hello World\"]\\n (println i))\\n```\\nSome language communities prefer camelCase, with embedded capital letters, while others prefer snake_case with embedded underscores to separate words. The languages themselves will of course accept either, but that doesn\\'t make it right. Honor the local culture.\\n\\nSome languages allow a subset of Unicode in names. Get a sense of what the community expects before going all cute with names like ɹǝsn or εξέρχεται.\\n\\n## **Consistency**\\n\\nEmerson is famous for writing \"A foolish consistency is the hobgoblin of little minds…,\" but Emerson wasn\\'t on a team of programmers.\\n\\nEvery project has its own vocabulary: jargon words that have a special meaning to the team. \"Order\" means one thing to a team creating an online store, and something very different to a team whose app charts the lineage of religious groups. It\\'s important that everyone on the team knows what these words mean, and that they use them consistently.\\n\\nOne way is to encourage a lot of communication. If everyone pair programs, and pairs switch frequently, then jargon will spread osmotically.\\n\\nAnother way is to have a project glossary, listing the terms that have special meaning to the team. This is an informal document, possibly maintained on a wiki, possibly just index cards on a wall somewhere.\\n\\nAfter a while, the project jargon will take on a life of its own. As everyone gets comfortable with the vocabulary, you\\'ll be able to use the jargon as a shorthand, expressing a lot of meaning accurately and concisely. (This is exactly what a *pattern language* is.)\\n\\n## **Renaming Is Even Harder**\\n\\nNo matter how much effort you put in up front, things change. Code is refactored, usage shifts, meaning becomes subtly altered. If you aren\\'t vigilant about updating names as you go, you can quickly descend into a nightmare much worse than meaningless names: *misleading* names. Have you ever had someone explain inconsistencies in code such as, \"The routine called getData really writes data to an archive file\"?\\n\\nAs we discuss in Topic 3, *Software Entropy*, when you spot a problem, fix it —right here and now. When you see a name that no longer expresses the\\n\\nintent, or is misleading or confusing, fix it. You\\'ve got full regression tests, so you\\'ll spot any instances you may have missed.\\n\\n- \\n- **Tip 74** Name Well; Rename When Needed\\n\\nIf for some reason you can\\'t change the now-wrong name, then you\\'ve got a bigger problem: an ETC violation (see Topic 8, *The Essence of Good Design*). Fix that first, then change the offending name. Make renaming easy, and do it often.\\n\\nOtherwise you\\'ll have to explain to the new folks on the team that getData really writes data to a file, and you\\'ll have to do it with a straight face.\\n\\n## **Related Sections Include**\\n\\n- Topic 3, *Software Entropy*\\n- Topic 40, *Refactoring*\\n- Topic 45, *The Requirements Pit*\\n\\n## **Challenges**\\n\\n- When you find a function or method with an overly generic name, try and rename it to express all the things it really does. Now it\\'s an easier target for refactoring.\\n- In our examples, we suggested using more specific names such as *buyer* instead of the more traditional and generic *user.* What other names do you habitually use that could be better?\\n- Are the names in your system congruent with user terms from the domain? If not, why? Does this cause a Stroop-effect style cognitive dissonance for the team?\\n\\n- Are names in your system hard to change? What can you do to fix that particular broken window?\\n#### **Footnotes**\\n\\n- [50] Note from the battle-scarred: UTC is there for a reason. Use it.\\n- [51] https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation\\n- [52] See Topic 50, *Coconuts Don\\'t Cut It*.\\n- [53] You can also go too far here. We once knew a developer who rewrote all source he was given because he had his own naming conventions.\\n- [54] https://media-origin.pragprog.com/titles/tpp20/code/algorithm_speed/sort/src/main.rs\\n- [55] And yes, we did voice our concerns over the title.\\n- [56] Originally spotted in *UML Distilled: A Brief Guide to the Standard Object Modeling Language* [Fow00].\\n- [57] This is excellent advice in general (see Topic 27, *Don\\'t Outrun Your Headlights*).\\n- [58] Some folks argue that test-first and test-driven development are two different things, saying that the intents of the two are different. However, historically, test-first (which comes from eXtreme Programming) was identical to what people now call TDD.\\n- [59] https://ronjeffries.com/categories/sudoku. A big \"thank you\" to Ron for letting us use this story.\\n- [60] http://norvig.com/sudoku.html\\n- [61] We\\'ve been trying since at least 1986, when Cox and Novobilski coined the term \"software IC\" in their Objective-C book *Object-Oriented Programming Object-Oriented Programming: An Evolutionary Approach* [CN91].\\n- [62] See Topic 20, *Debugging*.\\n- [63] Remember our good friend, little Bobby Tables (https://xkcd.com/327)? While you\\'re reminiscing have a look at https://bobby-tables.com, which lists ways of sanitizing data passed to database queries.\\n- [64] This technique has proven to be successful at the CPU chip level, where well-known exploits target debugging and administrative facilities. Once cracked, the entire machine is left exposed.\\n- [65] *NIST Special Publication 800-63B: Digital Identity Guidelines: Authentication and Lifecycle Management*, available free online at https://doi.org/10.6028/NIST.SP.800-63b\\n- [66] Unless you have a PhD in cryptography, and even then only with major peer review, extensive field trials with a bug bounty, and budget for long-term maintenance.\\n- [67] *Studies of Interference in Serial Verbal Reactions* [Str35]\\n- [68] We have two versions of this panel. One uses different colors, and the other uses shades of gray. If you\\'re seeing this in black and white and want the color version, or if you\\'re having trouble distinguishing colors and want to try the grayscale version, pop over to https://pragprog.com/the-pragmatic-programmer/stroop-effect.\\n- [69] Do you know *why* i is commonly used as a loop variable? The answer comes from over 60 years ago, when variables starting with I through N were integers in the original FORTRAN. And FORTRAN was in turn influenced by algebra.\\n\\nCopyright © 2020 Pearson Education, Inc.\\n\\n# Chapter 8\\n\\n# **Before the Project**\\n\\nAt the very beginning of a project, you and the team need to learn the requirements. Simply being told what to do or listening to users is not enough: read Topic 45, *The Requirements Pit* and learn how to avoid the common traps and pitfalls.\\n\\nConventional wisdom and constraint management are the topics of Topic 46, *Solving Impossible Puzzles*. Whether you are performing requirements, analysis, coding, or testing, difficult problems will crop up. Most of the time, they won\\'t be as difficult as they first appear to be.\\n\\nAnd when that impossible project comes up, we like to turn to our secret weapon: Topic 47, *Working Together*. And by \"working together\" we don\\'t mean sharing a massive requirements document, flinging heavily cc\\'d emails or enduring endless meetings. We mean solving problems together while coding. We\\'ll show you who you need and how to start.\\n\\nEven though the Agile Manifesto begins with \"Individuals and interactions over processes and tools,\" virtually all \"agile\" projects begin with an ironic discussion of which process and which tools they\\'ll use. But no matter how well thought out it is, and regardless of which \"best practices\" it includes, no method can replace *thinking*. You don\\'t need any particular process or tool, what you do need is the Topic 48, *The Essence of Agility*.\\n\\nWith these critical issues sorted out *before* the project gets under way, you can be better positioned to avoid \"analysis paralysis\" and actually begin and complete—your successful project.\\n\\n## **Topic 45** The Requirements Pit\\n\\n*Perfection is achieved, not when there is nothing left to add but when there is nothing left to take away...*\\n\\n> *Antoine de St. Exupery, Wind, Sand, and Stars, 1939*\\n\\nMany books and tutorials refer to *requirements gathering* as an early phase of the project. The word \"gathering\" seems to imply a tribe of happy analysts, foraging for nuggets of wisdom that are lying on the ground all around them while the Pastoral Symphony plays gently in the\\n\\nbackground. \"Gathering\" implies that the requirements are already there you need merely find them, place them in your basket, and be merrily on your way.\\n\\nIt doesn\\'t quite work that way. Requirements rarely lie on the surface. Normally, they\\'re buried deep beneath layers of assumptions, misconceptions, and politics. Even worse, often they don\\'t really exist at all.\\n\\n**Tip 75** No One Knows Exactly What They Want\\n\\n## **The Requirements Myth**\\n\\nIn the early days of software, computers were more valuable (in terms of amortized cost per hour) than the people who worked with them. We saved money by trying to get things correct the first time. Part of that process was trying to specify exactly what we were going to get the machine to do. We\\'d start by getting a specification of the requirements, parlay that into a design document, then into flowcharts and pseudo code, and finally into\\n\\ncode. Before feeding it into a computer, though, we\\'d spend time desk checking it.\\n\\nIt cost a lot of money. And that cost meant that people only tried to automate something when they knew exactly what they wanted. As early machines were fairly limited, the scope of problems they solved was constrained: it was actually *possible* to understand the whole problem before you started.\\n\\nBut that is not the real world. The real world is messy, conflicted, and unknown. In that world, exact specifications of anything are rare, if not downright impossible.\\n\\nThat\\'s where we programmers come in. Our job is to help people understand what they want. In fact, that\\'s probably our most valuable attribute. And it\\'s worth repeating:\\n\\n**Tip 76** Programmers Help People Understand What They Want\\n\\n## **Programming as Therapy**\\n\\nLet\\'s call the people who ask us to write software our clients.\\n\\nThe typical client comes to us with a need. The need may be strategic, but it is just as likely to be a tactical issue: a response to a current problem. The need may be for a change to an existing system or it may ask for something new. The need will sometimes be expressed in business terms, and sometimes in technical ones.\\n\\nThe mistake new developers often make is to take this statement of need and implement a solution for it.\\n\\nIn our experience, this initial statement of need is not an absolute requirement. The client may not realize this, but it is really an invitation to explore.\\n\\nLet\\'s take a simple example.\\n\\nYou work for a publisher of paper and electronic books. You\\'re given a new requirement:\\n\\nShipping should be free on all orders costing $50 or more.\\n\\nStop for a second and imagine yourself in that position. What\\'s the first thing that comes to mind?\\n\\nThe chances are very good that you had questions:\\n\\n- Does the $50 include tax?\\n- Does the $50 include current shipping charges?\\n- Does the $50 have to be for paper books, or can the order also include ebooks?\\n- What kind of shipping is offered? Priority? Ground?\\n- What about international orders?\\n- How often will the $50 limit change in the future?\\n\\nThat\\'s what we do. When given something that seems simple, we annoy people by looking for edge cases and asking about them.\\n\\nThe chances are the client will have already thought of some of these, and just assumed that the implementation would work that way. Asking the question just flushes that information out.\\n\\nBut other questions will likely be things that the client hadn\\'t previously considered. That\\'s where things get interesting, and where a good developer learns to be diplomatic.\\n\\nYou:\\n\\nWe were wondering about the $50 total. Does that include what we\\'d normally charge for shipping?\\n\\nClient:\\n\\nOf course. It\\'s the total they\\'d pay us.\\n\\nYou:\\n\\nThat\\'s nice and simple for our customers to understand: I can see the attraction. But I can see some less scrupulous customers trying to game that system.\\n\\nClient:\\n\\nHow so?\\n\\nYou:\\n\\nWell, let\\'s say they buy a book for $25, and then select overnight shipping, the most expensive option. That\\'ll likely be about $30, making the whole order $55. We\\'d then make the shipping free, and they\\'d get overnight shipping on a $25 book for just $25.\\n\\n(At this point the experienced developer stops. Deliver facts, and let the client make the decisions,)\\n\\nClient:\\n\\nOuch. That certainly wasn\\'t what I intended; we\\'d lose money on those orders. What are the options?\\n\\nAnd this starts an exploration. Your role in this is to interpret what the client says and to feed back to them the implications. This is both an intellectual process and a creative one: you\\'re thinking on your feet and you\\'re contributing to a solution that is likely to be better than one that either you or the client would have produced alone.\\n\\n## **Requirements Are a Process**\\n\\nIn the previous example, the developer took the requirements and fed-back a consequence to the client. This initiated the exploration. During that exploration, you are likely to come up with more feedback as the client plays with different solutions. This is the reality of all requirements gathering:\\n\\n- **Tip 77** Requirements Are Learned in a Feedback Loop\\nYour job is to help the client understand the consequences of their stated requirements. You do that by generating feedback, and letting them use that feedback to refine their thinking.\\n\\nIn the previous example, the feedback was easy to express in words. Sometimes that\\'s not the case. And sometimes you honestly won\\'t know enough about the domain to be as specific as that.\\n\\nIn those cases, Pragmatic Programmers rely on the \"is this what you meant?\" school of feedback. We produce mockups and prototypes, and let the client play with them. Ideally the things we produce are flexible enough that we can change them during our discussions with the client, letting us respond to \"that isn\\'t what I meant\" with \"so more like this?\"\\n\\nSometimes these mockups can be thrown together in an hour or so. They are obviously just hacks to get an idea across.\\n\\nBut the reality is that *all* of the work we do is actually some form of mockup. Even at the end of a project we\\'re still interpreting what our client wants. In fact, by that point we\\'re likely to have more clients: the QA people, operations, marketing, and maybe even test groups of customers.\\n\\nSo the Pragmatic Programmer looks at *all* of the project as a requirements gathering exercise. That\\'s why we prefer short iterations; ones that end with direct client feedback. This keeps us on track, and makes sure that if we *do* go in the wrong direction, the amount of time lost is minimized.\\n\\n## **Walk in Your Client\\'s Shoes**\\n\\nThere\\'s a simple technique for getting inside your clients\\' heads that isn\\'t used often enough: become a client. Are you writing a system for the help desk? Spend a couple of days monitoring the phones with an experienced support person. Are you automating a manual stock control system? Work in the warehouse for a week.[70]\\n\\nAs well as giving you insight into how the system will *really* be used, you\\'d be amazed at how the request \"May I sit in for a week while you do your job?\\'\\' helps build trust and establishes a basis for communication with your clients. Just remember not to get in the way!\\n\\n## **Tip 78** Work with a User to Think Like a User\\n\\nGathering feedback is also the time to start to build a rapport with your client base, learning their expectations and hopes for the system you are building. See Topic 52, *Delight Your Users*, for more.\\n\\n## **Requirements vs. Policy**\\n\\nLet\\'s imagine that while discussing a Human Resources system, a client says \"Only an employee\\'s supervisors and the personnel department may view that employee\\'s records.\" Is this statement truly a requirement? Perhaps today, but it embeds business policy in an absolute statement.\\n\\nBusiness policy? Requirement? It\\'s a relatively subtle distinction, but it\\'s one that will have profound implications for the developers. If the requirement is stated as \"Only supervisors and personnel can view an employee record,\" the developer may end up coding an explicit test every time the application accesses this data. However, if the statement is \"Only\\n\\nauthorized users may access an employee record,\" the developer will probably design and implement some kind of access control system. When policy changes (and it will), only the metadata for that system will need to be updated. In fact, gathering requirements in this way naturally leads you to a system that is well factored to support metadata.\\n\\nIn fact, there\\'s a general rule here:\\n\\n![](_page_368_Picture_2.jpeg)\\n\\nImplement the general case, with the policy information as an example of the type of thing the system needs to support.\\n\\n## **Requirements vs. Reality**\\n\\nIn a January 1999 *Wired* magazine article,[71] producer and musician Brian Eno described an incredible piece of technology—the ultimate mixing board. It does anything to sound that can be done. And yet, instead of letting musicians make better music, or produce a recording faster or less expensively, it gets in the way; it disrupts the creative process.\\n\\nTo see why, you have to look at how recording engineers work. They balance sounds intuitively. Over the years, they develop an innate feedback loop between their ears and their fingertips—sliding faders, rotating knobs, and so on. However, the interface to the new mixer didn\\'t leverage off those abilities. Instead, it forced its users to type on a keyboard or click a mouse. The functions it provided were comprehensive, but they were packaged in unfamiliar and exotic ways. The functions the engineers needed were sometimes hidden behind obscure names, or were achieved with nonintuitive combinations of basic facilities.\\n\\nThis example also illustrates our belief that successful tools adapt to the hands that use them. Successful requirements gathering takes this into\\n\\naccount. And this is why early feedback, with prototypes or tracer bullets, will let your clients say \"yes, it does *what* I want, but not *how* I want.\"\\n\\n## **Documenting Requirements**\\n\\nWe believe that the best requirements documentation, perhaps the *only* requirements documentation, is working code.\\n\\nBut that doesn\\'t mean that you can get away without documenting your understanding of what the client wants. It just means that those documents are not a deliverable: they are not something that you give to a client to sign off on. Instead, they are simply mileposts to help guide the implementation process.\\n\\n#### **Requirements Documents Are Not for Clients**\\n\\nIn the past, both Andy and Dave have been on projects that produced incredibly detailed requirements. These substantial documents expanded on the client\\'s initial two-minute explanation of what was wanted, producing inch-thick masterpieces full of diagrams and tables. Things were specified to the point where there was almost no room for ambiguity in the implementation. Given sufficiently powerful tools, the document could actually *be* the final program.\\n\\nCreating these documents was a mistake for two reasons. First, as we\\'ve discussed, the client doesn\\'t really know what they want up front. So when we take what they say and expand it into what is almost a legal document, we are building an incredibly complex castle on quicksand.\\n\\nYou might say \"but then we take the document to the client and they sign off on it. We\\'re getting feedback.\" And that leads us to the second problem with these requirement specifications: the client never reads them.\\n\\nThe client uses programmers because, while the client is motivated by solving a high-level and somewhat nebulous problem, programmers are interested in all the details and nuances. The requirements document is written for developers, and contains information and subtleties that are sometimes incomprehensible and frequently boring to the client.\\n\\nSubmit a 200-page requirements document, and the client will likely heft it to decide if it weighs enough to be important, they may read the first couple of paragraphs (which is why the first two paragraphs are always titled *Management Summary*), and they may flick through the rest, sometimes stopping when there\\'s a neat diagram.\\n\\nThis isn\\'t putting the client down. But giving them a large technical document is like giving the average developer a copy of the *Iliad* in Homeric Greek and asking them to code the video game from it.\\n\\n#### **Requirements Documents Are for Planning**\\n\\nSo we don\\'t believe in the monolithic, heavy-enough-to-stun-an-ox, requirements document. We do, however, know that requirements have to be written down, simply because developers on a team need to know what they\\'ll be doing.\\n\\nWhat form does this take? We favor something that can fit on a real (or virtual) index card. These short descriptions are often called *user stories*. They describe what a small portion of the application should do from the perspective of a user of that functionality.\\n\\nWhen written this way, the requirements can be placed on a board and moved around to show both status and priority.\\n\\nYou might think that a single index card can\\'t hold the information needed to implement a component of the application. You\\'d be right. And that\\'s part of the point. By keeping this statement of requirements short, you\\'re encouraging developers to ask clarifying questions. You\\'re enhancing the feedback process between clients and coders before and during the creation of each piece of code.\\n\\n## **Overspecification**\\n\\nAnother big danger in producing a requirements document is being too specific. Good requirements are abstract. Where requirements are concerned, the simplest statement that accurately reflects the business need is best. This doesn\\'t mean you can be vague—you must capture the underlying semantic invariants as requirements, and document the specific or current work practices as policy.\\n\\nRequirements are not architecture. Requirements are not design, nor are they the user interface. Requirements are *need*.\\n\\n## **Just One More Wafer-Thin Mint…**\\n\\nMany project failures are blamed on an increase in scope—also known as feature bloat, creeping featurism, or requirements creep. This is an aspect of the boiled-frog syndrome from Topic 4, *Stone Soup and Boiled Frogs*. What can we do to prevent requirements from creeping up on us?\\n\\nThe answer (again) is feedback. If you\\'re working with the client in iterations with constant feedback, then the client will experience first-hand the impact of \"just one more feature.\" They\\'ll see another story card go up on the board, and they\\'ll get to help choose another card to move into the next iteration to make room. Feedback works both ways.\\n\\n## **Maintain a Glossary**\\n\\nAs soon as you start discussing requirements, users and domain experts will use certain terms that have specific meaning to them. They may differentiate between a \"client\" and a \"customer,\" for example. It would then be inappropriate to use either word casually in the system.\\n\\nCreate and maintain a *project glossary*—one place that defines all the specific terms and vocabulary used in a project. All participants in the project, from end users to support staff, should use the glossary to ensure consistency. This implies that the glossary needs to be widely accessible—a good argument for online documentation.\\n\\n|\\n|  |\\n\\nIt\\'s hard to succeed on a project if users and developers call the same thing by different names or, even worse, refer to different things by the same name.\\n\\n## **Related Sections Include**\\n\\n- Topic 5, *Good-Enough Software*\\n- Topic 7, *Communicate!*\\n- Topic 11, *Reversibility*\\n- Topic 13, *Prototypes and Post-it Notes*\\n- Topic 23, *Design by Contract*\\n- Topic 43, *Stay Safe Out There*\\n- Topic 44, *Naming Things*\\n- Topic 46, *Solving Impossible Puzzles*\\n- Topic 52, *Delight Your Users*\\n\\n## **Exercises**\\n\\n## **Exercise 33** (possible answer)\\n\\nWhich of the following are probably genuine requirements? Restate those that are not to make them more useful (if possible).\\n\\n- 1. The response time must be less than ~500ms.\\n- 2. Modal windows will have a gray background.\\n- 3. The application will be organized as a number of front-end processes and a back-end server.\\n- 4. If a user enters non-numeric characters in a numeric field, the system will flash the field background and not accept them.\\n- 5. The code and data for this embedded application must fit within 32Mb.\\n\\n## **Challenges**\\n\\n- Can you use the software you are writing? Is it possible to have a good feel for requirements *without* being able to use the software yourself?\\n- Pick a non-computer-related problem you currently need to solve. Generate requirements for a noncomputer solution.\\n\\n## **Topic 46** Solving Impossible Puzzles\\n\\n*Gordius, the King of Phrygia, once tied a knot that no one could untie. It was said that whoever solved the riddle of the Gordian Knot would rule all of Asia. So along comes Alexander the Great, who chops the knot to bits with his sword. Just a little different interpretation of the requirements, that\\'s all…. And he did end up ruling most of Asia.*\\n\\nEvery now and again, you will find yourself embroiled in the middle of a project when a really tough puzzle comes up: some piece of engineering that you just can\\'t get a handle on, or perhaps some bit of code that is turning out to be much harder to write than you thought. Maybe it looks impossible. But is it really as hard as it seems?\\n\\nConsider real-world puzzles—those\\n\\ndevious little bits of wood, wrought iron, or plastic that seem to turn up as Christmas presents or at garage sales. All you have to do is remove the ring, or fit the T-shaped pieces in the box, or whatever.\\n\\nSo you pull on the ring, or try to put the Ts in the box, and quickly discover that the obvious solutions just don\\'t work. The puzzle can\\'t be solved that way. But even though it\\'s obvious, that doesn\\'t stop people from trying the same thing—over and over—thinking there must be a way.\\n\\nOf course, there isn\\'t. The solution lies elsewhere. The secret to solving the puzzle is to identify the real (not imagined) constraints, and find a solution therein. Some constraints are *absolute*; others are merely *preconceived notions*. Absolute constraints *must* be honored, however distasteful or stupid they may appear to be.\\n\\nOn the other hand, as Alexander proved, some apparent constraints may not be real constraints at all. Many software problems can be just as sneaky.\\n\\n### **Degrees of Freedom**\\n\\nThe popular buzz-phrase \"thinking outside the box\" encourages us to recognize constraints that might not be applicable and to ignore them. But this phrase isn\\'t entirely accurate. If the \"box\" is the boundary of constraints and conditions, then the trick is to *find* the box, which may be considerably larger than you think.\\n\\nThe key to solving puzzles is both to recognize the constraints placed on you and to recognize the degrees of freedom you *do* have, for in those you\\'ll find your solution. This is why some puzzles are so effective; you may dismiss potential solutions too readily.\\n\\nFor example, can you connect all of the dots in the following puzzle and return to the starting point with just three straight lines—without lifting your pen from the paper or retracing your steps (*Math Puzzles & Games* [Hol92])?\\n\\n![](_page_375_Picture_3.jpeg)\\n\\nYou must challenge any preconceived notions and evaluate whether or not they are real, hard-and-fast constraints.\\n\\nIt\\'s not whether you think inside the box or outside the box. The problem lies in *finding* the box—identifying the real constraints.\\n\\n![](_page_375_Picture_6.jpeg)\\n\\nWhen faced with an intractable problem, enumerate *all* the possible avenues you have before you. Don\\'t dismiss anything, no matter how unusable or stupid it sounds. Now go through the list and explain why a certain path cannot be taken. Are you sure? Can you *prove* it?\\n\\nConsider the Trojan horse—a novel solution to an intractable problem. How do you get troops into a walled city without being discovered? You can bet that \"through the front door\" was initially dismissed as suicide.\\n\\nCategorize and prioritize your constraints. When woodworkers begin a project, they cut the longest pieces first, then cut the smaller pieces out of the remaining wood. In the same manner, we want to identify the most restrictive constraints first, and fit the remaining constraints within them.\\n\\nBy the way, a solution to the Four Posts puzzle is shown at the end of the book.\\n\\n## **Get Out of Your Own Way!**\\n\\nSometimes you will find yourself working on a problem that seems much harder than you thought it should be. Maybe it feels like you\\'re going down the wrong path—that there must be an easier way than this! Perhaps you are running late on the schedule now, or even despair of ever getting the system to work because this particular problem is \"impossible.\"\\n\\nThis is an ideal time to do something else for a while. Work on something different. Go walk the dog. Sleep on it.\\n\\nYour conscious brain is aware of the problem, but your conscious brain is really pretty dumb (no offense). So it\\'s time to give your real brain, that amazing associative neural net that lurks below your consciousness, some space. You\\'ll be amazed how often the answer will just pop into your head when you deliberately distract yourself.\\n\\nIf that sounds too mystical for you, it isn\\'t. *Psychology Today*[72] reports:\\n\\nTo put it plainly—people who were distracted did better on a complex problem-solving task than people who put in conscious effort.\\n\\nIf you\\'re still not willing to drop the problem for a while, the next best thing is probably finding someone to explain it to. Often, the distraction of simply talking about it will lead you to enlightenment.\\n\\nHave them ask you questions such as:\\n\\n- Why are you solving this problem?\\n- What\\'s the benefit of solving it?\\n- Are the problems you\\'re having related to edge cases? Can you eliminate them?\\n- Is there a simpler, related problem you can solve?\\n\\nThis is another example of Rubber Ducking in practice.\\n\\n## **Fortune Favors the Prepared Mind**\\n\\nLouis Pasteur is reported to have said:\\n\\nDans les champs de l\\'observation le hasard ne favorise que les esprits préparés. *(When it comes to observation, fortune favors the prepared mind.)*\\n\\nThat is true for problem solving, too. In order to have those *eureka!* moments, your nonconscious brain needs to have plenty of raw material; prior experiences that can contribute to an answer.\\n\\nA great way to feed your brain is to give it feedback on what works and what doesn\\'t work as you do your daily job. And we describe a great way to do that using an Engineering Daybook (Topic 22, *Engineering Daybooks*).\\n\\nAnd always remember the advice on the cover of *The Hitchhiker\\'s Guide to the Galaxy*: DON\\'T PANIC.\\n\\n## **Related Sections Include**\\n\\n- Topic 5, *Good-Enough Software*\\n- Topic 37, *Listen to Your Lizard Brain*\\n- Topic 45, *The Requirements Pit*\\n- Andy wrote an entire book about this kind of thing: *Pragmatic Thinking and Learning: Refactor Your Wetware* [Hun08].\\n\\n## **Challenges**\\n\\n- Take a hard look at whatever difficult problem you are embroiled in today. Can you cut the Gordian knot? Do you have to do it this way? Do you have to do it at all?\\n- Were you handed a set of constraints when you signed on to your current project? Are they all still applicable, and is the interpretation of them still valid?\\n\\n## **Topic 47** Working Together\\n\\n*I\\'ve never met a human being who would want to read 17,000 pages of documentation, and if there was, I\\'d kill him to get him out of the gene pool.*\\n\\n*Joseph Costello, President of Cadence*\\n\\nIt was one of those \"impossible\" projects, the kind you hear about that sounds both exhilarating and terrifying at the same time. An ancient system was approaching end-of-life, the hardware was physically going away, and a brand-new system had to be\\n\\ncrafted that would match the (often undocumented) behavior *exactly.* Many hundreds of millions of dollars of other people\\'s money would pass through this system, and the deadline from inception to deployment was on the order of months.\\n\\nAnd that is where Andy and Dave first met. An impossible project with a ridiculous deadline. There was only one thing that made the project a roaring success. The expert who had managed this system for years was sitting right there in her office, just across the hall from our broom closet– sized development room. Continuously available for questions, clarifications, decisions, and demos.\\n\\nThroughout this book we recommend working closely with users; they are part of your team. On that first project together, we practiced what now might be called *pair programming* or *mob programming*: one person typing code while one or more other team members comment, ponder, and solve problems together. It\\'s a powerful way of working together that transcends endless meetings, memos, and overstuffed legalistic documentation prized for weight over usefulness.\\n\\nAnd that\\'s what we really mean by \"working with\": not just asking questions, having discussions, and taking notes, but asking questions and\\n\\n#### having discussions *while you\\'re actually coding*.\\n\\n**Conway\\'s Law**\\n\\nIn 1967, Melvin Conway introduced an idea in *How do Committees Invent?* [Con68] which would become known as Conway\\'s Law:\\n\\nOrganizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations.\\n\\nThat is, the social structures and communication pathways of the team and the organization will be mirrored in the application, website, or product being developed. Various studies have shown strong support for this idea. We\\'ve witnessed it first-hand countless times—for example, in teams where no one talks to each other at all, resulting in siloed, \"stove-pipe\" systems. Or teams that were split into two, resulting in a client/server or frontend/backend division.\\n\\nStudies also offer support for the reverse principle: you can deliberately structure your team the way you want your code to look. For example, geographically distributed teams are shown to tend toward more modular, distributed software.\\n\\nBut most importantly, development teams that include users will produce software that clearly reflects that involvement, and teams that don\\'t bother will reflect that, too.\\n\\n## **Pair Programming**\\n\\n*Pair programming* is one of the practices of eXtreme Programming that has become popular outside of XP itself. In pair programming, one developer operates the keyboard, and the other does not. Both work on the problem together, and can switch typing duties as needed.\\n\\nThere are many benefits to pair programming. Different people bring different backgrounds and experience, different problem-solving techniques and approaches, and differing levels of focus and attention to any given problem. The developer acting as typist must focus on the low-level details of syntax and coding style, while the other developer is free to consider higher-level issues and scope. While that might sound like a small distinction, remember that we humans have only so much brain bandwidth. Fiddling around with typing esoteric words and symbols that the compiler\\n\\nwill grudgingly accept takes a fair bit of our own processing power. Having a second developer\\'s full brain available during the task brings a lot more mental power to bear.\\n\\nThe inherent peer-pressure of a second person helps against moments of weakness and bad habits of naming variables foo and such. You\\'re less inclined to take a potentially embarrassing shortcut when someone is actively watching, which also results in higher-quality software.\\n\\n## **Mob Programming**\\n\\nAnd if two heads are better than one, what about having a dozen diverse people all working on the same problem at the same time, with one typist?\\n\\n*Mob programming*, despite the name, does not involve torches or pitchforks. It\\'s an extension of pair programming that involves more than just two developers. Proponents report great results using mobs to solve hard problems. Mobs can easily include people not usually considered part of the development team, including users, project sponsors, and testers. In fact, in our first \"impossible\" project together, it was a common sight for one of us to be typing while the other discussed the issue with our business expert. It was a small mob of three.\\n\\nYou might think of mob programming as *tight collaboration with live coding.*\\n\\n## **What Should I Do?**\\n\\nIf you\\'re currently only programming solo, maybe try pair programming. Give it a minimum of two weeks, only a few hours at a time, as it will feel strange at first. To brainstorm new ideas or diagnose thorny issues, perhaps try a mob programming session.\\n\\nIf you are already pairing or mobbing, who\\'s included? Is it just developers, or do you allow members of your extended team to participate: users,\\n\\ntesters, sponsors…?\\n\\nAnd as with all collaboration, you need to manage the human aspects of it as well as the technical. Here are just a few tips to get started:\\n\\n- Build the code, not your ego. It\\'s not about who\\'s brightest; we all have our moments, good and bad.\\n- Start small. Mob with only 4-5 people, or start with just a few pairs, in short sessions.\\n- Criticize the code, not the person. \"Let\\'s look at this block\" sounds much better than \"you\\'re wrong.\"\\n- Listen and try to understand others\\' viewpoints. Different isn\\'t wrong.\\n- Conduct frequent retrospectives to try and improve for next time.\\n\\nCoding in the same office or remote, alone, in pairs, or in mobs, are all effective ways of working together to solve problems. If you and your team have only ever done it one way, you might want to experiment with a different style. But don\\'t just jump in with a naive approach: there are rules, suggestions, and guidelines for each of these development styles. For instance, with mob programming you swap out the typist every 5-10 minutes.\\n\\nDo some reading and research, from both textbook and experience reports, and get a feel for the advantages and pitfalls you may encounter. You might want to start by coding a simple exercise, and not just jump straight into your toughest production code.\\n\\nBut however you go about it, let us suggest one final piece of advice:\\n\\n**Tip 82** Don\\'t Go into the Code Alone\\n\\n## **Topic 48** The Essence of Agility\\n\\n*You keep using that word, I do not think it means what you think it means.*\\n\\n*Inigo Montoya, The Princess Bride*\\n\\n*Agile* is an adjective: it\\'s how you do something. You can be an agile developer. You can be on a team that adopts agile practices, a team that responds to change and setbacks with agility. Agility is your style, not you.\\n\\n## **Tip 83** Agile Is Not a Noun; Agile Is How You Do Things\\n\\nAs we write this, almost 20 years after the inception of the Manifesto for Agile Software Development,[73] we see many, many developers successfully applying its values. We see many fantastic teams who find ways to take these values and use them to guide what they do, and how they change what they do.\\n\\nBut we also see another side of agility. We see teams and companies eager for off-the-shelf solutions: Agile-in-a-Box. And we see many consultants and companies all too happy to sell them what they want. We see companies adopting more layers of management, more formal reporting, more specialized developers, and more fancy job titles which just mean \"someone with a clipboard and a stopwatch.\"[74]\\n\\nWe feel that many people have lost sight of the true meaning of agility, and we\\'d like to see folks return to the basics.\\n\\nRemember the values from the manifesto:\\n\\nWe are uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value:\\n\\n- **Individuals and interactions** over processes and tools\\n- **Working software** over comprehensive documentation\\n- **Customer collaboration** over contract negotiation\\n- **Responding to change** over following a plan\\n\\nThat is, while there is value in the items on the right, we value the items on the left more.\\n\\nAnyone selling you something that increases the importance on things on the right over things on the left clearly doesn\\'t value the same things that we and the other manifesto writers did.\\n\\nAnd anyone selling you a solution-in-a-box hasn\\'t read the introductory statement. The values are motivated and informed by the continuous act of uncovering better ways to produce software. This is not a static document. It\\'s suggestions for a generative process.\\n\\n## **There Can Never Be an Agile Process**\\n\\nIn fact, whenever someone says \"do this, and you\\'ll be agile,\" they are wrong. By definition.\\n\\nBecause agility, both in the physical world and in software development, is all about responding to change, responding to the unknowns you encounter after you set out. A running gazelle doesn\\'t go in a straight line. A gymnast makes hundreds of corrections a second as they respond to changes in their environment and minor errors in their foot placement.\\n\\nSo it is with teams and individual developers. There is no single plan you can follow when you develop software. Three of the four values tell you that. They\\'re all about gathering and responding to feedback.\\n\\nThe values don\\'t tell you what to do. They tell you what to look for when you decide for yourself what to do.\\n\\nThese decisions are always contextual: they depend on who you are, the nature of your team, your application, your tooling, your company, your customer, the outside world; an incredibly large number of factors, some major and some trivial. No fixed, static plan can survive this uncertainty.\\n\\n## **So What Do We Do?**\\n\\nNo one can tell you *what* to do. But we think we can tell you something about the spirit with which you do it. It all boils down to how you deal with uncertainty. The manifesto suggests that you do this by gathering and acting on feedback. So here\\'s our recipe for working in an agile way:\\n\\n- 1. Work out where you are.\\n- 2. Make the smallest meaningful step towards where you want to be.\\n- 3. Evaluate where you end up, and fix anything you broke.\\n\\nRepeat these steps until you\\'re done. And use them recursively, at every level of everything you do.\\n\\nSometimes even the most trivial-seeming decision becomes important when you gather feedback.\\n\\n\"Now my code needs to get the account owner.\\n\\n**let** user = accountOwner(accountID);\\n\\nHmmm… user is a useless name. I\\'ll make it owner.\\n\\n```\\nlet owner = accountOwner(accountID);\\n```\\nBut now that feels a little redundant. What am I actually trying to do here? The story says that I\\'m sending this person an email, so I need to find their email address. Maybe I don\\'t need the whole account owner at all.\\n\\n```\\nlet email = emailOfAccountOwner(accountID);\\n```\\nBy applying the feedback loop at a really low level (the naming of a variable) we\\'ve actually improved the design of the overall system, reducing the coupling between this code and the code that deals with accounts.\\n\\nThe feedback loop also applies at the highest level of a project. Some of our most successful work has happened when we started working on a client\\'s requirements, took a single step, and realized that what we were about to do wasn\\'t necessary, that the best solution didn\\'t even involve software.\\n\\nThis loop applies outside the scope of a single project. Teams should apply it to review their process and how well it worked. A team that doesn\\'t continuously experiment with their process is not an agile team.\\n\\n## **And This Drives Design**\\n\\nIn Topic 8, *The Essence of Good Design* we assert that the measure of design is how easy the result of that design is to change: a good design produces something that\\'s easier to change than a bad design.\\n\\nAnd this discussion about agility explains *why* that\\'s the case.\\n\\nYou make a change, and discover you don\\'t like it. Step 3 in our list says we have to be able to fix what we break. To make our feedback loop efficient, this fix has to be as painless as possible. If it isn\\'t, we\\'ll be tempted to shrug it off and leave it unfixed. We talk about this effect in Topic 3, *Software Entropy*. To make this whole agile thing work, we need to practice good design, because good design makes things easy to change. And if it\\'s easy to change, we can adjust, at every level, without any hesitation.\\n\\nThat is agility.\\n\\n## **Related Sections Include**\\n\\n- Topic 27, *Don\\'t Outrun Your Headlights*\\n- Topic 40, *Refactoring*\\n- Topic 50, *Coconuts Don\\'t Cut It*\\n\\n## **Challenges**\\n\\nThe simple feedback loop isn\\'t just for software. Think of other decisions you\\'ve made recently. Could any of them have been improved by thinking about how you might be able to undo them if things didn\\'t take you in the direction you were going? Can you think of ways you can improve what you do by gathering and acting on feedback?\\n\\n#### **Footnotes**\\n\\n- [70] Does a week sound like a long time? It really isn\\'t, particularly when you\\'re looking at processes in which management and workers occupy different worlds. Management will give you one view of how things operate, but when you get down on the floor, you\\'ll find a very different reality—one that will take time to assimilate.\\n- [71] https://www.wired.com/1999/01/eno/\\n- [72] https://www.psychologytoday.com/us/blog/your-brain-work/201209/stop-trying-solve-problems\\n- [73] https://agilemanifesto.org\\n- [74] For more on just how bad that approach can be, see *The Tyranny of Metrics* [Mul18].\\n\\nCopyright © 2020 Pearson Education, Inc.\\n\\n# Chapter 9\\n\\n# **Pragmatic Projects**\\n\\nAs your project gets under way, we need to move away from issues of individual philosophy and coding to talk about larger, project-sized issues. We aren\\'t going to go into specifics of project management, but we will talk about a handful of critical areas that can make or break any project.\\n\\nAs soon as you have more than one person working on a project, you need to establish some ground rules and delegate parts of the project accordingly. In Topic 49, *Pragmatic Teams*, we\\'ll show how to do this while honoring the Pragmatic philosophy.\\n\\nThe purpose of a software development method is to help people work together. Are you and your team doing what works well for you, or are you only investing in the trivial surface artifacts, and not getting the real benefits you deserve? We\\'ll see why Topic 50, *Coconuts Don\\'t Cut It* and offer the true secret to success.\\n\\nAnd of course none of that matters if you can\\'t deliver software consistently and reliably. That\\'s the basis of the magic trio of version control, testing, and automation: the Topic 51, *Pragmatic Starter Kit*.\\n\\nUltimately, though, success is in the eye of the beholder—the sponsor of the project. The perception of success is what counts, and in Topic 52, *Delight Your Users* we\\'ll show you how to delight every project\\'s sponsor.\\n\\nThe last tip in the book is a direct consequence of all the rest. In Topic 53, *Pride and Prejudice*, we ask you to sign your work, and to take pride in what you do.\\n\\n### **Topic 49** Pragmatic Teams\\n\\n*At Group L, Stoffel oversees six firstrate programmers, a managerial challenge roughly comparable to herding cats.*\\n\\n> *The Washington Post Magazine, June 9, 1985*\\n\\nEven in 1985, the joke about herding cats was getting old. By the time of the first edition at the turn of the century, it was positively ancient. Yet it persists, because it has a ring of truth to it. Programmers are a bit like cats: intelligent, strong willed, opinionated, independent, and often worshiped by\\n\\nthe net.\\n\\nSo far in this book we\\'ve looked at pragmatic techniques that help an individual be a better programmer. Can these methods work for teams as well, even for teams of strong-willed, independent people? The answer is a resounding \"yes!\\'\\' There are advantages to being a pragmatic individual, but these advantages are multiplied manyfold if the individual is working on a pragmatic team.\\n\\nA team, in our view, is a small, mostly stable entity of its own. Fifty people aren\\'t a team, they\\'re a horde.[75] Teams where members are constantly being pulled onto other assignments and no one knows each other aren\\'t a team either, they are merely strangers temporarily sharing a bus stop in the rain.\\n\\nA pragmatic team is small, under 10-12 or so members. Members come and go rarely. Everyone knows everyone well, trusts each other, and depends on each other.\\n\\n**Tip 84** Maintain Small, Stable Teams\\n\\nIn this section we\\'ll look briefly at how pragmatic techniques can be applied to teams as a whole. These notes are only a start. Once you\\'ve got a group of pragmatic developers working in an enabling environment, they\\'ll quickly develop and refine their own team dynamics that work for them.\\n\\nLet\\'s recast some of the previous sections in terms of teams.\\n\\n## **No Broken Windows**\\n\\nQuality is a team issue. The most diligent developer placed on a team that just doesn\\'t care will find it difficult to maintain the enthusiasm needed to fix niggling problems. The problem is further exacerbated if the team actively discourages the developer from spending time on these fixes.\\n\\nTeams as a whole should not tolerate broken windows—those small imperfections that no one fixes. The team *must* take responsibility for the quality of the product, supporting developers who understand the *no broken windows* philosophy we describe in Topic 3, *Software Entropy*, and encouraging those who haven\\'t yet discovered it.\\n\\nSome team methodologies have a \"quality officer\"—someone to whom the team delegates the responsibility for the quality of the deliverable. This is clearly ridiculous: quality can come only from the individual contributions of *all* team members. Quality is built in, not bolted on.\\n\\n## **Boiled Frogs**\\n\\nRemember the apocryphal frog in the pan of water, back in Topic 4, *Stone Soup and Boiled Frogs*? It doesn\\'t notice the gradual change in its environment, and ends up cooked. The same can happen to individuals who aren\\'t vigilant. It can be difficult to keep an eye on your overall environment in the heat of project development.\\n\\nIt\\'s even easier for teams as a whole to get boiled. People assume that someone else is handling an issue, or that the team leader must have OK\\'d a change that your user is requesting. Even the best-intentioned teams can be oblivious to significant changes in their projects.\\n\\nFight this. Encourage everyone to actively monitor the environment for changes. Stay awake and aware for increased scope, decreased time scales, additional features, new environments—anything that wasn\\'t in the original understanding. Keep metrics on new requirements.[76] The team needn\\'t reject changes out of hand—you simply need to be aware that they\\'re happening. Otherwise, it\\'ll be *you* in the hot water.\\n\\n## **Schedule Your Knowledge Portfolio**\\n\\nIn Topic 6, *Your Knowledge Portfolio* we looked at ways you should invest in your personal Knowledge Portfolio on your own time. Teams that want to succeed need to consider their knowledge and skill investments as well.\\n\\nIf your team is serious about improvement and innovation, you need to schedule it. Trying to get things done \"whenever there\\'s a free moment\" means *they will never happen.* Whatever sort of backlog or task list or flow you\\'re working with, don\\'t reserve it for only feature development. The team works on more than just new features. Some possible examples include:\\n\\n#### *Old Systems Maintenance*\\n\\nWhile we love working on the shiny new system, there\\'s likely maintenance work that needs to be done on the old system. We\\'ve met teams who try and shove this work in the corner. If the team is charged with doing these tasks, then do them—for real.\\n\\n### *Process Reflection and Refinement*\\n\\nContinuous improvement can only happen when you take the time to look around, figure out what\\'s working and not, and then make changes (see Topic 48, *The Essence of Agility*). Too many teams are so busy bailing out water that they don\\'t have time to fix the leak. Schedule it. Fix it.\\n\\n#### *New tech experiments*\\n\\nDon\\'t adopt new tech, frameworks, or libraries just because \"everyone is doing it,\" or based on something you saw at a conference or read online. Deliberately vet candidate technologies with prototypes. Put tasks on the schedule to try the new things and analyze results.\\n\\n### *Learning and skill improvements*\\n\\nPersonal learning and improvements are a great start, but many skills are more effective when spread team-wide. Plan to do it, whether it\\'s the informal brown-bag lunch or more formal training sessions.\\n\\n## **Tip 85** Schedule It to Make It Happen\\n\\n## **Communicate Team Presence**\\n\\nIt\\'s obvious that developers in a team must talk to each other. We gave some suggestions to facilitate this in Topic 7, *Communicate!*. However, it\\'s easy to forget that the team itself has a presence within the organization. The team as an entity needs to communicate clearly with the rest of the world.\\n\\nTo outsiders, the worst project teams are those that appear sullen and reticent. They hold meetings with no structure, where no one wants to talk. Their emails and project documents are a mess: no two look the same, and each uses different terminology.\\n\\nGreat project teams have a distinct personality. People look forward to meetings with them, because they know that they\\'ll see a well-prepared performance that makes everyone feel good. The documentation they produce is crisp, accurate, and consistent. The team speaks with one voice. [77] They may even have a sense of humor.\\n\\nThere is a simple marketing trick that helps teams communicate as one: generate a brand. When you start a project, come up with a name for it, ideally something off-the-wall. (In the past, we\\'ve named projects after things such as killer parrots that prey on sheep, optical illusions, gerbils, cartoon characters, and mythical cities.) Spend 30 minutes coming up with a zany logo, and use it. Use your team\\'s name liberally when talking with people. It sounds silly, but it gives your team an identity to build on, and the world something memorable to associate with your work.\\n\\n## **Don\\'t Repeat Yourselves**\\n\\nIn Topic 9, *DRY—The Evils of Duplication*, we talked about the difficulties of eliminating duplicated work between members of a team. This duplication leads to wasted effort, and can result in a maintenance nightmare. \"Stovepipe\" or \"siloed\" systems are common in these teams, with little sharing and a lot of duplicated functionality.\\n\\nGood communication is key to avoiding these problems. And by \"good\" we mean *instant* and *frictionless*.\\n\\nYou should be able to ask a question of team members and get a more-orless instant reply. If the team is co-located, this might be as simple as poking your head over the cube wall or down the hall. For remote teams, you may have to rely on a messaging app or other electronic means.\\n\\nIf you have to wait a week for the team meeting to ask your question or share your status, that\\'s an awful lot of friction.[78] Frictionless means it\\'s easy and low-ceremony to ask questions, share your progress, your problems, your insights and learnings, and to stay aware of what your teammates are doing.\\n\\nMaintain awareness to stay DRY.\\n\\n## **Team Tracer Bullets**\\n\\nA project team has to accomplish many different tasks in different areas of the project, touching a lot of different technologies. Understanding requirements, designing architecture, coding for frontend and server, testing, all have to happen. But it\\'s a common misconception that these activities and tasks can happen separately, in isolation. They can\\'t.\\n\\nSome methodologies advocate all sort of different roles and titles within the team, or create separate specialized teams entirely. But the problem with that approach is that it introduces *gates* and *handoffs*. Now instead of a smooth flow from the team to deployment, you have artificial gates where the work stops. Handoffs that have to wait to be accepted. Approvals. Paperwork. The Lean folks call this *waste*, and strive to actively eliminate it.\\n\\nAll of these different roles and activities are actually different views of the same problem, and artificially separating them can cause a boatload of trouble. For example, programmers who are two or three levels removed from the actual users of their code are unlikely to be aware of the context in which their work is used. They will not be able to make informed decisions.\\n\\nWith Topic 12, *Tracer Bullets*, we recommend developing individual features, however small and limited initially, that go end-to-end through the entire system. That means that you need all the skills to do that within the team: frontend, UI/UX, server, DBA, QA, etc., all comfortable and accustomed to working with each other. With a tracer bullet approach, you can implement very small bits of functionality very quickly, and get immediate feedback on how well your team communicates and delivers. That creates an environment where you can make changes and tune your team and process quickly and easily.\\n\\n## **Tip 86** Organize Fully Functional Teams\\n\\nBuild teams so you can build code end-to-end, incrementally and iteratively.\\n\\n## **Automation**\\n\\nA great way to ensure both consistency and accuracy is to automate everything the team does. Why struggle with code formatting standards when your editor or IDE can do it for you automatically? Why do manual testing when the continuous build can run tests automatically? Why deploy by hand when automation can do it the same way every time, repeatably and reliably?\\n\\nAutomation is an essential component of every project team. Make sure the team has skills at *tool building* to construct and deploy the tools that automate the project development and production deployment.\\n\\n## **Know When to Stop Adding Paint**\\n\\nRemember that teams are made up of individuals. Give each member the ability to shine in their own way. Give them just enough structure to support them and to ensure that the project delivers value. Then, like the painter in Topic 5, *Good-Enough Software*, resist the temptation to add more paint.\\n\\n## **Related Sections Include**\\n\\n- Topic 2, *The Cat Ate My Source Code*\\n- Topic 7, *Communicate!*\\n- Topic 12, *Tracer Bullets*\\n- Topic 19, *Version Control*\\n- Topic 50, *Coconuts Don\\'t Cut It*\\n- Topic 51, *Pragmatic Starter Kit*\\n\\n## **Challenges**\\n\\n- Look around for successful teams outside the area of software development. What makes them successful? Do they use any of the\\nprocesses discussed in this section?\\n\\n- Next time you start a project, try convincing people to brand it. Give your organization time to become used to the idea, and then do a quick audit to see what difference it made, both within the team and externally.\\n- You were probably once given problems such as \"If it takes 4 workers 6 hours to dig a ditch, how long would it take 8 workers?\" In real life, however, what factors affect the answer if the workers were writing code instead? In how many scenarios is the time actually reduced?\\n- Read *The Mythical Man Month* [Bro96] by Frederick Brooks. For extra credit, buy two copies so you can read it twice as fast.\\n\\nThe native islanders had never seen an airplane before, or met people such as these strangers. In return for use of their land, the strangers provided mechanical birds that flew in and out all day long on a \"runway,\" bringing incredible material wealth to their island home. The strangers mentioned something about war and fighting. One day it was over and they all left, taking their strange riches with them.\\n\\nThe islanders were desperate to restore their good fortunes, and re-built a facsimile of the airport, control tower, and equipment using local materials: vines, coconut shells, palm fronds, and such. But for some reason, even though they had everything in place, the planes didn\\'t come. They had imitated the form, but not the content. Anthropologists call this a *cargo cult*.\\n\\nAll too often, *we* are the islanders.\\n\\nIt\\'s easy and tempting to fall into the cargo cult trap: by investing in and building up the easily-visible artifacts, you hope to attract the underlying, working magic. But as with the original cargo cults of Melanesia,[79] a fake airport made out of coconut shells is no substitute for the real thing.\\n\\nFor example, we have personally seen teams that claim to be using Scrum. But, upon closer examination, it turned out they were doing a daily stand up meeting once a week, with four-week iterations that often turned into six- or eight-week iterations. They felt that this was okay because they were using a popular \"agile\" scheduling tool. They were only investing in the superficial artifacts—and even then, often in name only, as if \"stand up\" or \"iteration\" were some sort of incantation for the superstitious. Unsurprisingly, they, too, failed to attract the real magic.\\n\\n## **Context Matters**\\n\\nHave you or your team fallen in this trap? Ask yourself, why are you even using that particular development method? Or that framework? Or that testing technique? Is it actually well-suited for the job at hand? Does it work well for you? Or was it adopted just because it was being used by the latest internet-fueled success story?\\n\\nThere\\'s a current trend to adopt the policies and processes of successful companies such as Spotify, Netflix, Stripe, GitLab, and others. Each have their own unique take on software development and management. But consider the context: are you in the same market, with the same constraints and opportunities, similar expertise and organization size, similar management, and similar culture? Similar user base and requirements?\\n\\nDon\\'t fall for it. Particular artifacts, superficial structures, policies, processes, and methods are not enough.\\n\\n## **Tip 87** Do What Works, Not What\\'s Fashionable\\n\\nHow do you know \"what works\"? You rely on that most fundamental of Pragmatic techniques:\\n\\nTry it.\\n\\nPilot the idea with a small team or set of teams. Keep the good bits that seem to work well, and discard anything else as waste or overhead. No one will downgrade your organization because it operates differently from Spotify or Netflix, because even they didn\\'t follow their current processes while they were growing. And years from now, as those companies mature and pivot and continue to thrive, they\\'ll be doing something different yet again.\\n\\n*That\\'s* the actual secret to their success.\\n\\n## **One Size Fits No One Well**\\n\\nThe purpose of a software development methodology is to help people work together. As we discuss in Topic 48, *The Essence of Agility*, there is no single plan you can follow when you develop software, especially not a plan that someone *else* came up with at another company.\\n\\nMany certification programs are actually even worse than that: they are predicated on the student being able to memorize and follow the rules. But that\\'s not what you want. You need the ability to see beyond the existing rules and exploit possibilities for advantage. That\\'s a very different mindset from \"but Scrum/Lean/Kanban/XP/agile does it this way…\" and so on.\\n\\nInstead, you want to take the best pieces from any particular methodology and adapt them for use. No one size fits all, and current methods are far from complete, so you\\'ll need to look at more than just one popular method.\\n\\nFor example, Scrum defines some project management practices, but Scrum by itself doesn\\'t provide enough guidance at the technical level for teams or at the portfolio/governance level for leadership. So where do you start?\\n\\n#### **Be Like Them!**\\n\\nWe frequently hear software development leaders tell their staff, \"We should operate like Netflix\" (or one of these other leading companies). Of course you *could* do that.\\n\\nFirst, get yourself a few hundred thousand servers and tens of millions of users...\\n\\n## **The Real Goal**\\n\\nThe goal of course isn\\'t to \"do Scrum,\" \"do agile,\" \"do Lean,\" or whathave-you. The goal is to be in a position to deliver working software that gives the users some new capability *at a moment\\'s notice.* Not weeks, months, or years from now, but *now*. For many teams and organizations, continuous delivery feels like a lofty, unattainable goal, especially if you\\'re saddled with a process that restricts delivery to months, or even weeks. But as with any goal, the key is to keep aiming in the right direction.\\n\\n![](_page_401_Figure_1.jpeg)\\n\\nIf you\\'re delivering in years, try and shorten the cycle to months. From months, cut it down to weeks. From a four-week sprint, try two. From a two week sprint, try one. Then daily. Then, finally, on demand. Note that being able to deliver on demand does not mean you are forced to deliver every minute of every day. You deliver when the users need it, when it makes business sense to do so.\\n\\n![](_page_401_Figure_3.jpeg)\\n\\nIn order to move to this style of continuous development, you need a rocksolid infrastructure, which we discuss in the next topic, Topic 51, *Pragmatic Starter Kit*. You do development in the main trunk of your version control system, not in branches, and use techniques such as *feature switches* to roll out test features to users selectively.\\n\\nOnce your infrastructure is in order, you need to decide how to organize the work. Beginners might want to start with Scrum for project management, plus the technical practices from eXtreme Programming (XP). More\\n\\ndisciplined and experienced teams might look to Kanban and Lean techniques, both for the team and perhaps for larger governance issues.\\n\\nBut don\\'t take our word for it, investigate and try these approaches for yourself. Be careful, though, in overdoing it. Overly investing in any particular methodology can leave you blind to alternatives. You get used to it. Soon it becomes hard to see any other way. You\\'ve become calcified, and now you can\\'t adapt quickly anymore.\\n\\nMight as well be using coconuts.\\n\\n## **Related Sections Include**\\n\\n- Topic 12, *Tracer Bullets*\\n- Topic 27, *Don\\'t Outrun Your Headlights*\\n- Topic 48, *The Essence of Agility*\\n- Topic 49, *Pragmatic Teams*\\n- Topic 51, *Pragmatic Starter Kit*\\n\\n## **Topic 51** Pragmatic Starter Kit\\n\\n*Civilization advances by extending the number of important operations we can perform without thinking.*\\n\\n*Alfred North Whitehead*\\n\\nBack when cars were a novelty, the instructions for starting a Model-T Ford were more than two pages long. With modern cars, you just push a button the starting procedure is automatic and foolproof. A person following a list of instructions might flood the engine, but\\n\\nthe automatic starter won\\'t.\\n\\nAlthough software development is still an industry at the Model-T stage, we can\\'t afford to go through two pages of instructions again and again for some common operation. Whether it is the build and release procedure, testing, project paperwork, or any other recurring task on the project, it has to be automatic and repeatable on any capable machine.\\n\\nIn addition, we want to ensure consistency and repeatability on the project. Manual procedures leave consistency up to chance; repeatability isn\\'t guaranteed, especially if aspects of the procedure are open to interpretation by different people.\\n\\nAfter we wrote the first edition of *The Pragmatic Programmer,* we wanted to create more books to help teams develop software. We figured we should start at the beginning: what are the most basic, most important elements that *every* team needs regardless of methodology, language, or technology stack. And so the idea of the *Pragmatic Starter Kit* was born, covering these three critical and interrelated topics:\\n\\n- Version Control\\n- Regression Testing\\n- Full Automation\\n\\nThese are the three legs that support every project. Here\\'s how.\\n\\n## **Drive with Version Control**\\n\\nAs we said in Topic 19, *Version Control*, you want to keep everything needed to build your project under version control. That idea becomes even more important in the context of the project itself.\\n\\nFirst, it allows build machines to be ephemeral. Instead of one hallowed, creaky machine in the corner of the office that everyone is afraid to touch, [80] build machines and/or clusters are created on demand as spot instances in the cloud. Deployment configuration is under version control as well, so releasing to production can be handled automatically.\\n\\nAnd that\\'s the important part: at the project level, version control *drives* the build and release process.\\n\\n**Tip 89** Use Version Control to Drive Builds, Tests, and Releases\\n\\nThat is, build, test, and deployment are triggered via commits or pushes to version control, and built in a container in the cloud. Release to staging or production is specified by using a tag in your version control system. Releases then become a much more low-ceremony part of every day life true continuous delivery, not tied to any one build machine or developer\\'s machine.\\n\\n## **Ruthless and Continuous Testing**\\n\\nMany developers test gently, subconsciously knowing where the code will break and avoiding the weak spots. Pragmatic Programmers are different. We are *driven* to find our bugs *now*, so we don\\'t have to endure the shame of others finding our bugs later.\\n\\nFinding bugs is somewhat like fishing with a net. We use fine, small nets (unit tests) to catch the minnows, and big, coarse nets (integration tests) to catch the killer sharks. Sometimes the fish manage to escape, so we patch any holes that we find, in hopes of catching more and more slippery defects that are swimming about in our project pool.\\n\\n- \\n## **Tip 90** Test Early, Test Often, Test Automatically\\n\\nWe want to start testing as soon as we have code. Those tiny minnows have a nasty habit of becoming giant, man-eating sharks pretty fast, and catching a shark is quite a bit harder. So we write unit tests. A lot of unit tests.\\n\\nIn fact, a good project may well have *more* test code than production code. The time it takes to produce this test code is worth the effort. It ends up being much cheaper in the long run, and you actually stand a chance of producing a product with close to zero defects.\\n\\nAdditionally, knowing that you\\'ve passed the test gives you a high degree of confidence that a piece of code is \"done.\\'\\'\\n\\n## **Tip 91** Coding Ain\\'t Done \\'Til All the Tests Run\\n\\nThe automatic build runs all available tests. It\\'s important to aim to \"test for real,\" in other words, the test environment should match the production environment closely. Any gaps are where bugs breed.\\n\\nThe build may cover several major types of software testing: unit testing; integration testing; validation and verification; and performance testing.\\n\\nThis list is by no means complete, and some specialized projects will require various other types of testing as well. But it gives us a good starting point.\\n\\n### **Unit Testing**\\n\\nA *unit test* is code that exercises a module. We covered this in Topic 41, *Test to Code*. Unit testing is the foundation of all the other forms of testing that we\\'ll discuss in this section. If the parts don\\'t work by themselves, they probably won\\'t work well together. All of the modules you are using must pass their own unit tests before you can proceed.\\n\\nOnce all of the pertinent modules have passed their individual tests, you\\'re ready for the next stage. You need to test how all the modules use and interact with each other throughout the system.\\n\\n## **Integration Testing**\\n\\n*Integration testing* shows that the major subsystems that make up the project work and play well with each other. With good contracts in place and well tested, any integration issues can be detected easily. Otherwise, integration becomes a fertile breeding ground for bugs. In fact, it is often the single largest source of bugs in the system.\\n\\nIntegration testing is really just an extension of the unit testing we\\'ve described—you\\'re just testing how entire subsystems honor their contracts.\\n\\n## **Validation and Verification**\\n\\nAs soon as you have an executable user interface or prototype, you need to answer an all-important question: the users told you what they wanted, but is it what they need?\\n\\nDoes it meet the functional requirements of the system? This, too, needs to be tested. A bug-free system that answers the wrong question isn\\'t very useful. Be conscious of end-user access patterns and how they differ from developer test data (for an example, see the story about brush strokes here).\\n\\n## **Performance Testing**\\n\\nPerformance or stress testing may be important aspects of the project as well.\\n\\nAsk yourself if the software meets the performance requirements under real-world conditions—with the expected number of users, or connections, or transactions per second. Is it scalable?\\n\\nFor some applications, you may need specialized testing hardware or software to simulate the load realistically.\\n\\n## **Testing the Tests**\\n\\nBecause we can\\'t write perfect software, it follows that we can\\'t write perfect test software either. We need to test the tests.\\n\\nThink of our set of test suites as an elaborate security system, designed to sound the alarm when a bug shows up. How better to test a security system than to try to break in?\\n\\nAfter you have written a test to detect a particular bug, *cause* the bug deliberately and make sure the test complains. This ensures that the test will catch the bug if it happens for real.\\n\\n![](_page_407_Picture_7.jpeg)\\n\\nIf you are *really* serious about testing, take a separate branch of the source tree, introduce bugs on purpose, and verify that the tests will catch them. At a higher level, you can use something like Netflix\\'s *Chaos Monkey*[81] to disrupt (i.e., \"kill\") services and test your application\\'s resilience.\\n\\nWhen writing tests, make sure that alarms sound when they should.\\n\\n## **Testing Thoroughly**\\n\\nOnce you are confident that your tests are correct, and are finding bugs you create, how do you know if you have tested the code base thoroughly enough?\\n\\nThe short answer is \"you don\\'t,\\'\\' and you never will. You might look to try *coverage analysis* tools that watch your code during testing and keep track of which lines of code have been executed and which haven\\'t. These tools help give you a general feel for how comprehensive your testing is, but don\\'t expect to see 100% coverage.[82]\\n\\nEven if you do happen to hit every line of code, that\\'s not the whole picture. What *is* important is the number of states that your program may have. States are not equivalent to lines of code. For instance, suppose you have a function that takes two integers, each of which can be a number from 0 to 999:\\n\\n```\\nint test( int a, int b) {\\n return a / (a + b);\\n}\\n```\\nIn theory, this three-line function has 1,000,000 logical states, 999,999 of which will work correctly and one that will not (when a + b equals zero). Simply knowing that you executed this line of code doesn\\'t tell you that you would need to identify all possible states of the program. Unfortunately, in general this is a *really hard* problem. Hard as in, \"The sun will be a cold hard lump before you can solve it.\"\\n\\n## **Tip 93** Test State Coverage, Not Code Coverage\\n\\n## **Property-Based Testing**\\n\\nA great way to explore how your code handles unexpected states is to have a computer generate those states.\\n\\nUse *property-based* testing techniques to generate test data according to the contracts and invariants of the code under test. We cover this topic in detail in Topic 42, *Property-Based Testing*.\\n\\n## **Tightening the Net**\\n\\nFinally, we\\'d like to reveal the single most important concept in testing. It is an obvious one, and virtually every textbook says to do it this way. But for some reason, most projects still do not.\\n\\nIf a bug slips through the net of existing tests, you need to add a new test to trap it next time.\\n\\n![](_page_409_Picture_4.jpeg)\\n\\nOnce a human tester finds a bug, it should be the *last* time a human tester finds that bug. The automated tests should be modified to check for that particular bug from then on, every time, with no exceptions, no matter how trivial, and no matter how much the developer complains and says, \"Oh, that will never happen again.\"\\n\\nBecause it will happen again. And we just don\\'t have the time to go chasing after bugs that the automated tests could have found for us. We have to spend our time writing new code—and new bugs.\\n\\n## **Full Automation**\\n\\nAs we said at the beginning of this section, modern development relies on scripted, automatic procedures. Whether you use something as simple as shell scripts with rsync and ssh, or full-featured solutions such as Ansible, Puppet, Chef, or Salt, just don\\'t rely on any manual intervention.\\n\\nOnce upon a time, we were at a client site where all the developers were using the same IDE. Their system administrator gave each developer a set of instructions on installing add-on packages to the IDE. These instructions filled many pages—pages full of click here, scroll there, drag this, doubleclick that, and do it again.\\n\\nNot surprisingly, every developer\\'s machine was loaded slightly differently. Subtle differences in the application\\'s behavior occurred when different developers ran the same code. Bugs would appear on one machine but not on others. Tracking down version differences of any one component usually revealed a surprise.\\n\\n### **Tip 95** Don\\'t Use Manual Procedures\\n\\nPeople just aren\\'t as repeatable as computers are. Nor should we expect them to be. A shell script or program will execute the same instructions, in the same order, time after time. It is under version control itself, so you can examine changes to the build/release procedures over time as well (\"but it *used* to work…\").\\n\\nEverything depends on automation. You can\\'t build the project on an anonymous cloud server unless the build is fully automatic. You can\\'t deploy automatically if there are manual steps involved. And once you introduce manual steps (\"just for this one part…\") you\\'ve broken a very large window. [83]\\n\\nWith these three legs of version control, ruthless testing, and full automation, your project will have the firm foundation you need so you can concentrate on the hard part: delighting users.\\n\\n## **Related Sections Include**\\n\\n- Topic 11, *Reversibility*\\n- Topic 12, *Tracer Bullets*\\n- Topic 17, *Shell Games*\\n- Topic 19, *Version Control*\\n- Topic 41, *Test to Code*\\n- Topic 49, *Pragmatic Teams*\\n- Topic 50, *Coconuts Don\\'t Cut It*\\n\\n## **Challenges**\\n\\n- Are your nightly or continuous builds automatic, but deploying to production isn\\'t? Why? What\\'s special about that server?\\n- Can you automatically test your project completely? Many teams are forced to answer \"no.\" Why? Is it too hard to define the acceptable results? Won\\'t this make it hard to prove to the sponsors that the project is \"done\"?\\n- Is it too hard to test the application logic independent of the GUI? What does this say about the GUI? About coupling?\\n\\n## **Topic 52** Delight Your Users\\n\\n*When you enchant people, your goal is not to make money from them or to get them to do what you want, but to fill them with great delight.*\\n\\n*Guy Kawasaki*\\n\\nOur goal as developers is to *delight users.* That\\'s why we\\'re here. Not to mine them for their data, or count their eyeballs or empty their wallets. Nefarious goals aside, even delivering working software in a timely manner isn\\'t enough. That alone won\\'t delight them.\\n\\nYour users are not particularly motivated by code. Instead, they have a business problem that needs solving within the context of their objectives and budget. Their belief is that by working with your team they\\'ll be able to do this.\\n\\nTheir expectations are not software related. They aren\\'t even implicit in any specification they give you (because that specification will be incomplete until your team has iterated through it with them several times).\\n\\nHow do you unearth their expectations, then? Ask a simple question:\\n\\nHow will you know that we\\'ve all been successful a month (or a year, or whatever) after this project is done?\\n\\nYou may well be surprised by the answer. A project to improve product recommendations might actually be judged in terms of customer retention; a project to consolidate two databases might be judged in terms of data quality, or it might be about cost savings. But it\\'s these expectations of business value that really count—not just the software project itself. The software is only a means to these ends.\\n\\nAnd now that you\\'ve surfaced some of the underlying expectations of value behind the project, you can start thinking about how you can deliver against them:\\n\\n- Make sure everyone on the team is totally clear about these expectations.\\n- When making decisions, think about which path forward moves closer to those expectations.\\n- Critically analyze the user requirements in light of the expectations. On many projects we\\'ve discovered that the stated \"requirement\" was in fact just a guess at what could be done by technology: it was actually an amateur implementation plan dressed up as a requirements document. Don\\'t be afraid to make suggestions that change the requirement if you can demonstrate that they will move the project closer to the objective.\\n- Continue to think about these expectations as you progress through the project.\\n\\nWe\\'ve found that as our knowledge of the domain increases, we\\'re better able to make suggestions on other things that could be done to address the underlying business issues. We strongly believe that developers, who are exposed to many different aspects of an organization, can often see ways of weaving different parts of the business together that aren\\'t always obvious to individual departments.\\n\\n## **Tip 96** Delight Users, Don\\'t Just Deliver Code\\n\\nIf you want to delight your client, forge a relationship with them where you can actively help solve their problems. Even though your title might be some variation of \"Software Developer\" or \"Software Engineer,\" in truth it\\n\\nshould be \"Problem Solver.\" That\\'s what we do, and that\\'s the essence of a Pragmatic Programmer.\\n\\nWe solve problems.\\n\\n## **Related Sections Include**\\n\\n- Topic 12, *Tracer Bullets*\\n- Topic 13, *Prototypes and Post-it Notes*\\n- Topic 45, *The Requirements Pit*\\n\\n## **Topic 53** Pride and Prejudice\\n\\n*You have delighted us long enough.*\\n\\n*Jane Austen, Pride and Prejudice*\\n\\nPragmatic Programmers don\\'t shirk from responsibility. Instead, we rejoice in accepting challenges and in making our expertise well known. If we are responsible for a design, or a piece of\\n\\ncode, we do a job we can be proud of.\\n\\n![](_page_415_Picture_6.jpeg)\\n\\nArtisans of an earlier age were proud to sign their work. You should be, too.\\n\\nProject teams are still made up of people, however, and this rule can cause trouble. On some projects, the idea of *code ownership* can cause cooperation problems. People may become territorial, or unwilling to work on common foundation elements. The project may end up like a bunch of insular little fiefdoms. You become prejudiced in favor of your code and against your coworkers.\\n\\nThat\\'s not what we want. You shouldn\\'t jealously defend your code against interlopers; by the same token, you should treat other people\\'s code with respect. The Golden Rule (\"Do unto others as you would have them do unto you\\'\\') and a foundation of mutual respect among the developers is critical to make this tip work.\\n\\nAnonymity, especially on large projects, can provide a breeding ground for sloppiness, mistakes, sloth, and bad code. It becomes too easy to see yourself as just a cog in the wheel, producing lame excuses in endless status reports instead of good code.\\n\\nWhile code must be owned, it doesn\\'t have to be owned by an individual. In fact, Kent Beck\\'s eXtreme Programming[84] recommends communal ownership of code (but this also requires additional practices, such as pair programming, to guard against the dangers of anonymity).\\n\\nWe want to see pride of ownership. \"I wrote this, and I stand behind my work.\" Your signature should come to be recognized as an indicator of quality. People should see your name on a piece of code and expect it to be solid, well written, tested, and documented. A really professional job. Written by a professional.\\n\\nA Pragmatic Programmer.\\n\\nThank you.\\n\\n#### **Footnotes**\\n\\n- [75] As team size grows, communication paths grow at the rate of , where is the number of team members. On larger teams, communication begins to break down and becomes ineffective.\\n- [76] A *burnup* chart is better for this than the more usual *burndown* chart. With a burnup chart, you can clearly see how the additional features move the goalposts.\\n- [77] The team speaks with one voice—externally. Internally, we strongly encourage lively, robust debate. Good developers tend to be passionate about their work.\\n- [78] Andy has met teams who conduct their daily Scrum standups on Fridays.\\n- [79] See https://en.wikipedia.org/wiki/Cargo_cult.\\n- [80]\\n\\nWe\\'ve seen this first-hand more times than you\\'d think.\\n\\n- [81] https://netflix.github.io/chaosmonkey\\n- [82] For an interesting study of the correlation between test coverage and defects, see *Mythical Unit Test Coverage* [ADSS18].\\n- [83] Always remember Topic 3, *Software Entropy*. Always.\\n- [84] http://www.extremeprogramming.org\\n\\n#### Copyright © 2020 Pearson Education, Inc.\\n\\n*In the long run, we shape our lives, and we shape ourselves. The process never ends until we die. And the choices we make are ultimately our own responsibility.*\\n\\n*Eleanor Roosevelt*\\n\\n# Chapter 10\\n\\n# **Postface**\\n\\nIn the twenty years leading up to the first edition, we were part of the evolution of the computer from a peripheral curiosity to a modern imperative for businesses. In the twenty years since then, software has grown beyond mere business machines and has truly taken over the world. But what does that really mean for us?\\n\\nIn *The Mythical Man-Month: Essays on Software Engineering* [Bro96], Fred Brooks said \"The programmer, like the poet, works only slightly removed from pure thought-stuff. He builds his castles in the air, from air, creating by exertion of the imagination.\" We start with a blank page, and we can create pretty much anything we can imagine. And the things we create can change the world.\\n\\nFrom Twitter helping people plan revolutions, to the processor in your car working to stop you skidding, to the smartphone which means we no longer have to remember pesky daily details, our programs are everywhere. Our imagination is everywhere.\\n\\nWe developers are incredibly privileged. We are truly building the future. It\\'s an extraordinary amount of power. And with that power comes an extraordinary responsibility.\\n\\nHow often do we stop to think about that? How often do we discuss, both among ourselves and with a more general audience, what this means?\\n\\nEmbedded devices use an order of magnitude more computers than those used in laptops, desktops, and data centers. These embedded computers often control life-critical systems, from power plants to cars to medical equipment. Even a simple central heating control system or home appliance can kill someone if it is poorly designed or implemented. When you develop for these devices, you take on a staggering responsibility.\\n\\nMany nonembedded systems can also do both great good and great harm. Social media can promote peaceful revolution or foment ugly hate. Big data can make shopping easier, and it can destroy any vestige of privacy you might think you have. Banking systems make loan decisions that change people\\'s lives. And just about any system can be used to snoop on its users.\\n\\nWe\\'ve seen hints of the possibilities of a utopian future, and examples of unintended consequences leading to nightmare dystopias. The difference between the two outcomes might be more subtle than you think. And it\\'s all in your hands.\\n\\n## **The Moral Compass**\\n\\nThe price of this unexpected power is vigilance. Our actions directly affect people. No longer the hobby program on the 8-bit CPU in the garage, the isolated batch business process on the mainframe in the data center, or even just the desktop PC; our software weaves the very fabric of daily modern life.\\n\\nWe have a duty to ask ourselves two questions about every piece of code we deliver:\\n\\n- 1. Have I protected the user?\\n- 2. Would I use this myself?\\n\\nFirst, you should ask \"Have I done my best to protect the users of this code from harm?\" Have I made provisions to apply ongoing security patches to that simple baby monitor? Have I ensured that *however* the automatic central heating thermostat fails the customer will still have manual control? Am I storing only the data I need, and encrypting anything personal?\\n\\nNo one is perfect; everyone misses things now and then. But if you can\\'t truthfully say that you tried to list all the consequences, and made sure to protect the users from them, then you bear some responsibility when things go bad.\\n\\n![](_page_420_Picture_7.jpeg)\\n\\nSecond, there\\'s a judgment related to the Golden Rule: would I be happy to be a user of this software? Do I want my details shared? Do I want my movements to be given to retail outlets? Would I be happy to be driven by this autonomous vehicle? Am I comfortable doing this?\\n\\nSome inventive ideas begin to skirt the bounds of ethical behavior, and if you\\'re involved in that project, you are just as responsible as the sponsors. No matter how many degrees of separation you might rationalize, one rule remains true:\\n\\n![](_page_421_Picture_1.jpeg)\\n\\n## **Imagine the Future you Want**\\n\\nIt\\'s up to you. It\\'s your imagination, your hopes, your concerns that provide the pure thought-stuff that builds the next twenty years and beyond.\\n\\nYou are building the future, for yourselves and for your descendants. Your duty is to make it a future that we\\'d all want to inhabit. Recognize when you\\'re doing something against this ideal, and have the courage to say \"no!\" Envision the future we *could* have, and have the courage to create it. Build castles in the air every day.\\n\\nWe all have an amazing life.\\n\\n![](_page_422_Picture_4.jpeg)\\n\\nCopyright © 2020 Pearson Education, Inc.\\n\\n# Appendix 1\\n\\n# **Bibliography**\\n\\n| [ADSS18] | Vard Antinyan, Jesper Derehag, Anna Sandberg, and Miroslaw |\\n| --- | --- |\\n|  | Staron. Mythical Unit Test Coverage. IEEE Software. 35:73- |\\n|  | 79, 2018. |\\n| [And10] | Jackie Andrade. What does doodling do? Applied Cognitive |\\n|  | Psychology. 24(1):100-106, 2010, January. |\\n| [Arm07] | Joe Armstrong. Programming Erlang: Software for a |\\n|  | Concurrent World. The Pragmatic Bookshelf, Raleigh, NC, |\\n|  | 2007. |\\n| [BR89] | Albert J. Bernstein and Sydney Craft Rozen. Dinosaur Brains: |\\n|  | Dealing with All Those Impossible People at Work. John Wiley |\\n|  | & Sons, New York, NY, 1989. |\\n| [Bro96] | Frederick P. Brooks, Jr. The Mythical Man-Month: Essays on |\\n|  | Software Engineering. Addison-Wesley, Reading, MA, |\\n|  | Anniversary, 1996. |\\n| [CN91] | Brad J. Cox and Andrew J. Novobilski. Object-Oriented |\\n|  | Programming: An Evolutionary Approach. Addison-Wesley, |\\n|  | Reading, MA, Second, 1991. |\\n| [Con68] | Melvin E. Conway. How do Committees Invent? Datamation. |\\n|  | 14(5):28-31, 1968, April. |\\n| [de 98] | Gavin de Becker. The Gift of Fear: And Other Survival |\\n|  | Signals That Protect Us from Violence. Dell Publishing, New |\\n|  | York City, 1998. |\\n| [DL13] | Tom DeMacro and Tim Lister. Peopleware: Productive Projects |\\n|  | and Teams. Addison-Wesley, Boston, MA, Third, 2013. |\\n\\n| [Fow00] | Martin Fowler. UML Distilled: A Brief Guide to the Standard |\\n| --- | --- |\\n|  | Object Modeling Language. Addison-Wesley, Boston, MA, |\\n|  | Second, 2000. |\\n| [Fow04] | Martin Fowler. UML Distilled: A Brief Guide to the Standard |\\n|  | Object Modeling Language. Addison-Wesley, Boston, MA, |\\n|  | Third, 2004. |\\n| [Fow19] | Martin Fowler. Refactoring: Improving the Design of Existing |\\n|  | Code. Addison-Wesley, Boston, MA, Second, 2019. |\\n| [GHJV95] | Erich Gamma, Richard Helm, Ralph Johnson, and John |\\n|  | Vlissides. Design Patterns: Elements of Reusable Object |\\n|  | Oriented Software. Addison-Wesley, Reading, MA, 1995. |\\n| [Hol92] | Michael Holt. Math Puzzles & Games. Dorset House, New |\\n|  | York, NY, 1992. |\\n| [Hun08] | Andy Hunt. Pragmatic Thinking and Learning: Refactor Your |\\n|  | Wetware. The Pragmatic Bookshelf, Raleigh, NC, 2008. |\\n| [Joi94] | T.E. Joiner. Contagious depression: Existence, specificity to |\\n|  | depressed symptoms, and the role of reassurance seeking. |\\n|  | Journal of Personality and Social Psychology. 67(2):287--296, |\\n|  | 1994, August. |\\n| [Knu11] | Donald E. Knuth. The Art of Computer Programming, Volume |\\n|  | 4A: Combinatorial Algorithms, Part 1. Addison-Wesley, |\\n|  | Boston, MA, 2011. |\\n| [Knu98] | Donald E. Knuth. The Art of Computer Programming, Volume |\\n|  | 1: Fundamental Algorithms. Addison-Wesley, Reading, MA, |\\n|  | Third, 1998. |\\n| [Knu98a] | Donald E. Knuth. The Art of Computer Programming, Volume |\\n|  | 2: Seminumerical Algorithms. Addison-Wesley, Reading, MA, |\\n|  | Third, 1998. |\\n| [Knu98b] | Donald E. Knuth. The Art of Computer Programming, Volume |\\n|  | 3: Sorting and Searching. Addison-Wesley, Reading, MA, |\\n|  | Second, 1998. |\\n| [KP99] | Brian W. Kernighan and Rob Pike. The Practice of |\\n|  | Programming. Addison-Wesley, Reading, MA, 1999. |\\n| [Mey97] |  |\\n\\n|  | Bertrand Meyer. Object-Oriented Software Construction. |\\n| --- | --- |\\n|  | Prentice Hall, Upper Saddle River, NJ, Second, 1997. |\\n| [Mul18] | Jerry Z. Muller. The Tyranny of Metrics. Princeton University |\\n|  | Press, Princeton NJ, 2018. |\\n| [SF13] | Robert Sedgewick and Phillipe Flajolet. An Introduction to the |\\n|  | Analysis of Algorithms. Addison-Wesley, Boston, MA, Second, |\\n|  | 2013. |\\n| [Str35] | James Ridley Stroop. Studies of Interference in Serial Verbal |\\n|  | Reactions. Journal of Experimental Psychology. 18:643--662, |\\n|  | 1935. |\\n| [SW11] | Robert Sedgewick and Kevin Wayne. Algorithms. Addison |\\n|  | Wesley, Boston, MA, Fourth, 2011. |\\n| [Tal10] | Nassim Nicholas Taleb. The Black Swan: Second Edition: The |\\n|  | Impact of the Highly Improbable. Random House, New York, |\\n|  | NY, Second, 2010. |\\n| [WH82] | James Q. Wilson and George Helling. The police and |\\n|  | neighborhood safety. The Atlantic Monthly. 249[3]:29--38, |\\n|  | 1982, March. |\\n| [YC79] | Edward Yourdon and Larry L. Constantine. Structured Design: |\\n|  | Fundamentals of a Discipline of Computer Program and |\\n|  | Systems Design. Prentice Hall, Englewood Cliffs, NJ, 1979. |\\n| [You95] | Edward Yourdon. When good-enough software is best. IEEE |\\n|  | Software. 1995, May. |\\n\\nCopyright © 2020 Pearson Education, Inc.\\n\\n*I would rather have questions that can\\'t be answered than answers that can\\'t be questioned.*\\n\\n*Richard Feynman*\\n\\n# Appendix 2\\n\\n# **Possible Answers to the Exercises**\\n\\n### **Answer 1** (from exercise 1)\\n\\nTo our way of thinking, class Split2 is more orthogonal. It concentrates on its own task, splitting lines, and ignores details such as where the lines are coming from. Not only does this make the code easier to develop, but it also makes it more flexible. Split2 can split lines read from a file, generated by another routine, or passed in via the environment.\\n\\n## **Answer 2** (from exercise 2)\\n\\nLet\\'s start with an assertion: you can write good, orthogonal code in just about any language. At the same time, every language has temptations: features that can lead to increased coupling and decreased orthogonality.\\n\\nIn OO languages, features such as multiple inheritance, exceptions, operator overloading, and parent-method overriding (via subclassing) provide ample opportunity to increase coupling in nonobvious ways. There is also a kind of coupling because a class couples code to data. This is normally a good thing (when coupling is good, we call it cohesion). But if you don\\'t make your classes focused enough, it can lead to some pretty ugly interfaces.\\n\\nIn functional languages, you\\'re encouraged to write lots of small, decoupled functions, and to combine them in different ways to solve your problem. In theory this sounds good. In practice it often is. But there\\'s a form of coupling that can happen here, too. These functions typically transform data, which means the result of one function can become the input to another. If you\\'re not careful, making a change to the data format a function generates can result in a failure somewhere down the transformational stream. Languages with good type systems can help mitigate this.\\n\\n**Answer 3** (from exercise 3)\\n\\nLow-tech to the rescue! Draw a few cartoons with markers on a whiteboard —a car, a phone, and a house. It doesn\\'t have to be great art; stick-figure outlines are fine. Put Post-it notes that describe the contents of target pages on the clickable areas. As the meeting progresses, you can refine the drawings and placements of the Post-it notes.\\n\\n#### **Answer 4** (from exercise 4)\\n\\nBecause we want to make the language extendable, we\\'ll make the parser table driven. Each entry in the table contains the command letter, a flag to say whether an argument is required, and the name of the routine to call to handle that particular command.\\n\\n```\\nlang/turtle.c\\n   typedef struct {\\n    char cmd; /* the command letter */\\n    int hasArg; /* does it take an argument */\\n    void (*func)( int, int); /* routine to call */\\n   } Command;\\n   static Command cmds[] = {\\n    { \\'P\\', ARG, doSelectPen },\\n    { \\'U\\', NO_ARG, doPenUp },\\n    { \\'D\\', NO_ARG, doPenDown },\\n    { \\'N\\', ARG, doPenDir },\\n    { \\'E\\', ARG, doPenDir },\\n    { \\'S\\', ARG, doPenDir },\\n    { \\'W\\', ARG, doPenDir }\\n   };\\n```\\nThe main program is pretty simple: read a line, look up the command, get the argument if required, then call the handler function.\\n\\n```\\nlang/turtle.c\\n   while (fgets(buff, sizeof(buff), stdin)) {\\n    Command *cmd = findCommand(*buff);\\n```\\n\\n```\\n if (cmd) {\\n int arg = 0;\\n if (cmd->hasArg && !getArg(buff+1, &arg)) {\\n fprintf(stderr, \"\\'%c\\' needs an argument\\\\n\", *buff);\\n continue;\\n }\\n cmd->func(*buff, arg);\\n }\\n}\\n```\\nThe function that looks up a command performs a linear search of the table, returning either the matching entry or NULL.\\n\\n```\\nlang/turtle.c\\n   Command *findCommand( int cmd) {\\n    int i;\\n    for (i = 0; i < ARRAY_SIZE(cmds); i++) {\\n    if (cmds[i].cmd == cmd)\\n    return cmds + i;\\n    }\\n    fprintf(stderr, \"Unknown command \\'%c\\'\\\\n\", cmd);\\n    return 0;\\n   }\\n```\\nFinally, reading the numeric argument is pretty simple using sscanf.\\n\\n```\\nlang/turtle.c\\n   int getArg( const char *buff, int *result) {\\n    return sscanf(buff, \"%d\", result) == 1;\\n   }\\n```\\n## **Answer 5** (from exercise 5)\\n\\nActually, you\\'ve already solved this problem in the previous exercise, where you wrote an interpreter for the external language, will contain the internal\\n\\ninterpreter. In the case of our sample code, this is the doXxx functions.\\n\\n**Answer 6** (from exercise 6)\\n\\nUsing BNF, a time specification could be\\n\\n| time | ::= | hour |  | ampm |  |  | hour |  | minute : | ampm |  | hour | : | minute |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| ampm | ::= | am | pm |  |  |  |  |  |  |  |  |  |  |  |\\n| hour | ::= |  |  | digit   digit |  | digit |  |  |  |  |  |  |  |  |\\n| minute | ::= |  | digit | digit |  |  |  |  |  |  |  |  |  |  |\\n| digit | ::= | 0   1 | 2 | 3 | 4 | 5 | 6 | 7 | 8   9 |  |  |  |  |  |\\n\\nA better definition of *hour* and *minute* would take into account that an hours can only be from 00 to 23, and a minute from 00 to 59:\\n\\n| hour | ::= | h-tens |  |  | digit |  | digit |  |  |  |  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| minute | ::= | m-tens |  |  | digit |  |  |  |  |  |  |\\n| h-tens | ::= | 0 | 1 |  |  |  |  |  |  |  |  |\\n| m-tens | ::= | 0 | 1 | 2 | 3 | 4 | 5 |  |  |  |  |\\n| digit | ::= | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |\\n\\n#### **Answer 7** (from exercise 7)\\n\\nHere\\'s the parser written using the Pegjs JavaScript library:\\n\\n```\\nlang/peg_parser/time_parser.pegjs\\n```\\n\\n```\\ntime\\n = h:hour offset:ampm { return h + offset }\\n / h:hour \":\" m:minute offset:ampm { return h + m + offset }\\n / h:hour \":\" m:minute { return h + m }\\nampm\\n = \"am\" { return 0 }\\n```\\n\\n```\\n / \"pm\" { return 12*60 }\\nhour\\n = h:two_hour_digits { return h*60 }\\n / h:digit { return h*60 }\\nminute\\n = d1:[0-5] d2:[0-9] { return parseInt(d1+d2, 10); }\\ndigit\\n = digit:[0-9] { return parseInt(digit, 10); }\\ntwo_hour_digits\\n = d1:[01] d2:[0-9 ] { return parseInt(d1+d2, 10); }\\n / d1:[2] d2:[0-3] { return parseInt(d1+d2, 10); }\\n```\\nThe tests show it in use:\\n\\n```\\nlang/peg_parser/test_time_parser.js\\n```\\n\\n```\\nlet test = require( \\'tape\\');\\nlet time_parser = require( \\'./time_parser.js\\');\\n// time ::= hour ampm |\\n// hour : minute ampm |\\n// hour : minute\\n//\\n// ampm ::= am | pm\\n//\\n// hour ::= digit | digit digit\\n//\\n// minute ::= digit digit\\n//\\n// digit ::= 0 |1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9\\nconst h = (val) => val*60;\\nconst m = (val) => val;\\nconst am = (val) => val;\\nconst pm = (val) => val + h(12);\\nlet tests = {\\n```\\n\\n```\\n \"1am\": h(1),\\n \"1pm\": pm(h(1)),\\n \"2:30\": h(2) + m(30),\\n \"14:30\": pm(h(2)) + m(30),\\n \"2:30pm\": pm(h(2)) + m(30),\\n}\\ntest( \\'time parsing\\', function (t) {\\n for ( const string in tests) {\\n let result = time_parser.parse(string)\\n t.equal(result, tests[string], string);\\n }\\n t.end()\\n});\\n```\\n\\n```\\nAnswer 8 (from exercise 8)\\n```\\nHere\\'s a possible solution in Ruby:\\n\\n```\\nlang/re_parser/time_parser.rb\\n   TIME_RE = %r{\\n   (?<digit>[0-9]){0}\\n   (?<h_ten>[0-1]){0}\\n   (?<m_ten>[0-6]){0}\\n   (?<ampm> am | pm){0}\\n   (?<hour> (\\\\g<h_ten> \\\\g<digit>) | \\\\g<digit>){0}\\n   (?<minute> \\\\g<m_ten> \\\\g<digit>){0}\\n   \\\\A(\\n    ( \\\\g<hour> \\\\g<ampm> )\\n    | ( \\\\g<hour> : \\\\g<minute> \\\\g<ampm> )\\n    | ( \\\\g<hour> : \\\\g<minute> )\\n   )\\\\Z\\n   }x\\n   def parse_time(string)\\n    result = TIME_RE.match(string)\\n    if result\\n    result[ :hour].to_i * 60 +\\n```\\n\\n```\\n (result[ :minute] || \"0\").to_i +\\n (result[ :ampm] == \"pm\" ? 12*60 : 0)\\n end\\nend\\n```\\n(This code uses the trick of defining named patterns at the start of the regular expression, and then referencing them as subpatterns in the actual match.)\\n\\n## **Answer 9** (from exercise 9)\\n\\nOur answer must be couched in several assumptions:\\n\\n- The storage device contains the information we need to be transferred.\\n- We know the speed at which the person walks.\\n- We know the distance between the machines.\\n- We are not accounting for the time it takes to transfer information to and from the storage device.\\n- The overhead of storing data is roughly equal to the overhead of sending it over a communications line.\\n\\n### **Answer 10** (from exercise 10)\\n\\nSubject to the caveats in the previous answer: A 1TB tape contains 8×240, or 2 43 bits, so a 1Gbps line would have to pump data for about 9,000 seconds, or roughly 2½ hours, to transfer the equivalent amount of information. If the person is walking at a constant 3½ mph, then our two machines would need to be almost 9 miles apart for the communications line to outperform our courier. Otherwise, the person wins.\\n\\n### **Answer 14** (from exercise 14)\\n\\nWe\\'ll show the function signatures in Java, with the pre- and postconditions in comments.\\n\\nFirst, the invariant for the class:\\n\\n*/** * @invariant getSpeed() > 0*\\n\\n```\\n * implies isFull() // Don\\'t run empty\\n *\\n * @invariant getSpeed() >= 0 &&\\n * getSpeed() < 10 // Range check\\n */\\n```\\nNext, the pre- and postconditions:\\n\\n```\\n/**\\n * @pre Math.abs(getSpeed() - x) <= 1 // Only change by one\\n * @pre x >= 0 && x < 10 // Range check\\n * @post getSpeed() == x // Honor requested speed\\n */\\npublic void setSpeed( final int x)\\n/**\\n * @pre !isFull() // Don\\'t fill it twice\\n * @post isFull() // Ensure it was done\\n */\\nvoid fill()\\n/**\\n * @pre isFull() // Don\\'t empty it twice\\n * @post !isFull() // Ensure it was done\\n */\\nvoid empty()\\n```\\n\\n```\\nAnswer 15 (from exercise 15)\\n```\\nThere are 21 terms in the series. If you said 20, you just experienced a fencepost error (not knowing whether to count the fenceposts or the spaces between them).\\n\\n### **Answer 16** (from exercise 16)\\n\\n- September, 1752 had only 19 days. This was done to synchronize calendars as part of the Gregorian Reformation.\\n- The directory could have been removed by another process, you might not have permission to read it, the drive might not be mounted, …; you\\n\\nget the picture.\\n\\n- We sneakily didn\\'t specify the types of a and b. Operator overloading might have defined +, =, or != to have unexpected behavior. Also, a and b may be aliases for the same variable, so the second assignment will overwrite the value stored in the first. Also, if the program is concurrent and badly written, a might have been updated by the time the addition takes place.\\n- In non-Euclidean geometry, the sum of the angles of a triangle will not add up to 180°. Think of a triangle mapped on the surface of a sphere.\\n- Leap minutes may have 61 or 62 seconds.\\n- Depending on the language, numeric overflow may leave the result of a+1 negative.\\n\\n## **Answer 17** (from exercise 17)\\n\\nIn most C and C++ implementations, there is no way of checking that a pointer actually points to valid memory. A common mistake is to deallocate a block of memory and reference that memory later in the program. By then, the memory pointed to may well have been reallocated to some other purpose. By setting the pointer to NULL, the programmers hope to prevent these rogue references—in most cases, dereferencing a NULL pointer will generate a runtime error.\\n\\n### **Answer 18** (from exercise 18)\\n\\nBy setting the reference to NULL, you reduce the number of pointers to the referenced object by one. Once this count reaches zero, the object is eligible for garbage collection. Setting the references to NULL can be significant for long-running programs, where the programmers need to ensure that memory utilization doesn\\'t increase over time.\\n\\n#### **Answer 19** (from exercise 19)\\n\\nA simple implementation could be:\\n\\n```\\nevent/strings_ex_1.rb\\n   class FSM\\n    def initialize(transitions, initial_state)\\n    @transitions = transitions\\n    @state = initial_state\\n    end\\n    def accept(event)\\n    @state, action = TRANSITIONS[@state][event] || TRANSITIONS[@state][ \\n   :default]\\n    end\\n   end\\n```\\n(Download this file to get the updated code that uses this new FSM class.)\\n\\n**Answer 20** (from exercise 20)\\n\\n- …three *network interface down* events within five minutes\\nThis *could* be implemented using a state machine, but it would be trickier than it might first appear: if you get events at minutes 1, 4, 7, and 8, then you should trigger the warning on the fourth event, which means the state machine needs to be able to handle reseting itself.\\n\\nFor this reason, event streams would seem to be the technology of choice. There\\'s a reactive function named buffer with size and offset parameters that would let you return each group of three incoming events. You could then look at the timestamps of the first and last event in a group to determine if the alarm should be triggered.\\n\\n- …after sunset, and there is motion detected at the bottom of the stairs followed by motion detected at the top of the stairs…\\nThis could probably be implemented using a combination of pubsub and state machines. You could use pubsub to disseminate events to any number of state machines, and then have the state machines determine what to do.\\n\\n- …notify various reporting systems that an order was completed.\\nThis is probably best handled using pubsub. You might want to use streams, but that would require that the systems being notified were also stream based.\\n\\n- …three backend services and wait for the responses.\\nThis is similar to our example that used streams to fetch user data.\\n\\n**Answer 21** (from exercise 21)\\n\\n- 1. Shipping and sales tax are added to an order:\\nbasic order → finalized order\\n\\nIn conventional code, it\\'s likely you\\'d have a function that calculated shipping costs and another that calculated tax. But we\\'re thinking about transformations here, so we transform an order with just items into a new kind of thing: an order that can be shipped.\\n\\n- 2. Your application loads configuration information from a named file:\\nfile name → configuration structure\\n\\n- 3. Someone logs in to a web application:\\nuser credentials → session\\n\\n**Answer 22** (from exercise 22)\\n\\nThe high-level transformation:\\n\\n```\\nfield contents as string\\n → [validate & convert]\\n → {:ok, value} | {:error, reason}\\n```\\ncould be broken down into:\\n\\n```\\nfield contents as string\\n → [convert string to integer]\\n → [check value >= 18]\\n → [check value <= 150]\\n → {:ok, value} | {:error, reason}\\n```\\nThis assumes that you have an error-handling pipeline.\\n\\n#### **Answer 23** (from exercise 23)\\n\\nLet\\'s answer the second part first: we prefer the first piece of code.\\n\\nIn the second chunk of code, each step returns an object that implements the next function we call: the object returned by content_of must implement find_matching_lines, and so on.\\n\\nThis means that the object returned by content_of is coupled to our code. Imagine the requirement changed, and we have to ignore lines starting with a # character. In the transformation style, that would be easy:\\n\\n| const content | = File.read(file_name); |\\n| --- | --- |\\n| const no_comments = remove_comments(content) |  |\\n| const lines | = find_matching_lines(no_comments, pattern) |\\n| const result | = truncate_lines(lines) |\\n\\nWe could even swap the order of remove_comments and find_matching_lines and it would still work.\\n\\nBut in the chained style, this would be more difficult. Where should our remove_comments method live: in the object returned by content_of or the object returned by find_matching_lines? And what other code will we break if we change that object? This coupling is why the method chaining style is sometimes called a *train wreck*.\\n\\n#### **Answer 24** (from exercise 24)\\n\\n#### *Image processing.*\\n\\nFor simple scheduling of a workload among the parallel processes, a shared work queue may be more than adequate. You might want to consider a blackboard system if there is feedback involved—that is, if the results of one processed chunk affect other chunks, as in machine vision applications, or complex 3D image-warp transforms.\\n\\n#### *Group calendaring*\\n\\nThis might be a good fit. You can post scheduled meetings and availability to the blackboard. You have entities functioning autonomously, feedback from decisions is important, and participants may come and go.\\n\\nYou might want to consider partitioning this kind of blackboard system depending on who is searching: junior staff may care about only the immediate office, human resources may want only English-speaking offices worldwide, and the CEO may want the whole enchilada.\\n\\nThere is also some flexibility on data formats: we are free to ignore formats or languages we don\\'t understand. We have to understand different formats only for those offices that have meetings with each other, and we do not need to expose all participants to a full transitive closure of all possible formats. This reduces coupling to where it is necessary, and does not constrain us artificially.\\n\\n*Network monitoring tool*\\n\\nThis is very similar to the mortgage/loan application program. You\\'ve got trouble reports sent in by users and statistics reported automatically, all posting to the blackboard. A human or software agent can analyze the blackboard to diagnose network failures: two errors on a line might\\n\\njust be cosmic rays, but 20,000 errors and you\\'ve got a hardware problem. Just as the detectives solve the murder mystery, you can have multiple entities analyzing and contributing ideas to solve the network problems.\\n\\n### **Answer 25** (from exercise 25)\\n\\nThe assumption with a list of key-value pairs is generally that the key is unique, and hash libraries typically enforce that either by the behavior of the hash itself or with explicit error messages for duplicated keys. However, an array typically does not have those constraints, and will happily store duplicate keys unless you code it specifically not to. So in this case, the first key found that matches DepositAccount wins, and any remaining matching entries are ignored. The order of entries is not guaranteed, so sometimes it works and sometimes it doesn\\'t.\\n\\nAnd what about the difference in machines from development and production? It\\'s just a coincidence.\\n\\n### **Answer 26** (from exercise 26)\\n\\nThe fact that a purely numeric field works in the US, Canada, and the Caribbean is a coincidence. Per the ITU spec, international call format starts with a literal + sign. The * character is also used in some locales, and more commonly, leading zeros can be a part of the number. Never store a phone number in a numeric field.\\n\\n### **Answer 27** (from exercise 27)\\n\\nDepends on where you are. In the US, volume measures are based on the gallon, which is the volume of a cylinder 6 inches high and 7 inches in diameter, rounded to the nearest cubic inch.\\n\\nIn Canada, \"one cup\" in a recipe could mean any of\\n\\n- 1/5 of an imperial quart, or 227ml\\n- 1/4 of a US quart, or 236ml\\n- 16 metric tablespoons, or 240ml\\n- 1/4 of a liter, or 250ml\\n\\nUnless you\\'re talking about a rice cooker, in which case \"one cup\" is 180ml. That derives from the *koku*, which was the estimated volume of dry rice required to feed one person for one year: apparently, around 180L. Rice cooker cups are 1 *gō*, which is 1/1000 of a koku. So, roughly the amount of rice a person would eat at a single meal.[85]\\n\\n## **Answer 28** (from exercise 28)\\n\\nClearly, we can\\'t give any absolute answers to this exercise. However, we can give you a couple of pointers.\\n\\nIf you find that your results don\\'t follow a smooth curve, you might want to check to see if some other activity is using some of your processor\\'s power. You probably won\\'t get good figures if background processes periodically take cycles away from your programs. You might also want to check memory: if the application starts using swap space, performance will nose dive.\\n\\nHere\\'s a graph of the results of running the code on one of our machines:\\n\\n![](_page_441_Figure_0.jpeg)\\n\\n**Answer 29** (from exercise 29)\\n\\nThere are a couple of ways of getting there. One is to turn the problem on its head. If the array has just one element, we don\\'t iterate around the loop. Each additional iteration doubles the size of the array we can search. The general formula for the array size is therefore , where is the number of iterations. If you take logs to the base 2 of each side, you get , which by the definition of logs becomes .\\n\\n## **Answer 30** (from exercise 30)\\n\\nThis is probably too much of a flashback to secondary school math, but the formula for converting a logarithm in base to one in base is:\\n\\n$$\\\\begin{array}{r l r l r l}{{\\\\log}}&{{}_{A}^{x}}&{{}}&{{}}\\\\\\\\ {\\\\log}&{{}_{b}}&{{}}&{{}}&{{}}\\\\\\\\ {\\\\log}&{{}_{A}^{b}}&{{}}&{{}}&{{}}\\\\end{array}$$\\n\\nBecause is a constant, then we can ignore it inside a Big-O result.\\n\\n#### **Answer 31** (from exercise 31)\\n\\nOne property we can test is that an order succeeds if the warehouse has enough items on hand. We can generate orders for random quantities of items, and verify that an \"OK\" tuple is returned if the warehouse had stock.\\n\\n### **Answer 32** (from exercise 32)\\n\\nThis is a good use of property-based testing. The unit tests can focus on individual cases where you\\'ve worked out the result by some other means, and the property tests can focus on things like:\\n\\n- Do any two crates overlap?\\n- Does any part of any crate exceed the width or length of the truck?\\n- Is the packing density (area used by crates divided by the area of the truck bed) less than or equal to 1?\\n- If it\\'s part of the requirement, does the packing density exceed the minimum acceptable density?\\n\\n### **Answer 33** (from exercise 33)\\n\\n- 1. This statement sounds like a real requirement: there may be constraints placed on the application by its environment.\\n- 2. On its own, this statement isn\\'t really a requirement. But to find out what\\'s *really* required, you have to ask the magic question, \"Why?\"\\n\\nIt may be that this is a corporate standard, in which case the actual requirement should be something like \"all UI elements must conform to the MegaCorp User Interface Standards, V12.76.\"\\n\\nIt may be that this is a color that the design team happen to like. In that case, you should think about the way the design team also likes to change their minds, and phrase the requirement as \"the background color of all modal windows must be configurable. As shipped, the color will be gray.\" Even better would be the broader statement \"All visual elements of the application (colors, fonts, and languages) must be configurable.\"\\n\\nOr it may simply mean that the user needs to be able to distinguish modal and nonmodal windows. If that\\'s the case, some more discussions are needed.\\n\\n- 3. This statement is not a requirement, it\\'s architecture. When faced with something like this, you have to dig deep to find out what the user is thinking. Is this a scaling issue? Or performance? Cost? Security? The answers will inform your design.\\n- 4. The underlying requirement is probably something closer to \"The system will prevent the user from making invalid entries in fields, and will warn the user when these entries are made.\\'\\'\\n- 5. This statement is probably a hard requirement, based on some hardware limitation.\\n\\n#### And here\\'s a solution to the four-dots problem:\\n\\n![](_page_444_Figure_1.jpeg)\\n\\n#### **Footnotes**\\n\\n- [85] Thanks for this bit of trivia goes to Avi Bryant (@avibryant)\\nCopyright © 2020 Pearson Education, Inc.\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rendered.markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_page_0_Picture_0.jpeg': <PIL.Image.Image image mode=RGB size=1307x205>,\n",
       " '_page_0_Picture_4.jpeg': <PIL.Image.Image image mode=RGB size=119x128>,\n",
       " '_page_13_Picture_0.jpeg': <PIL.Image.Image image mode=RGB size=38x45>,\n",
       " '_page_68_Picture_4.jpeg': <PIL.Image.Image image mode=RGB size=124x111>,\n",
       " '_page_72_Picture_4.jpeg': <PIL.Image.Image image mode=RGB size=733x59>,\n",
       " '_page_83_Figure_4.jpeg': <PIL.Image.Image image mode=RGB size=496x282>,\n",
       " '_page_87_Figure_0.jpeg': <PIL.Image.Image image mode=RGB size=863x494>,\n",
       " '_page_101_Figure_1.jpeg': <PIL.Image.Image image mode=RGB size=316x34>,\n",
       " '_page_101_Figure_2.jpeg': <PIL.Image.Image image mode=RGB size=704x518>,\n",
       " '_page_155_Figure_3.jpeg': <PIL.Image.Image image mode=RGB size=1208x132>,\n",
       " '_page_175_Picture_7.jpeg': <PIL.Image.Image image mode=RGB size=1217x141>,\n",
       " '_page_186_Picture_8.jpeg': <PIL.Image.Image image mode=RGB size=94x45>,\n",
       " '_page_194_Picture_5.jpeg': <PIL.Image.Image image mode=RGB size=1230x128>,\n",
       " '_page_207_Picture_7.jpeg': <PIL.Image.Image image mode=RGB size=765x340>,\n",
       " '_page_207_Picture_10.jpeg': <PIL.Image.Image image mode=RGB size=752x227>,\n",
       " '_page_210_Picture_3.jpeg': <PIL.Image.Image image mode=RGB size=589x75>,\n",
       " '_page_219_Figure_0.jpeg': <PIL.Image.Image image mode=RGB size=872x331>,\n",
       " '_page_220_Figure_5.jpeg': <PIL.Image.Image image mode=RGB size=991x373>,\n",
       " '_page_232_Figure_0.jpeg': <PIL.Image.Image image mode=RGB size=1228x650>,\n",
       " '_page_246_Figure_13.jpeg': <PIL.Image.Image image mode=RGB size=345x247>,\n",
       " '_page_247_Figure_16.jpeg': <PIL.Image.Image image mode=RGB size=368x453>,\n",
       " '_page_249_Picture_2.jpeg': <PIL.Image.Image image mode=RGB size=1221x102>,\n",
       " '_page_250_Picture_13.jpeg': <PIL.Image.Image image mode=RGB size=61x29>,\n",
       " '_page_254_Picture_6.jpeg': <PIL.Image.Image image mode=RGB size=447x524>,\n",
       " '_page_262_Figure_0.jpeg': <PIL.Image.Image image mode=RGB size=868x916>,\n",
       " '_page_267_Figure_0.jpeg': <PIL.Image.Image image mode=RGB size=990x494>,\n",
       " '_page_269_Picture_6.jpeg': <PIL.Image.Image image mode=RGB size=801x314>,\n",
       " '_page_284_Figure_0.jpeg': <PIL.Image.Image image mode=RGB size=1214x916>,\n",
       " '_page_295_Picture_6.jpeg': <PIL.Image.Image image mode=RGB size=106x49>,\n",
       " '_page_303_Picture_4.jpeg': <PIL.Image.Image image mode=RGB size=100x45>,\n",
       " '_page_310_Figure_0.jpeg': <PIL.Image.Image image mode=RGB size=1219x1228>,\n",
       " '_page_322_Picture_7.jpeg': <PIL.Image.Image image mode=RGB size=108x57>,\n",
       " '_page_353_Picture_1.jpeg': <PIL.Image.Image image mode=RGB size=594x305>,\n",
       " '_page_368_Picture_2.jpeg': <PIL.Image.Image image mode=RGB size=553x54>,\n",
       " '_page_375_Picture_3.jpeg': <PIL.Image.Image image mode=RGB size=203x165>,\n",
       " '_page_375_Picture_6.jpeg': <PIL.Image.Image image mode=RGB size=1219x120>,\n",
       " '_page_401_Figure_1.jpeg': <PIL.Image.Image image mode=RGB size=1101x557>,\n",
       " '_page_401_Figure_3.jpeg': <PIL.Image.Image image mode=RGB size=1221x149>,\n",
       " '_page_407_Picture_7.jpeg': <PIL.Image.Image image mode=RGB size=1209x127>,\n",
       " '_page_409_Picture_4.jpeg': <PIL.Image.Image image mode=RGB size=1082x99>,\n",
       " '_page_415_Picture_6.jpeg': <PIL.Image.Image image mode=RGB size=571x91>,\n",
       " '_page_420_Picture_7.jpeg': <PIL.Image.Image image mode=RGB size=1224x110>,\n",
       " '_page_421_Picture_1.jpeg': <PIL.Image.Image image mode=RGB size=1238x131>,\n",
       " '_page_422_Picture_4.jpeg': <PIL.Image.Image image mode=RGB size=1222x270>,\n",
       " '_page_441_Figure_0.jpeg': <PIL.Image.Image image mode=RGB size=826x682>,\n",
       " '_page_444_Figure_1.jpeg': <PIL.Image.Image image mode=RGB size=602x343>}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rendered.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Header 1': '2 Background'}, page_content='The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.  \\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].  \\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].  \\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "with open(\n",
    "    \"../docs/attention-is-all-you-need/attention-is-all-you-need.md\",\n",
    "    \"r\",\n",
    "    encoding=\"utf-8\",\n",
    ") as file:\n",
    "    content = file.read()\n",
    "\n",
    "splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "        (\"#####\", \"Header 5\"),\n",
    "        (\"######\", \"Header 6\"),\n",
    "    ]\n",
    ")\n",
    "chunks = splitter.split_text(content)\n",
    "chunks[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Título: Attention Is All You Need\n",
      "Contenido: \n",
      "Ashish Vaswani∗ Google Brain avaswani@google.com\n",
      "> Llion Jones∗ Google Research llion@google.com\n",
      "Google Brain noam@google.com Aidan N. Gomez∗ †\n",
      "University of Toronto aidan@cs.toronto.edu\n",
      "Noam Shazeer∗\n",
      "Niki Parmar∗ Google Research nikip@google.com\n",
      "Jakob Uszkoreit∗ Google Research usz@google.com\n",
      "Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com\n",
      "Nivel: 1\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: Abstract\n",
      "Contenido: \n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "<sup>∗</sup>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n",
      "<sup>†</sup>Work performed while at Google Brain.\n",
      "<sup>‡</sup>Work performed while at Google Research.\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 1 Introduction\n",
      "Contenido: \n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "Nivel: 2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 2 Background\n",
      "Contenido: \n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "Nivel: 1\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 3 Model Architecture\n",
      "Contenido: \n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\n",
      "![](_page_2_Figure_0.jpeg)\n",
      "Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 3.1 Encoder and Decoder Stacks\n",
      "Contenido: \n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\n",
      "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 3.2 Attention\n",
      "Contenido: \n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "![](_page_3_Figure_0.jpeg)\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 3.2.1 Scaled Dot-Product Attention\n",
      "Contenido: \n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by √ dk, and apply a softmax function to obtain the weights on the values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\n",
      "Attention($Q,K,V$) = softmax($\\frac{QK^{T}}{\\sqrt{d_{k}}}$)$V$ (1)\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of √ 1 dk . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4 . To counteract this effect, we scale the dot products by √ 1 dk .\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 3.2.2 Multi-Head Attention\n",
      "Contenido: \n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "<sup>4</sup>To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q · k = Pdk i=1 qiki, has mean 0 and variance dk.\n",
      "output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "MultiHead$(Q,K,V)=$ Concat$(\\text{head}_{1},...,\\text{head}_{\\text{h}})W^{O}$  \n",
      "  \n",
      "where head${}_{\\text{i}}=$ Attention$(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$\n",
      "Where the projections are parameter matrices W Q i ∈ R dmodel×dk , W K i ∈ R dmodel×dk , WV i ∈ R dmodel×dv and WO ∈ R hdv×dmodel .\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 3.2.3 Applications of Attention in our Model\n",
      "Contenido: \n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\n",
      "- The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 3.3 Position-wise Feed-Forward Networks\n",
      "Contenido: \n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
      "$${\\rm FFN}(x)=\\max(0,xW_{1}+b_{1})W_{2}+b_{2}\\tag{2}$$\n",
      "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality df f = 2048.\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 3.4 Embeddings and Softmax\n",
      "Contenido: \n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √ dmodel.\n",
      "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\n",
      "| Layer Type | Complexity per Layer |  | Sequential | Maximum Path Length |\n",
      "| --- | --- | --- | --- | --- |\n",
      "|  |  |  | Operations |  |\n",
      "| Self-Attention | 2 O(n · d) |  | O(1) | O(1) |\n",
      "| Recurrent | O(n · d | 2 ) | O(n) | O(n) |\n",
      "| Convolutional | O(k · n · d | 2 ) | O(1) | O(logk(n)) |\n",
      "| Self-Attention (restricted) | O(r · n · d) |  | O(1) | O(n/r) |\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 3.5 Positional Encoding\n",
      "Contenido: \n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "$$\\begin{array}{c}{{P E_{(p o s,2i)}=s i n(p o s/10000^{2i/d_{\\mathrm{model}}})}}\\\\ {{P E_{(p o s,2i+1)}=c o s(p o s/10000^{2i/d_{\\mathrm{model}}})}}\\end{array}$$\n",
      "where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of P Epos.\n",
      "We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 4 Why Self-Attention\n",
      "Contenido: \n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi , zi ∈ R d , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
      "The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n",
      "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.\n",
      "A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d 2 ). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\n",
      "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 5 Training\n",
      "Contenido: \n",
      "This section describes the training regime for our models.\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 5.1 Training Data and Batching\n",
      "Contenido: \n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 5.2 Hardware and Schedule\n",
      "Contenido: \n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 5.3 Optimizer\n",
      "Contenido: \n",
      "We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9 . We varied the learning rate over the course of training, according to the formula:\n",
      "$$lrate=d_{\\text{model}}^{-0.5}\\cdot\\min(step\\_num^{-0.5},step\\_num\\cdot warmup\\_steps^{-1.5})\\tag{3}$$\n",
      "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 5.4 Regularization\n",
      "Contenido: \n",
      "We employ three types of regularization during training:\n",
      "| Model | BLEU |  | Training Cost (FLOPs) |  |\n",
      "| --- | --- | --- | --- | --- |\n",
      "|  | EN-DE | EN-FR | EN-DE | EN-FR |\n",
      "| ByteNet [18] | 23.75 |  |  |  |\n",
      "| Deep-Att + PosUnk [39] |  | 39.2 |  | 1.0 · 1020 |\n",
      "| GNMT + RL [38] | 24.6 | 39.92 | 2.3 · 1019 | 1.4 · 1020 |\n",
      "| ConvS2S [9] | 25.16 | 40.46 | 9.6 · 1018 | 1.5 · 1020 |\n",
      "| MoE [32] | 26.03 | 40.56 | 2.0 · 1019 | 1.2 · 1020 |\n",
      "| Deep-Att + PosUnk Ensemble [39] |  | 40.4 |  | 8.0 · 1020 |\n",
      "| GNMT + RL Ensemble [38] | 26.30 | 41.16 | 1.8 · 1020 | 1.1 · 1021 |\n",
      "| ConvS2S Ensemble [9] | 26.36 | 41.29 | 7.7 · 1019 | 1.2 · 1021 |\n",
      "| Transformer (base model) | 27.3 | 38.1 | 3.3 · 1018 |  |\n",
      "| Transformer (big) | 28.4 | 41.8 | 2.3 · 1019 |  |\n",
      "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\n",
      "Label Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 6 Results\n",
      "Contenido: \n",
      "\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 6.1 Machine Translation\n",
      "Contenido: \n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38].\n",
      "Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5 .\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 6.2 Model Variations\n",
      "Contenido: \n",
      "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\n",
      "<sup>5</sup>We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
      "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n",
      "|  | N | dmodel | dff | h | dk | dv | Pdrop | ϵls | train | PPL | BLEU | params ×106 |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "|  |  |  |  |  |  |  |  |  | steps | (dev) | (dev) |  |\n",
      "| base | 6 | 512 | 2048 | 8 | 64 | 64 | 0.1 | 0.1 | 100K | 4.92 | 25.8 | 65 |\n",
      "| (A) |  |  |  | 1 | 512 | 512 |  |  |  | 5.29 | 24.9 |  |\n",
      "|  |  |  |  | 4 | 128 | 128 |  |  |  | 5.00 | 25.5 |  |\n",
      "|  |  |  |  | 16 | 32 | 32 |  |  |  | 4.91 | 25.8 |  |\n",
      "|  |  |  |  | 32 | 16 | 16 |  |  |  | 5.01 | 25.4 |  |\n",
      "| (B) |  |  |  |  | 16 32 |  |  |  |  | 5.16 5.01 | 25.1 25.4 | 58 60 |\n",
      "| (C) | 2 |  |  |  |  |  |  |  |  | 6.11 | 23.7 | 36 |\n",
      "|  | 4 |  |  |  |  |  |  |  |  | 5.19 | 25.3 | 50 |\n",
      "|  | 8 |  |  |  |  |  |  |  |  | 4.88 | 25.5 | 80 |\n",
      "|  |  | 256 |  |  | 32 | 32 |  |  |  | 5.75 | 24.5 | 28 |\n",
      "|  |  | 1024 |  |  | 128 | 128 |  |  |  | 4.66 | 26.0 | 168 |\n",
      "|  |  |  | 1024 |  |  |  |  |  |  | 5.12 | 25.4 | 53 |\n",
      "|  |  |  | 4096 |  |  |  |  |  |  | 4.75 | 26.2 | 90 |\n",
      "| (D) |  |  |  |  |  |  | 0.0 |  |  | 5.77 | 24.6 |  |\n",
      "|  |  |  |  |  |  |  | 0.2 |  |  | 4.95 | 25.5 |  |\n",
      "|  |  |  |  |  |  |  |  | 0.0 |  | 4.67 | 25.3 |  |\n",
      "|  |  |  |  |  |  |  |  | 0.2 |  | 5.47 | 25.7 |  |\n",
      "| (E) |  |  |  |  | positional embedding instead of sinusoids |  |  |  |  | 4.92 | 25.7 |  |\n",
      "| big | 6 | 1024 | 4096 | 16 |  |  | 0.3 |  | 300K | 4.33 | 26.4 | 213 |\n",
      "development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 6.3 English Constituency Parsing\n",
      "Contenido: \n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
      "We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\n",
      "| Parser | Training | WSJ 23 F1 |\n",
      "| --- | --- | --- |\n",
      "| Vinyals & Kaiser el al. (2014) [37] | WSJ only, discriminative | 88.3 |\n",
      "| Petrov et al. (2006) [29] | WSJ only, discriminative | 90.4 |\n",
      "| Zhu et al. (2013) [40] | WSJ only, discriminative | 90.4 |\n",
      "| Dyer et al. (2016) [8] | WSJ only, discriminative | 91.7 |\n",
      "| Transformer (4 layers) | WSJ only, discriminative | 91.3 |\n",
      "| Zhu et al. (2013) [40] | semi-supervised | 91.3 |\n",
      "| Huang & Harper (2009) [14] | semi-supervised | 91.3 |\n",
      "| McClosky et al. (2006) [26] | semi-supervised | 92.1 |\n",
      "| Vinyals & Kaiser el al. (2014) [37] | semi-supervised | 92.1 |\n",
      "| Transformer (4 layers) | semi-supervised | 92.7 |\n",
      "| Luong et al. (2015) [23] | multi-task | 93.0 |\n",
      "| Dyer et al. (2016) [8] | generative | 93.3 |\n",
      "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\n",
      "increased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3 for both WSJ only and the semi-supervised setting.\n",
      "Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\n",
      "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-Parser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: 7 Conclusion\n",
      "Contenido: \n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: References\n",
      "Contenido: \n",
      "- [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. *arXiv preprint arXiv:1607.06450*, 2016.\n",
      "- [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. *CoRR*, abs/1409.0473, 2014.\n",
      "- [3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. *CoRR*, abs/1703.03906, 2017.\n",
      "- [4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. *arXiv preprint arXiv:1601.06733*, 2016.\n",
      "- [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. *CoRR*, abs/1406.1078, 2014.\n",
      "- [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. *arXiv preprint arXiv:1610.02357*, 2016.\n",
      "- [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. *CoRR*, abs/1412.3555, 2014.\n",
      "- [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In *Proc. of NAACL*, 2016.\n",
      "- [9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. *arXiv preprint arXiv:1705.03122v2*, 2017.\n",
      "- [10] Alex Graves. Generating sequences with recurrent neural networks. *arXiv preprint arXiv:1308.0850*, 2013.\n",
      "- [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 770–778, 2016.\n",
      "- [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
      "- [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. *Neural computation*, 9(8):1735–1780, 1997.\n",
      "- [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In *Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing*, pages 832–841. ACL, August 2009.\n",
      "- [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. *arXiv preprint arXiv:1602.02410*, 2016.\n",
      "- [16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In *Advances in Neural Information Processing Systems, (NIPS)*, 2016.\n",
      "- [17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In *International Conference on Learning Representations (ICLR)*, 2016.\n",
      "- [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. *arXiv preprint arXiv:1610.10099v2*, 2017.\n",
      "- [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In *International Conference on Learning Representations*, 2017.\n",
      "- [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In *ICLR*, 2015.\n",
      "- [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. *arXiv preprint arXiv:1703.10722*, 2017.\n",
      "- [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. *arXiv preprint arXiv:1703.03130*, 2017.\n",
      "- [23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. *arXiv preprint arXiv:1511.06114*, 2015.\n",
      "- [24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. *arXiv preprint arXiv:1508.04025*, 2015.\n",
      "- [25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. *Computational linguistics*, 19(2):313–330, 1993.\n",
      "- [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In *Proceedings of the Human Language Technology Conference of the NAACL, Main Conference*, pages 152–159. ACL, June 2006.\n",
      "- [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In *Empirical Methods in Natural Language Processing*, 2016.\n",
      "- [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. *arXiv preprint arXiv:1705.04304*, 2017.\n",
      "- [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In *Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL*, pages 433–440. ACL, July 2006.\n",
      "- [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. *arXiv preprint arXiv:1608.05859*, 2016.\n",
      "- [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. *arXiv preprint arXiv:1508.07909*, 2015.\n",
      "- [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. *arXiv preprint arXiv:1701.06538*, 2017.\n",
      "- [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. *Journal of Machine Learning Research*, 15(1):1929–1958, 2014.\n",
      "- [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, *Advances in Neural Information Processing Systems 28*, pages 2440–2448. Curran Associates, Inc., 2015.\n",
      "- [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In *Advances in Neural Information Processing Systems*, pages 3104–3112, 2014.\n",
      "- [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. *CoRR*, abs/1512.00567, 2015.\n",
      "- [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In *Advances in Neural Information Processing Systems*, 2015.\n",
      "- [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. *arXiv preprint arXiv:1609.08144*, 2016.\n",
      "- [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. *CoRR*, abs/1606.04199, 2016.\n",
      "- [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In *Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers)*, pages 434–443. ACL, August 2013.\n",
      "Nivel: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Título: Attention Visualizations **Input-Input Layer5**\n",
      "Contenido: \n",
      "![](_page_12_Figure_1.jpeg)\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'. Attentions here shown only for the word 'making'. Different colors represent different heads. Best viewed in color.\n",
      "![](_page_13_Figure_0.jpeg)\n",
      "**Input-Input Layer5**\n",
      "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word 'its' for attention heads 5 and 6. Note that the attentions are very sharp for this word.\n",
      "![](_page_14_Figure_0.jpeg)\n",
      "**Input-Input Layer5**\n",
      "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "Nivel: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def parse_markdown(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # regex for headers `#`\n",
    "    section_pattern = r\"^(#+)\\s*(.*?)\\s*(?=\\n(?!#)|$)\"\n",
    "    sections = []\n",
    "\n",
    "    # Buscar todas las secciones\n",
    "    matches = list(re.finditer(section_pattern, content, re.MULTILINE))\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        # Obtener nivel, título y la posición en el texto\n",
    "        level = len(match.group(1))  # Nivel basado en el número de '#'\n",
    "        title = match.group(2).strip()\n",
    "\n",
    "        # Determinar el inicio y fin del contenido\n",
    "        start_idx = match.end()\n",
    "\n",
    "        # Verificar si es la última sección\n",
    "        if i + 1 < len(matches):\n",
    "            # El contenido va desde el final de este encabezado hasta el próximo encabezado\n",
    "            next_start_idx = matches[i + 1].start()\n",
    "            content_section = content[start_idx:next_start_idx].strip()\n",
    "        else:\n",
    "            # Si es la última sección, tomamos todo hasta el final del archivo\n",
    "            content_section = content[start_idx:].strip()\n",
    "\n",
    "        # Almacenar la sección como un diccionario\n",
    "        sections.append({\"title\": title, \"content\": content_section, \"level\": level})\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "filename = \"../docs/attention-is-all-you-need/attention-is-all-you-need.md\"\n",
    "secciones = parse_markdown(filename)\n",
    "\n",
    "# Mostrar las secciones\n",
    "for seccion in secciones:\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"Título: {seccion['title']}\")\n",
    "    print(f\"Contenido: \\n{seccion['content'].replace('\\n\\n', '\\n')}\")\n",
    "    print(f\"Nivel: {seccion['level']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
