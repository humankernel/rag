{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by retrieving a set of candidate documents based on a query. This can be done using traditional methods like TF-IDF or BM25 for keyword-based retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using BM25 for Initial Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"Dogs are great companions.\",\n",
    "    \"The sun is shining today.\",\n",
    "    \"Cats and dogs are popular pets.\",\n",
    "    \"It is a beautiful day.\",\n",
    "    \"Pets provide emotional support.\"\n",
    "]\n",
    "\n",
    "# Tokenize documents\n",
    "tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "# Function to retrieve top K documents\n",
    "def retrieve_documents(query, k=3):\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_indices = scores.argsort()[-k:][::-1]  # Get indices of top K scores\n",
    "    return [documents[i] for i in top_indices]\n",
    "\n",
    "query = \"What pets are popular?\"\n",
    "initial_retrieval = retrieve_documents(query)\n",
    "print(\"Initial Retrieval:\", initial_retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Semantic Scoring with a Simple Model\n",
    "\n",
    "Next, implement a simple scoring mechanism to rerank the retrieved documents. You can use cosine similarity between embeddings of the query and documents.\n",
    "Example: Using Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load a pre-trained model for embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to compute semantic scores\n",
    "def semantic_score(query, documents):\n",
    "    # Generate embeddings for the query and documents\n",
    "    query_embedding = model.encode([query])\n",
    "    doc_embeddings = model.encode(documents)\n",
    "    \n",
    "    # Calculate cosine similarity scores\n",
    "    scores = cosine_similarity(query_embedding, doc_embeddings)\n",
    "    return scores.flatten()\n",
    "\n",
    "# Rerank documents based on semantic scores\n",
    "def rerank_documents(query, retrieved_docs):\n",
    "    semantic_scores = semantic_score(query, retrieved_docs)\n",
    "    ranked_docs = sorted(zip(retrieved_docs, semantic_scores), key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, score in ranked_docs]\n",
    "\n",
    "reranked_results = rerank_documents(query, initial_retrieval)\n",
    "print(\"Reranked Results:\", reranked_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Combine Scores (Optional)\n",
    "\n",
    "You can further enhance the reranking by combining keyword-based scores with semantic scores. This can be done by normalizing and weighting both scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_rerank(query, retrieved_docs):\n",
    "    bm25_scores = bm25.get_scores(query.lower().split())\n",
    "    semantic_scores = semantic_score(query, retrieved_docs)\n",
    "\n",
    "    combined_scores = [(bm25_scores[i] + semantic_scores[i]) / 2 for i in range(len(retrieved_docs))]\n",
    "    \n",
    "    ranked_docs = sorted(zip(retrieved_docs, combined_scores), key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, score in ranked_docs]\n",
    "\n",
    "final_results = combined_rerank(query, initial_retrieval)\n",
    "print(\"Final Combined Reranked Results:\", final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "This implementation provides a straightforward way to create a reranking system for RAG applications using basic libraries. The process involves:\n",
    "Initial Retrieval: Using BM25 to fetch relevant documents.\n",
    "Semantic Scoring: Utilizing sentence embeddings to compute relevance.\n",
    "Reranking: Sorting documents based on their semantic scores.\n",
    "Optional Combination: Merging keyword and semantic scores for improved ranking.\n",
    "This approach allows you to build an effective reranking mechanism tailored to your specific needs without heavy dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of Cross-Encoders in Sentence Transformers\n",
    "Cross-Encoders are a type of model used in the Sentence Transformers framework, specifically designed for scoring and classifying pairs of sentences. They differ fundamentally from Bi-Encoders, which are more efficient for certain applications.\n",
    "Key Differences: Cross-Encoder vs. Bi-Encoder\n",
    "Input Handling:\n",
    "Cross-Encoders process two sentences simultaneously, concatenating them with a special separator token (e.g., <SEP>). This allows them to evaluate the relationship between the sentences directly.\n",
    "Bi-Encoders, on the other hand, encode each sentence independently into embeddings, which can then be compared using methods like cosine similarity.\n",
    "Output:\n",
    "Cross-Encoders produce a score indicating the similarity between the two sentences (ranging from 0 to 1) but do not generate standalone embeddings for individual sentences.\n",
    "Bi-Encoders generate embeddings that can be used for various tasks like clustering or semantic search.\n",
    "Performance and Scalability:\n",
    "Cross-Encoders typically achieve higher accuracy in scoring and classification tasks due to their ability to consider both sentences together.\n",
    "However, they are less scalable for large datasets because they require computing scores for all possible pairs of sentences, which can be computationally expensive. For example, comparing 100,000 sentences would require processing nearly 5 billion pairs with a Cross-Encoder, whereas a Bi-Encoder would only need to encode the 100,000 sentences once134.\n",
    "Use Cases for Cross-Encoders\n",
    "Cross-Encoders are particularly useful when:\n",
    "You have a predefined set of sentence pairs and need to evaluate their similarity.\n",
    "Tasks require high accuracy in classification or ranking, such as:\n",
    "Natural Language Inference (NLI)\n",
    "Semantic Textual Similarity (STS)\n",
    "In practice, Cross-Encoders are often combined with Bi-Encoders in applications like Information Retrieval. A typical approach is to first use a Bi-Encoder to retrieve a smaller set of candidate sentences and then apply a Cross-Encoder to re-rank these candidates for better accuracy13.\n",
    "Implementing Cross-Encoders\n",
    "Using a Cross-Encoder is straightforward. Hereâ€™s an example implementation:\n",
    "python\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load a pre-trained Cross-Encoder model\n",
    "model = CrossEncoder(\"cross-encoder/ms-marco-TinyBERT-L-2-v2\")\n",
    "\n",
    "# Define sentence pairs\n",
    "sentence_pairs = [\n",
    "    [\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants.\"],\n",
    "    [\"What is the capital of France?\", \"Paris is the capital city of France.\"]\n",
    "]\n",
    "\n",
    "# Predict similarity scores\n",
    "scores = model.predict(sentence_pairs)\n",
    "print(scores)  # Output: array of similarity scores\n",
    "\n",
    "This code snippet demonstrates how to load a Cross-Encoder model and use it to predict similarity scores for predefined sentence pairs145.\n",
    "Conclusion\n",
    "Cross-Encoders offer a powerful method for evaluating sentence pairs with high accuracy but come with scalability challenges. They are best suited for tasks where precision is critical and where the number of sentence pairs is manageable. For broader applications requiring efficiency and speed, Bi-Encoders remain the preferred choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
