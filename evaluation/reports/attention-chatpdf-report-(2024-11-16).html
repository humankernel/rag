<!DOCTYPE html>


<style>
    body{
  background: #18181B;
}

.main{
  font-family: "Noto Sans", ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
  color: #FDFDFD;
}

h1 {
  font-size: 2.5rem;
  color: white;
}

h3 {
  font-size: 1.5rem;
  background: #0c087c;
  padding: 10px;
  margin: 0px;
  border: 1px solid #6b7280;}

.extended-title{
  width:100%;
}

#gsk-overview{
  display:flex;
}

h4 {
  font-size: 1rem;
  background: #27272A;
  padding: 10px;
  margin: 0px;
  border-bottom: 1px solid #6b7280;
}

h2 {
  font-size: 1.5rem;
  margin-top: 3px;
  color:#000000;
}

.header{
  display: flex;
  justify-content: center;
  align-items: center;
}
.header > * {
  margin-inline: 20px;
}

.flex-row {
  display: flex;
  flex-direction: row;
  padding:10px;
  border: 1px solid #27272A;
}

.flex-row>div {
  flex: auto;
  box-sizing: border-box;
  display: flex;
  flex-direction: column;
  justify-content: center;
  align-items: center;
}

progress[value] {
  --background: #6D6D6D;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  border: none;
  height: 4px;
  margin: 0 10px;
  border-radius: 10em;
  background: var(--background);
}

progress[value]::-webkit-progress-bar {
  border-radius: 10em;
  background: var(--background);
}

progress[value]::-webkit-progress-value {
  border-radius: 10em;
  background: var(--color);
}

progress[value]::-moz-progress-bar {
  border-radius: 10em;
  background: var(--color);
}

label {
  font-size: 20px;
  font-weight: bold;
  display: block;
  margin: 20px 0;
}

.tab {
  overflow: hidden;
  border: 1px solid #27272A;
  background-color: #27272A;
}

.tab button {
  background-color: inherit;
  float: left;
  border: none;
  outline: none;
  cursor: pointer;
  padding: 14px 16px;
  transition: 0.3s;
  color: #ffffff;
  font-size:1.2rem;
}

.tab div {
  background-color: inherit;
  float: left;
  border: none;
  outline: none;
  cursor: pointer;
  padding: 14px 16px;
  transition: 0.3s;
  color: #ffffff;
  font-size: 1.2rem;
}

.tab button:hover {
  background-color: #18181B;
}

.tab-title{
  font-size: 1.5rem;
  font-weight: bold;
  margin-bottom:-5px;
}

.tab button.active {
  background-color: #18181B;
  border-top: 1px solid #6b7280;
  border-bottom: 1px solid #18181B;
  border-left: 1px solid #6b7280;
  border-right: 1px solid #6b7280;
}

.tabcontent {
  display: none;
  padding: 6px 12px;
  background: #18181B;
  border: 1px solid #27272A;
  border-top: 1px solid #6b7280;
  margin-top: -2px;
}

#gsk-advice {
  display: flex;
  justify-content: center;
}

#gsk-metrics{
  width:100%;
}

#recommendation {
  margin-top: 20px;
  padding: 20px;
  border-radius: 10px;
  background-color: #e1ce86;
  color: #27272A;
  width:95%;
  box-shadow: 0 4px 8px 0 #000000, 0 6px 20px 0 #000000;
  font-size: 12pt;
}

.separator {
  margin: 20px 0;
}

.separator-border {
  margin: 20px 0;
  border-bottom: 1px solid #6b7280;
}

#gsk-rag{
  margin: 32px 28px;
  padding: 12px 24px;
  background-color: #111516;
}

.section-container {
  margin-bottom: 32px;
}

  .components-container {
    display: flex;
    flex-flow: row wrap;
    align-items: flex-start;
    gap: 0 32px;
  }

    .component-card {
      background-color: #14191B;
      border-radius: 16px;
      padding: 28px 32px 32px 32px;
      display: flex;
      flex-flow: column;
      align-items: center;
      margin-top: 32px;
      flex-grow: 1;
    }

    .component-title {
      font-size: 12px;
      font-weight: 500;
      color: #B1B1B1;
      padding-bottom: 8px;
    }

    .component-value {
      font-size: 32px;
      font-weight: 500;
      padding-bottom: 12px;
    }
      
      .text-green {
        color: #04B543;
      }
      
      .text-orange {
        color: #E76E0F;
      }
      
      .text-red {
        color: #EA3829;
      }

      .tooltip {
        position: relative;
        display: inline-block;
      }

      .tooltip .tooltiptext {
        visibility: hidden;
        width: 120px;
        background-color: #464646;
        color: #E6E6E6;
        text-align: center;
        border-radius: 6px;
        position: absolute;
        z-index: 1;
        top: 150%;
        left: 50%;
        margin-left: -60px;
        font-size: 12px;
        padding: 12px;
      }
      
      .tooltip .tooltiptext::after {
        content: "";
        position: absolute;
        bottom: 100%;
        left: 50%;
        margin-left: -5px;
        border-width: 5px;
        border-style: solid;
        border-color: transparent transparent black transparent;
      }
      
      .tooltip:hover .tooltiptext {
        visibility: visible;
      }

    .overall-card {
      background-color: #026836;
      border-radius: 16px;
      padding: 28px 32px 32px 32px;
      display: flex;
      flex-flow: column;
      align-items: center;
      justify-content: center;
      margin-top: 32px;
      flex-grow: 1;
    }
  
    .overall-title {
      font-size: 12px;
      font-weight: 500;
      color: #E6E6E6;
      padding: 14px 0 8px 0;
      text-transform: uppercase;
     }
  
    .overall-value {
      font-size: 32px;
      font-weight: 500;
      padding-bottom: 12px;
      color: #E6E6E6;
    }

.section-title {
  font-size: 12px;
  color: #B1B1B1;
  margin-bottom: 20px;
  text-align: left;
  width: 100%;
}

.section-content {
  color: #E6E6E6;
  font-size: 20px;
  line-height: 1.5;
}

.section-card {
  background-color: #14191B;
  border-radius: 16px;
  padding: 28px 32px 32px 32px;
  display: flex;
  flex-flow: column;
  align-items: center;
}

.correctness-indicator{
  padding: 20px;
  border-radius: 50px;
  font-size: 16pt;
  box-shadow: 0 4px 8px 0 #000000, 0 6px 20px 0 #000000;
  text-align: center;
}

.metric-title{
  margin: -2px;
  border-bottom: none;
}

.hist-row {
  display: flex;
  flex-direction: row;
  padding: 10px;
  justify-content: space-around;
  width: 85%;
}

.hist-row>div {
  flex: auto;
  box-sizing: border-box;
  display: flex;
  flex-direction: column;
  justify-content: center;
  align-items: center;
  padding-left: 1%;
  padding-right: 1%;
}

.tab-row{
  display: flex;
  flex-direction: column;
  align-items: center;
}

#component-table{
  width:50%;
  margin-top: 10px;
}

.green{
  background-color: #0a980a;
}

.orange {
  background-color: #e5b62a;
}

.red {
  background-color: #ba0e0e;
}

.progress-green {
  --color: #04B543;
}

.progress-orange {
  --color: #E76E0F;
}

.progress-red {
  --color: #EA3829;
}

.corr-plot{
  flex: 1;
  padding-left: 2%;
}

.tooltip-text {
  position: absolute;
  display: none;
  visibility: hidden;
  z-index: 1;
  top: 100%;
  left: 0%;
  width: 100%;
  color: white;
  font-size: 12px;
  background-color: #2d3d4c;
  border-radius: 10px;
  padding: 10px 15px 10px 15px;
}

#fade {
  opacity: 1;
  transition: opacity 0.5s;
}

#delay {
  opacity: 0;
  transition: opacity 0.2s;
  transition-delay: 1s;
}

td {
  position: relative;
}

tr:hover .tooltip-text {
  display: block;
  visibility: visible;
}

.tr:hover #fade {
  opacity: 1;
}

.tr:hover #delay {
  opacity: 1;
}

.callout {
  padding: 0.5rem 1rem 0.5rem 3rem;
  background: #D9EDF9;
  border: 3px solid #0088D1;
  color: #272eb5;
  position: relative;
  max-width: 40rem;
  border-radius: 10px;
  margin-top: 10%;
  font-size: 11pt;
}

.callout-icon {
  content: "";

  /* SVG via a data URI! */
  background-size: cover;
  width: 1.5rem;
  height: 1.5rem;
  display: block;
  position: absolute;
  left: 0.9rem;
  top: 1.1rem;
}

.callout-icon svg{
  fill: #016ca7;
}
.callout p+p {
  margin-top: 1em;
}

.callout a {
  color: #272eb5;
  font-weight: bold;
}

#gsk-logo {
  padding-top: 10px;
}
</style>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js" integrity="sha384-5QIrjQuyo4I/x6DK/Sau33lcA3hT2TCZGr9vbk+2ebd7Da6FnR1amdM+9B5xOrSf" crossorigin="anonymous"></script>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js" integrity="sha384-tXTWPp/bAKa+K9RPuXh7DNvye0Mv+P+6y4rAMVy+pWapsnXg9UG7g20WZ0N4i28A" crossorigin="anonymous"></script>





<div class="main">
    <div id="gsk-rag" class="dark:text-white dark:bg-zinc-800 rounded border border-gray-500">
        <div class="header border-b border-b-gray-500">
            
                <svg xmlns="http://www.w3.org/2000/svg" width="60" height="30" viewBox="0 0 30 15" fill="none" id="gsk-logo">
                    <path fill="#fff" fill-rule="evenodd"
                        d="M22.504 1.549a4.196 4.196 0 0 1 2.573-.887v.002a3.783 3.783 0 0 1 2.706 1.086 3.783 3.783 0 0 1 1.126 2.69 3.771 3.771 0 0 1-1.126 2.69 3.77 3.77 0 0 1-2.706 1.085l-4.794.011-2.533 3.467L8.203 15l2.881-3.335a9.829 9.829 0 0 1-4.663-1.68H3.185L0 7.163h3.934C4.263 3.165 8.187 0 12.96 0c2.24 0 4.489.696 6.175 1.909a7.423 7.423 0 0 1 1.882 1.919 4.194 4.194 0 0 1 1.487-2.28ZM7.05 3.249l3.91 3.915h1.505L7.89 2.584a7.773 7.773 0 0 0-.84.665Zm4.079-2.008 5.923 5.923h1.503l-6.086-6.087c-.45.023-.898.078-1.34.164ZM4.574 8.226h-1.77l.784.693h1.584a8.454 8.454 0 0 1-.598-.693Zm9.479 0H5.984c1.469 1.477 3.656 2.377 5.977 2.422l2.092-2.422Zm-2.458 4.472 5.492-1.902 1.878-2.569h-3.508l-3.862 4.47Zm10.361-5.552h3.265a2.714 2.714 0 0 0 1.747-4.648 2.711 2.711 0 0 0-1.888-.773 3.127 3.127 0 0 0-3.123 3.124v2.297Zm3.659-3.73a.677.677 0 1 1-.134 1.348.677.677 0 0 1 .134-1.348Z"
                        clip-rule="evenodd" />
                </svg>
            <h1>RAG Evaluation Toolkit</h1>
        </div>
        
        <div class="section-container">
            <div class="components-container">
                
                <div class="component-card">
                    <div class="component-title">GENERATOR</div>
                    <div class="component-value tooltip  text-green ">
                        70.0%
                            <span class="tooltiptext" id="fade">The Generator is the LLM inside the RAG to generate the answers.</span>
                    </div>
                    <div class="component-bar">
                        <progress max="100" value=70.0 class=" progress-green ">70.0%</progress>
                    </div>
                </div>
                
                <div class="component-card">
                    <div class="component-title">RETRIEVER</div>
                    <div class="component-value tooltip  text-orange ">
                        37.5%
                            <span class="tooltiptext" id="fade">The Retriever fetches relevant documents from the knowledge base according to a user query.</span>
                    </div>
                    <div class="component-bar">
                        <progress max="100" value=37.5 class=" progress-orange ">37.5%</progress>
                    </div>
                </div>
                
                <div class="component-card">
                    <div class="component-title">REWRITER</div>
                    <div class="component-value tooltip  text-orange ">
                        58.33%
                            <span class="tooltiptext" id="fade">The Rewriter modifies the user query to match a predefined format or to include the context from the chat history.</span>
                    </div>
                    <div class="component-bar">
                        <progress max="100" value=58.33333333333333 class=" progress-orange ">58.33%</progress>
                    </div>
                </div>
                
                <div class="component-card">
                    <div class="component-title">ROUTING</div>
                    <div class="component-value tooltip  text-green ">
                        100.0%
                            <span class="tooltiptext" id="fade">The Router filters the query of the user based on his intentions (intentions detection).</span>
                    </div>
                    <div class="component-bar">
                        <progress max="100" value=100.0 class=" progress-green ">100.0%</progress>
                    </div>
                </div>
                
                <div class="component-card">
                    <div class="component-title">KNOWLEDGE_BASE</div>
                    <div class="component-value tooltip  text-red ">
                        25.0%
                            <span class="tooltiptext" id="fade">The knowledge base is the set of documents given to the RAG to generate the answers. Its scores is computed differently from the other components: it is the difference between the maximum and minimum correctness score across all the topics of the knowledge base.</span>
                    </div>
                    <div class="component-bar">
                        <progress max="100" value=25.0 class=" progress-red ">25.0%</progress>
                    </div>
                </div>
                
                <div class="overall-card">
                    <div class="overall-title">Overall Correctness Score</div>
                    <div class="overall-value">65%</div>
                </div>
            </div>
        </div>

        <div class="section-container">
            <div class="section-card">
                <div class="section-title">RECOMMENDATION</div>
                <span class="section-content">Based on the scores, it appears that the RAG system is struggling with complex questions and conversational questions, particularly in the Self-Attention Mechanism and Transformer Model topics, which are not well-represented in the knowledge base. I recommend improving the knowledge base by adding more documents and information related to these topics, with a focus on increasing the coverage of Self-Attention Mechanism and Transformer Model, to boost the overall performance of the system.</span>
            </div>
        </div>

        <div class="section-container">
            <div class="section-card">
                <div class="section-title">CORRECTNESS BY TOPIC</div>
                    <script type="text/javascript">
        (function() {
  const fn = function() {
    Bokeh.safely(function() {
      (function(root) {
        function embed_document(root) {
        const docs_json = '{"f2c84543-cec8-4f69-9132-42141c549c77":{"version":"3.4.3","title":"Bokeh Application","roots":[{"type":"object","name":"Figure","id":"p2184","attributes":{"height":350,"width_policy":"max","x_range":{"type":"object","name":"DataRange1d","id":"p2186","attributes":{"start":0}},"y_range":{"type":"object","name":"FactorRange","id":"p2193","attributes":{"factors":["Deep Learning","Others","Self-Attention Mechanism","Transformer Model","Transformer Models"]}},"x_scale":{"type":"object","name":"LinearScale","id":"p2194"},"y_scale":{"type":"object","name":"CategoricalScale","id":"p2195"},"title":{"type":"object","name":"Title","id":"p2191","attributes":{"text_color":"#E0E0E0","text_font":"Helvetica","text_font_size":"14pt"}},"outline_line_color":"#E0E0E0","outline_line_alpha":0.25,"renderers":[{"type":"object","name":"GlyphRenderer","id":"p2213","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p2181","attributes":{"selected":{"type":"object","name":"Selection","id":"p2182","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p2183"},"data":{"type":"map","entries":[["correctness",{"type":"ndarray","array":{"type":"bytes","data":"AAAAAADAUkAAAAAAAIBRQAAAAAAAAAAAAAAAAAAAAAAAAAAAAMBSQA=="},"shape":[5],"dtype":"float64","order":"little"}],["metadata_values",["Deep Learning","Others","Self-Attention Mechanism","Transformer Model","Transformer Models"]],["colors",["#006837","#118847","#a50026","#a50026","#006837"]]]}}},"view":{"type":"object","name":"CDSView","id":"p2214","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p2215"}}},"glyph":{"type":"object","name":"HBar","id":"p2210","attributes":{"y":{"type":"field","field":"metadata_values"},"height":{"type":"value","value":0.85},"right":{"type":"field","field":"correctness"},"line_color":{"type":"value","value":"#1f77b4"},"fill_color":{"type":"value","value":"#14191B"}}},"nonselection_glyph":{"type":"object","name":"HBar","id":"p2211","attributes":{"y":{"type":"field","field":"metadata_values"},"height":{"type":"value","value":0.85},"right":{"type":"field","field":"correctness"},"line_color":{"type":"value","value":"#1f77b4"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#14191B"},"fill_alpha":{"type":"value","value":0.1},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"HBar","id":"p2212","attributes":{"y":{"type":"field","field":"metadata_values"},"height":{"type":"value","value":0.85},"right":{"type":"field","field":"correctness"},"line_color":{"type":"value","value":"#1f77b4"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#14191B"},"fill_alpha":{"type":"value","value":0.2},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p2222","attributes":{"data_source":{"id":"p2181"},"view":{"type":"object","name":"CDSView","id":"p2223","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p2224"}}},"glyph":{"type":"object","name":"HBar","id":"p2219","attributes":{"y":{"type":"field","field":"metadata_values"},"height":{"type":"value","value":0.85},"right":{"type":"field","field":"correctness"},"line_color":{"type":"value","value":"white"},"line_width":{"type":"value","value":2},"fill_color":{"type":"value","value":"#78BBFA"},"fill_alpha":{"type":"value","value":0.7}}},"nonselection_glyph":{"type":"object","name":"HBar","id":"p2220","attributes":{"y":{"type":"field","field":"metadata_values"},"height":{"type":"value","value":0.85},"right":{"type":"field","field":"correctness"},"line_color":{"type":"value","value":"white"},"line_alpha":{"type":"value","value":0.1},"line_width":{"type":"value","value":2},"fill_color":{"type":"value","value":"#78BBFA"},"fill_alpha":{"type":"value","value":0.1},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"HBar","id":"p2221","attributes":{"y":{"type":"field","field":"metadata_values"},"height":{"type":"value","value":0.85},"right":{"type":"field","field":"correctness"},"line_color":{"type":"value","value":"white"},"line_alpha":{"type":"value","value":0.2},"line_width":{"type":"value","value":2},"fill_color":{"type":"value","value":"#78BBFA"},"fill_alpha":{"type":"value","value":0.2},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p2232","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p2226","attributes":{"selected":{"type":"object","name":"Selection","id":"p2227","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p2228"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p2233","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p2234"}}},"glyph":{"type":"object","name":"Line","id":"p2229","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_color":"#EA3829","line_width":2,"line_dash":[6]}},"nonselection_glyph":{"type":"object","name":"Line","id":"p2230","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_color":"#EA3829","line_alpha":0.1,"line_width":2,"line_dash":[6]}},"muted_glyph":{"type":"object","name":"Line","id":"p2231","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_color":"#EA3829","line_alpha":0.2,"line_width":2,"line_dash":[6]}}}}],"toolbar":{"type":"object","name":"Toolbar","id":"p2192","attributes":{"tools":[{"type":"object","name":"HoverTool","id":"p2206","attributes":{"renderers":"auto","tooltips":[["topic","@metadata_values"],["Correctness","@correctness{0.00}"]]}}]}},"toolbar_location":null,"left":[{"type":"object","name":"CategoricalAxis","id":"p2201","attributes":{"ticker":{"type":"object","name":"CategoricalTicker","id":"p2202"},"formatter":{"type":"object","name":"CategoricalTickFormatter","id":"p2203"},"axis_label_standoff":10,"axis_label_text_color":"#E0E0E0","axis_label_text_font":"Helvetica","axis_label_text_font_size":"1.25em","axis_label_text_font_style":"normal","major_label_policy":{"type":"object","name":"AllLabels","id":"p2204"},"major_label_text_color":"#E0E0E0","major_label_text_font":"Helvetica","major_label_text_font_size":"1.025em","axis_line_color":"#E0E0E0","axis_line_alpha":0,"major_tick_line_color":"#E0E0E0","major_tick_line_alpha":0,"minor_tick_line_color":"#E0E0E0","minor_tick_line_alpha":0}}],"below":[{"type":"object","name":"LinearAxis","id":"p2196","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p2197","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p2198"},"axis_label":"Correctness (%)","axis_label_standoff":10,"axis_label_text_color":"#E0E0E0","axis_label_text_font":"Helvetica","axis_label_text_font_size":"1.25em","axis_label_text_font_style":"normal","major_label_policy":{"type":"object","name":"AllLabels","id":"p2199"},"major_label_text_color":"#E0E0E0","major_label_text_font":"Helvetica","major_label_text_font_size":"1.025em","axis_line_color":"#E0E0E0","axis_line_alpha":0,"major_tick_line_color":"#E0E0E0","major_tick_line_alpha":0,"minor_tick_line_color":"#E0E0E0","minor_tick_line_alpha":0}}],"center":[{"type":"object","name":"Grid","id":"p2200","attributes":{"axis":{"id":"p2196"},"grid_line_color":"#E0E0E0","grid_line_alpha":0.25}},{"type":"object","name":"Grid","id":"p2205","attributes":{"dimension":1,"axis":{"id":"p2201"},"grid_line_color":"#E0E0E0","grid_line_alpha":0.25}},{"type":"object","name":"Span","id":"p2225","attributes":{"location":65.0,"dimension":"height","line_color":"#EA3829","line_width":2,"line_dash":[6]}},{"type":"object","name":"Legend","id":"p2235","attributes":{"border_line_alpha":0,"background_fill_color":"#111516","background_fill_alpha":0.5,"label_text_color":"#E0E0E0","label_text_font":"Helvetica","label_text_font_size":"1.025em","label_standoff":8,"glyph_width":15,"spacing":8,"items":[{"type":"object","name":"LegendItem","id":"p2236","attributes":{"label":{"type":"value","value":"Correctness on the entire Testset"},"renderers":[{"id":"p2232"}]}}]}}],"background_fill_color":"#14191B","border_fill_color":"#15191C"}}]}}';
        const render_items = [{"docid":"f2c84543-cec8-4f69-9132-42141c549c77","roots":{"p2184":"cd079b06-d7cb-4947-b0cc-28f2990aaee6"},"root_ids":["p2184"]}];
        root.Bokeh.embed.embed_items(docs_json, render_items);
        }
        if (root.Bokeh !== undefined) {
          embed_document(root);
        } else {
          let attempts = 0;
          const timer = setInterval(function(root) {
            if (root.Bokeh !== undefined) {
              clearInterval(timer);
              embed_document(root);
            } else {
              attempts++;
              if (attempts > 100) {
                clearInterval(timer);
                console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing");
              }
            }
          }, 10, root)
        }
      })(window);
    });
  };
  if (document.readyState != "loading") fn();
  else document.addEventListener("DOMContentLoaded", fn);
})();
    </script>

                <div id="cd079b06-d7cb-4947-b0cc-28f2990aaee6" data-root-id="p2184" style="display: contents;"></div>
            </div>
        </div>

        <div class="section-container">
            <div class="section-card">
                <div class="section-title">KNOWLEDGE BASE OVERVIEW</div>
                    <script type="text/javascript">
        (function() {
  const fn = function() {
    Bokeh.safely(function() {
      (function(root) {
        function embed_document(root) {
        const docs_json = '{"ef57feaf-e4e2-4256-a518-332a18159f87":{"version":"3.4.3","title":"Bokeh Application","roots":[{"type":"object","name":"Tabs","id":"p2115","attributes":{"sizing_mode":"stretch_width","tabs":[{"type":"object","name":"TabPanel","id":"p2045","attributes":{"title":"Topic exploration","child":{"type":"object","name":"Figure","id":"p1998","attributes":{"sizing_mode":"stretch_width","x_range":{"type":"object","name":"Range1d","id":"p2007","attributes":{"start":13.708381032943725,"end":21.32900791168213}},"y_range":{"type":"object","name":"Range1d","id":"p2008","attributes":{"start":4.127101182937622,"end":11.613864660263062}},"x_scale":{"type":"object","name":"LinearScale","id":"p2009"},"y_scale":{"type":"object","name":"LinearScale","id":"p2010"},"title":{"type":"object","name":"Title","id":"p2005","attributes":{"text_color":"#E0E0E0","text_font":"Helvetica","text_font_size":"14pt"}},"outline_line_color":"#E0E0E0","outline_line_alpha":0.25,"renderers":[{"type":"object","name":"GlyphRenderer","id":"p2038","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p1995","attributes":{"selected":{"type":"object","name":"Selection","id":"p1996","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p1997"},"data":{"type":"map","entries":[["x",{"type":"ndarray","array":{"type":"bytes","data":"ZdGEQbjXfUHfuYRBx2qUQTl2gEG+NYtBeqSMQQ3VhEGnCYRBRo2AQeg9YkFiF2lBcViEQZjLgEFAyYZBsJuKQdSye0Gpf3NBwQhtQe+udkET0H1BWhd3QbyvckFCAIRBw0d/QW8piEEwXo5BVvyHQS1ackEPxGBBR7lkQaxwcUGtfm1BaQdfQbo3Y0HVzIRBgwZyQfuRfUHf44VBYU9/Qf3NiUEdq4xBHICPQYV2lEFmY4tB4mySQeWqkUEAh5NBLTiOQfhjaEFwSXBBTsZpQQ=="},"shape":[52],"dtype":"float32","order":"little"}],["y",{"type":"ndarray","array":{"type":"bytes","data":"mgn/QFvDF0EzRhtBK1wEQXXbBUFtvOxA9PXeQA++3kAa5ApBRQz4QAqr1kAs08FAaC2zQByMv0DvvMdAVQy8QCT/q0DmSsFAudLaQHoa90CJyMxACD7lQMMe0UCxCutAw+DkQPC5+UC/wvxAyf4kQcaqDUFntApB4WQSQaHKI0FuURZBXxIaQZppIUGmFSFBzVMaQW3bJUE6zxBBQXITQXb6GUFvPhJB1OkGQUHeFEGrOwtBdpYfQcRbEUGagvNAhIwgQcP88ECWdwNBrlgAQQ=="},"shape":[52],"dtype":"float32","order":"little"}],["topic",["Transformer Models","Others","Others","Others","Transformer Models","Transformer Models","Others","Others","Transformer Models","Transformer Models","Others","Others","Others","Others","Others","Others","Others","Others","Others","Transformer Models","Others","Transformer Models","Others","Transformer Models","Transformer Models","Transformer Models","Deep Learning","Others","Others","Others","Others","Others","Others","Others","Others","Others","Others","Others","Others","Others","Others","Deep Learning","Deep Learning","Others","Deep Learning","Others","Deep Learning","Others","Others","Others","Transformer Models","Transformer Models"]],["id",[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51]],["content",["Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works. Attention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez\\u2217\\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n\\u0141ukasz K...","We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014...","applying it successfully to English constituency parsing both with large and limited training data. \\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became...","and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research. \\u2020Work performed while at Google Brain. \\u2021Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS...","1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15]. Recurrent models typically factor computation along the symbol posit...","nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains. Attention mechanisms have become an integral part of com...","model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs. 2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which ...","positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\\n...","a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on sel...","[17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when...","Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively. 3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully conn...","of dimension dmodel = 512. Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subs...","Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel. of the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key. 3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \\"Scaled Dot-Product Attention\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of...","packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n\\u221adk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n\\u221adk . Additive attention computes the compatibility function using a feed-forward network with\\na single hid...","dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n\\u221adk . 3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe f...","get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q \\u00b7 k = Pdk\\ni=1 qiki, has mean 0 and variance dk. 4","output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this. MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n\\u2208Rdmodel\\u00d7dk, W K\\ni\\n\\u2208Rdmodel\\u00d7dk, W V\\n...","the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\u2022 In \\"encoder-decoder attention\\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. ...","in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. \\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \\u2212\\u221e) all values in the input\\nof the softma...","is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048. 3.4\\nEmbeddings and Sof...","model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by \\u221admodel. 5","Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention. Layer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 \\u00b7 d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n \\u00b7 d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k \\u00b7 n \\u00b7 d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r...","decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9]. In this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometr...","learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training. 4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ....","by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn lo...","length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npa...","the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k \\u00b7 n \\u00b7 d + n \\u00b7 d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in o...","to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models. 5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences ...","P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days). 5.3\\nOptimizer\\nWe used the Adam optimizer [20] with \\u03b21 = 0.9, \\u03b22 = 0.98 and \\u03f5 = 10\\u22129. We varied the learning\\nrate over the course of training, according ...","Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost. Model\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 \\u00b7 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 \\u00b7 1019\\n1.4 \\u00b7 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 \\u00b7 1018\\n1.5 \\u00b7 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 \\u00b7 1019\\n1.2 \\u00b7 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 \\u00b7 1020\\nGNMT + ...","encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1. Label Smoothing\\nDuring training, we employed label smoothing of value \\u03f5ls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) b...","surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3. For the base models, we used a singl...","set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5. 6.2\\nModel Variations\\nTo ...","Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities. N\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\n\\u03f5ls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n\\u00d7106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n...","4.33 26.4 213 development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that red...","we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to ...","a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9","Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals &amp; Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang &amp; Harper (2009) [14]\\nsemi-supervis...","300. We used a beam size of 21 and \\u03b1 = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K senten...","multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transfor...","another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor. Acknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration. References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016. [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlea...","[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014. [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016. [7] Junyoung Chung, \\u00c7aglar G\\u00fcl\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeli...","to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017. [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770\\u2013778, 2016. [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\u00fcrgen Schmidhuber. Gradient flow in\\nrecurrent nets: the dif...","Natural Language Processing, pages 832\\u2013841. ACL, August 2009. [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016. [16] \\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016. [17] \\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR),...","Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017. [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017. [23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-tas...","[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\u2013330, 1993. [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152\\u2013159. ACL, June 2006. [27] Ankur Parikh, Oscar T\\u00e4ckstr\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel...","of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433\\u2013440. ACL, July 2006. [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016. [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015. [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey ...","15(1):1929\\u20131958, 2014. [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440\\u20132448. Curran Associates,\\nInc., 2015. [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\\u20133112, 2014. [36] Christian S...","Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\u2019s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016. [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016. [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jing...","Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n&lt;EOS&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n&lt;EOS&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\nFigure 3: An example of the attention mechanism following long-dista...","Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n&lt;EOS&gt;\\n&lt;pad&gt;\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n&lt;EOS&gt;\\n&lt;pad&gt;\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n&lt;EOS&gt;\\n&lt;pad&gt;\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat...","Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n&lt;EOS&gt;\\n&lt;pad&gt;\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n&lt;EOS&gt;\\n&lt;pad&gt;\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n&lt;EOS&gt;\\n&lt;pad&gt;\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat..."]],["color",{"type":"ndarray","array":["#1f77b4","#999","#999","#999","#1f77b4","#1f77b4","#999","#999","#1f77b4","#1f77b4","#999","#999","#999","#999","#999","#999","#999","#999","#999","#1f77b4","#999","#1f77b4","#999","#1f77b4","#1f77b4","#1f77b4","#aec7e8","#999","#999","#999","#999","#999","#999","#999","#999","#999","#999","#999","#999","#999","#999","#aec7e8","#aec7e8","#999","#aec7e8","#999","#aec7e8","#999","#999","#999","#1f77b4","#1f77b4"],"shape":[52],"dtype":"object","order":"little"}]]}}},"view":{"type":"object","name":"CDSView","id":"p2039","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p2040"}}},"glyph":{"type":"object","name":"Scatter","id":"p2035","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":6},"line_color":{"type":"field","field":"color"},"line_width":{"type":"value","value":2},"fill_color":{"type":"field","field":"color"},"fill_alpha":{"type":"value","value":0.7},"hatch_color":{"type":"field","field":"color"},"hatch_alpha":{"type":"value","value":0.7}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p2036","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":6},"line_color":{"type":"field","field":"color"},"line_alpha":{"type":"value","value":0.1},"line_width":{"type":"value","value":2},"fill_color":{"type":"field","field":"color"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"field","field":"color"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p2037","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":6},"line_color":{"type":"field","field":"color"},"line_alpha":{"type":"value","value":0.2},"line_width":{"type":"value","value":2},"fill_color":{"type":"field","field":"color"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"field","field":"color"},"hatch_alpha":{"type":"value","value":0.2}}}}}],"toolbar":{"type":"object","name":"Toolbar","id":"p2006","attributes":{"logo":"grey","tools":[{"type":"object","name":"HoverTool","id":"p2021","attributes":{"renderers":"auto","tooltips":"\\n    &lt;div style=\\"width:400px;\\"&gt;\\n    &lt;b&gt;Document id:&lt;/b&gt; @id &lt;br&gt;\\n    &lt;b&gt;Topic:&lt;/b&gt; @topic &lt;br&gt;\\n    &lt;b&gt;Document Content:&lt;/b&gt; @content\\n    &lt;/div&gt;\\n    "}},{"type":"object","name":"PanTool","id":"p2022"},{"type":"object","name":"WheelZoomTool","id":"p2023","attributes":{"renderers":"auto"}},{"type":"object","name":"BoxZoomTool","id":"p2024","attributes":{"overlay":{"type":"object","name":"BoxAnnotation","id":"p2025","attributes":{"syncable":false,"level":"overlay","visible":false,"left":{"type":"number","value":"nan"},"right":{"type":"number","value":"nan"},"top":{"type":"number","value":"nan"},"bottom":{"type":"number","value":"nan"},"left_units":"canvas","right_units":"canvas","top_units":"canvas","bottom_units":"canvas","line_color":"black","line_alpha":1.0,"line_width":2,"line_dash":[4,4],"fill_color":"lightgrey","fill_alpha":0.5}}}},{"type":"object","name":"ResetTool","id":"p2030"},{"type":"object","name":"SaveTool","id":"p2031"}]}},"left":[{"type":"object","name":"LinearAxis","id":"p2016","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p2017","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p2018"},"axis_label_standoff":10,"axis_label_text_color":"#E0E0E0","axis_label_text_font":"Helvetica","axis_label_text_font_size":"1.25em","axis_label_text_font_style":"normal","major_label_policy":{"type":"object","name":"AllLabels","id":"p2019"},"major_label_text_color":"#E0E0E0","major_label_text_font":"Helvetica","major_label_text_font_size":"1.025em","axis_line_color":"#E0E0E0","axis_line_alpha":0,"major_tick_line_color":"#E0E0E0","major_tick_line_alpha":0,"minor_tick_line_color":"#E0E0E0","minor_tick_line_alpha":0}}],"below":[{"type":"object","name":"LinearAxis","id":"p2011","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p2012","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p2013"},"axis_label_standoff":10,"axis_label_text_color":"#E0E0E0","axis_label_text_font":"Helvetica","axis_label_text_font_size":"1.25em","axis_label_text_font_style":"normal","major_label_policy":{"type":"object","name":"AllLabels","id":"p2014"},"major_label_text_color":"#E0E0E0","major_label_text_font":"Helvetica","major_label_text_font_size":"1.025em","axis_line_color":"#E0E0E0","axis_line_alpha":0,"major_tick_line_color":"#E0E0E0","major_tick_line_alpha":0,"minor_tick_line_color":"#E0E0E0","minor_tick_line_alpha":0}}],"center":[{"type":"object","name":"Grid","id":"p2015","attributes":{"axis":{"id":"p2011"},"grid_line_color":"white","grid_line_alpha":0.25}},{"type":"object","name":"Grid","id":"p2020","attributes":{"dimension":1,"axis":{"id":"p2016"},"grid_line_color":"white","grid_line_alpha":0.25}},{"type":"object","name":"Legend","id":"p2041","attributes":{"title":"Knowledge Base Tospics","title_text_color":"#B1B1B1","title_text_font_style":"bold","border_line_alpha":0,"background_fill_color":"#111516","background_fill_alpha":0.5,"label_text_color":"#E0E0E0","label_text_font":"Helvetica","label_text_font_size":"1.025em","label_standoff":8,"glyph_width":15,"spacing":8,"items":[{"type":"object","name":"LegendItem","id":"p2042","attributes":{"label":{"type":"value","value":"Deep Learning"},"renderers":[{"id":"p2038"}],"index":26}},{"type":"object","name":"LegendItem","id":"p2043","attributes":{"label":{"type":"value","value":"Others"},"renderers":[{"id":"p2038"}],"index":1}},{"type":"object","name":"LegendItem","id":"p2044","attributes":{"label":{"type":"value","value":"Transformer Models"},"renderers":[{"id":"p2038"}],"index":0}}]}}],"background_fill_color":"#14191B","border_fill_color":"#15191C"}}}},{"type":"object","name":"TabPanel","id":"p2114","attributes":{"title":"Failures","child":{"type":"object","name":"Figure","id":"p2050","attributes":{"sizing_mode":"stretch_width","x_range":{"type":"object","name":"Range1d","id":"p2059","attributes":{"start":13.708381032943725,"end":21.32900791168213}},"y_range":{"type":"object","name":"Range1d","id":"p2060","attributes":{"start":4.127101182937622,"end":11.613864660263062}},"x_scale":{"type":"object","name":"LinearScale","id":"p2061"},"y_scale":{"type":"object","name":"LinearScale","id":"p2062"},"title":{"type":"object","name":"Title","id":"p2057","attributes":{"text_color":"#E0E0E0","text_font":"Helvetica","text_font_size":"14pt"}},"outline_line_color":"#E0E0E0","outline_line_alpha":0.25,"renderers":[{"type":"object","name":"GlyphRenderer","id":"p2089","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p2046","attributes":{"selected":{"type":"object","name":"Selection","id":"p2047","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p2048"},"data":{"type":"map","entries":[["x",{"type":"ndarray","array":{"type":"bytes","data":"HauMQVoXd0HibJJBHICPQR2rjEFAyYZB1cyEQS1ackHlqpFBrX5tQeg9YkEAh5NB+5F9QVoXd0Gwm4pBZdGEQUDJhkHvrnZBRo2AQWIXaUE="},"shape":[20],"dtype":"float32","order":"little"}],["y",{"type":"ndarray","array":{"type":"bytes","data":"bz4SQQg+5UB2lh9B1OkGQW8+EkHvvMdAphUhQcaqDUHEWxFBblEWQQqr1kCagvNAbdslQQg+5UBVDLxAmgn/QO+8x0B6GvdARQz4QCzTwUA="},"shape":[20],"dtype":"float32","order":"little"}],["topic",["Deep Learning","Self-Attention Mechanism","Others","Deep Learning","Deep Learning","Others","Others","Others","Deep Learning","Transformer Model","Others","Others","Others","Transformer Models","Others","Transformer Models","Others","Transformer Models","Transformer Models","Others"]],["correctness",[true,false,true,true,false,true,true,true,true,false,false,true,false,true,false,false,true,true,true,true]],["questions",["Who are the authors of the paper titled &#x27;Learning phrase representations using rnn encoder-decoder for statistical machine translation&#x27;?","What type of operations are required for the Positional Encoding?","Can you provide a brief summary of the context in which the papers listed were published, specifically highlighting any relevant conferences or journal names?","What year was the paper &#x27;Long short-term memory&#x27; first published, and was it initially presented at a conference or published in a journal?","What type of neural networks are mentioned in the context, specifically in relation to the attention mechanism described as &#x27;Scaled Dot-Product Attention&#x27;?","Under what conditions do the dot products in the attention mechanism become excessively large, leading to vanishing gradients in the softmax function?","As a researcher in France interested in evaluating the Transformer&#x27;s ability to generalize to diverse tasks, what task did the researchers perform to assess its performance on constituency parsing?","As a researcher evaluating the efficiency of GPU training for natural language processing models, I&#x27;m trying to understand how to adjust the learning rate during training. What&#x27;s the formula for varying the learning rate during training?","What are some recent research papers related to neural machine translation and what is the concept of the sparsely-gated mixture-of-experts layer?","What is the formula used?","What is the output dimension of each sub-layer in the Transformer model?","Who are the authors of the End-to-end memory networks paper?","What is the maximum output length used in the Transformer model when the input length is increased by 300, as in the experiment reported in Table 4?","What is the computational complexity of the Self-Attention layer, assuming a sequence length of n, a representation dimension of d, and a kernel size of k?","What is the variance of the dot product of two independent random variables q and k, given that q and k are vectors of dimension dk and dv, respectively?","How does the use of attention mechanisms in the proposed Transformer model differ from dot-product attention, especially for large values of sequence length?","As I&#x27;m trying to optimize the performance of my deep neural network with larger input sizes, I&#x27;ve noticed that the dot products in the attention mechanism are getting extremely large for bigger values of d_k. Can you help me understand why this is happening?","As I&#x27;m trying to implement a transformer-based model for natural language processing, I&#x27;m having trouble understanding how the input and output dimensions of the linear transformations in the Feed Forward Neural Network (FFN) work, considering I&#x27;m new to this architecture.","What is the structure of the most competitive neural sequence transduction models, and are they auto-regressive?","How many?"]],["agent_answer",["The authors of the paper titled \\"Learning phrase representations using rnn encoder-decoder for statistical machine translation\\" are Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio .","The Positional Encoding in the Transformer model involves adding information about the relative or absolute position of tokens in the sequence. The specific operations required for the Positional Encoding are based on sine and cosine functions of different frequencies. The formulas used are: - For even dimensions:   $   P E(pos, 2i) = \\\\sin\\\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)   $ - For odd dimensions:   $   P E(pos, 2i+1) = \\\\cos\\\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)   $ Here, $pos$ is the position and $i$ is the dimension. This approach allows the model to learn to attend to relative positions effectively [T6].","The paper \\"Attention Is All You Need\\" by Ashish Vaswani et al. was presented at the **Advances in Neural Information Processing Systems (NeurIPS)** conference in 2017. This conference is a prominent venue for research in machine learning and artificial intelligence, where cutting-edge advancements in these fields are shared. The other paper you mentioned, \\"Learning phrase representations using rnn encoder-decoder for statistical machine translation,\\" was published in the **Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)**. EMNLP is a leading conference focused on natural language processing and computational linguistics, showcasing significant research contributions in these areas. Both conferences are highly regarded in the machine learning and natural language processing communities, making the works published there influential in advancing the state of the art in their respective fields.","The paper titled \\"Long short-term memory\\" by Sepp Hochreiter and J\\u00fcrgen Schmidhuber was first published in **1997**. It was presented at the **Neural Information Processing Systems (NIPS)** conference, specifically at the NIPS 1997 conference. This foundational work introduced the Long Short-Term Memory (LSTM) architecture, which has since become a critical component in the field of recurrent neural networks and sequence modeling.","In the context of the \\"Scaled Dot-Product Attention\\" mechanism described in the paper, the attention mechanism is primarily associated with **transformer networks**. The transformer architecture relies entirely on attention mechanisms, specifically self-attention, to compute representations of input and output sequences without using recurrent neural networks (RNNs) or convolutional layers. Additionally, the paper discusses how attention mechanisms, including the scaled dot-product attention, have been integrated into various models, including **recurrent neural networks (RNNs)** and **long short-term memory (LSTM)** networks, to enhance their ability to model dependencies in sequences. However, the transformer model itself represents a departure from these traditional architectures by eliminating recurrence and leveraging attention for parallelization and efficiency","The dot products in the attention mechanism can become excessively large when the dimensionality of the keys and queries, denoted as $d_k$, is large. This situation arises because under the assumption that the components of the query $q$ and key $k$ are independent random variables with a mean of 0 and a variance of 1, the variance of their dot product $q \\\\cdot k$ grows linearly with $d_k$. Specifically, the variance of the dot product is given by:     $     \\text{Var         }(q \\\\cdot k) = d_k     $     As $d_k$ increases, the dot products can take on large values, which pushes the softmax function into regions where it has extremely small gradients. This phenomenon can lead to vanishing gradients, making it difficult for the model to learn effectively during training. To mitigate this effect, the dot products are scaled by a factor of $\\frac{             1         }{\\\\sqrt{d_k             }         }$ before applying the softmax function, which helps to keep the values in a more manageable range and maintains the gradients at a more useful scale.","To evaluate the Transformer&#x27;s ability to generalize to diverse tasks, the researchers performed experiments on **English constituency parsing**. This task involves analyzing the syntactic structure of sentences and presents specific challenges, such as strong structural constraints and significantly longer outputs compared to inputs.      In their experiments, the researchers trained a 4-layer transformer model with a dimensionality of $d_{model         } = 1024$ on the Wall Street Journal (WSJ) portion of the Penn Treebank, which consists of about 40,         000 training sentences. They also conducted training in a semi-supervised setting using larger corpora, which included approximately 17 million sentences. The results indicated that the Transformer performed surprisingly well on this task, yielding competitive results compared to previously reported models, demonstrating its generalization capabilities.","The formula for varying the learning rate during training, as described in the paper, is given by:     $\\text{lrate         } = d_{model         }^{             -0.5         } \\\\cdot \\\\min(\\text{step\\\\_num         }^{             -0.5         }, \\text{step\\\\_num         } \\\\cdot \\text{warmup\\\\_steps         }^{             -1.5         })     $     In this formula:     - $d_{model         }$ is the dimensionality of the model.     - $\\text{step\\\\_num         }$ is the current training step number.     - $\\text{warmup\\\\_steps         }$ is a predefined number of steps during which the learning rate increases linearly.          The learning rate increases linearly for the first $ \\text{warmup\\\\_steps         } $ training steps and then decreases proportionally to the inverse square root of the step number thereafter. This approach helps stabilize training in the initial phases and allows for effective learning as training progresses ","Some recent research papers related to neural machine translation mentioned in the document include: 1. **\\"Neural machine translation in linear time\\"** by Nal Kalchbrenner et al. (2017) [T3     ]. 2. **\\"Effective approaches to attention-based neural machine translation\\"** by Minh-Thang Luong et al. (2015) [T3     ]. 3. **\\"A decomposable attention model\\"** by Ankur Parikh et al. (2016) [T5     ]. 4. **\\"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\\"** by Noam Shazeer et al. (2017) [T5     ]. ### Concept of the Sparsely-Gated Mixture-of-Experts Layer The **sparsely-gated mixture-of-experts (MoE) layer** is a neural network architecture that allows for the use of multiple expert models while only activating a subset of them for each input. This approach is designed to improve model capacity and efficiency.  In the MoE layer: - Each input is routed to a small number of \\"expert\\" models based on a gating mechanism, which determines which experts to activate for a given input. - This sparsity means that, although the overall model may contain many parameters (from all experts), only a few are used at any one time, which reduces computational costs and memory usage. - The gating mechanism typically uses a softmax function to assign probabilities to each expert, allowing the model to learn which experts are most relevant for different types of inputs. This architecture enables the model to scale effectively, leveraging the strengths of multiple experts while maintaining efficiency, making it particularly useful in tasks like neural machine translation where diverse linguistic patterns may be present [T5     ].","The formula used for the sparsely-gated mixture-of-experts (MoE) layer is not explicitly detailed in the provided text. However, the general concept involves a gating mechanism that can be described as follows:    1. **Gating Function**: For an input $ x $, the gating function $ g $ produces a set of weights for the experts:       $       g(x) = \\text{softmax        }(W_g x + b_g)       $       where $ W_g $ and $ b_g $ are the parameters of the gating network.    2. **Expert Outputs**: Each expert $ e_i $ processes the input $ x $ to produce an output $ e_i(x) $.    3. **Final Output**: The final output $ y $ of the MoE layer is computed as a weighted sum of the outputs of the active experts:       $       y = \\\\sum_{i=1        }^{N        } g_i(x) \\\\cdot e_i(x)       $       where $ g_i(x) $ is the weight assigned to expert $ i $ for the input $ x $, and $ N $ is the total number of experts.    In practice, only a subset of experts is activated for each input, which allows the model to maintain efficiency while leveraging a larger capacity. This sparsity is what distinguishes the MoE layer from traditional dense layers in neural networks [T5        ].","In the Transformer model, the output dimension of each sub-layer is $d_{model} = 512$. This dimensionality is consistent across all sub-layers in both the encoder and decoder. Each sub-layer, which includes multi-head attention and position-wise feed-forward networks, produces outputs of this same dimension, facilitating the residual connections and layer normalization that are integral to the model&#x27;s architecture [T6].","The authors of the \\"End-to-end memory networks\\" paper are Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus [T6].","In the experiment reported in Table 4, the maximum output length in the Transformer model is increased to the input length plus 300. This means that if the input length is $ n $, the maximum output length would be $ n + 300 $ [T6].","The computational complexity of the Self-Attention layer, assuming a sequence length of $ n $ and a representation dimension of $ d $, is given by:    $    O(n^2 \\\\cdot d)    $    This complexity arises because the Self-Attention mechanism computes attention scores for all pairs of input positions, leading to $ n^2 $ interactions, each involving operations on vectors of dimension $ d $ [T5].","The variance of the dot product of two independent random variables $ q $ and $ k $, where both are vectors of dimension $ d_k $, is given by: $\\text{Var}(q \\\\cdot k) = d_k$ This is derived from the fact that if the components of $ q $ and $ k $ are independent random variables with mean 0 and variance 1, the dot product $ q \\\\cdot k = \\\\sum_{i=1     }^{d_k     } q_i k_i $ will have a variance equal to the sum of the variances of the individual products, which results in $ d_k $ [T6     ].","In the proposed Transformer model, the attention mechanism is specifically designed as \\"Scaled Dot-Product Attention.\\" The key difference from standard dot-product attention, particularly for large values of sequence length, lies in the scaling factor applied to the dot products. 1. **Scaling Factor**: In the Transformer model, the dot products of the queries and keys are divided by $ \\\\sqrt{d_k} $ (where $ d_k $ is the dimension of the keys). This scaling is crucial because, for large values of $ d_k $, the dot products can grow large in magnitude, which can push the softmax function into regions where it has very small gradients. By scaling down the dot products, the model mitigates this issue, allowing for more stable gradients during training [T6]. 2. **Computational Efficiency**: The Transformer model&#x27;s attention mechanism is implemented using highly optimized matrix multiplication, making it more efficient in practice compared to naive implementations of dot-product attention. This efficiency is particularly beneficial when dealing with long sequences, as it allows the model to handle larger inputs without a significant increase in computational cost [T6]. 3. **Parallelization**: The attention mechanism in the Transformer allows for the computation of attention scores for all positions in parallel, which is a significant advantage over recurrent models. This parallelization is especially effective for long sequences, as it reduces the time complexity associated with sequential processing [T2]. Overall, the use of scaled dot-product attention in the Transformer model enhances the model&#x27;s ability to learn from long sequences by addressing the issues of gradient stability and computational efficiency.","The issue of extremely large dot products in the attention mechanism, particularly for larger values of $ d_k $ (the dimension of the keys), arises from the statistical properties of the dot product of random vectors.    1. **Dot Product Growth**: When you compute the dot product of two independent random vectors $ q $ and $ k $, each with components that are independent random variables (assuming they have mean 0 and variance 1), the dot product can be expressed as:       $       q \\\\cdot k = \\\\sum_{i=1        }^{d_k        } q_i k_i       $       The variance of this dot product is $ d_k $. This means that as $ d_k $ increases, the expected magnitude of the dot product also increases, leading to larger values.        2. **Magnitude of Components**: For large dimensions, the components of the vectors $ q $ and $ k $ can take on larger values due to the central limit theorem, which suggests that the sum of a large number of independent random variables tends to be normally distributed with a mean that grows with the number of variables. Consequently, the dot product can become very large, which can push the softmax function into regions where it has very small gradients.        3. **Softmax Saturation**: When the dot products are large, the softmax function, which is used to compute the attention weights, can become saturated. This means that the output probabilities can become very close to 0 or 1, leading to a loss of information and making it difficult for the model to learn effectively. The gradients during backpropagation can also become very small, hindering the training process.        To mitigate this issue, the Transformer model employs a scaling factor of $ \\frac{            1        }{\\\\sqrt{d_k            }        } $ in the attention mechanism. This scaling helps to keep the dot products within a more manageable range, ensuring that the softmax function operates in a region where it can produce meaningful gradients for learning [T6        ].","In the Transformer architecture, the Feed Forward Neural Network (FFN) is a crucial component that processes the output from the attention layers. Here&#x27;s how the input and output dimensions work for the FFN: 1. Input Dimension: The input to the FFN is a vector of dimension dmodel\\u200b, which is the same dimension used throughout the Transformer model (typically 512 in the original paper). Each position in the sequence has a corresponding vector of this dimension. 2. Structure of the FFN: The FFN consists of two linear transformations with a ReLU activation in between. The mathematical representation is as follows: FFN(x)=max(0,xW1\\u200b+b1\\u200b)W2\\u200b+b2\\u200b - Here, W1\\u200b and W2\\u200b are weight matrices for the two linear transformations. - b1\\u200b and b2\\u200b are the corresponding bias vectors. 3. Output Dimension: - The first linear transformation (with weight matrix W1\\u200b) expands the input dimension from dmodel\\u200b to dff\\u200b (the inner-layer dimension), which is typically larger (e.g., 2048). - The second linear transformation (with weight matrix W2\\u200b) reduces the dimension back to dmodel\\u200b. 4. Example: If dmodel\\u200b=512 and dff\\u200b=2048: - The first transformation takes an input vector of size 512 and produces an output vector of size 2048. - After applying the ReLU activation, the second transformation takes the 2048-dimensional output and reduces it back to 512 dimensions. This structure allows the FFN to learn complex representations while maintaining the same dimensionality throughout the model, which is essential for the residual connections used in the Transformer architecture , . If you have further questions or need clarification on specific parts, feel free to ask!","In a Transformer model, the Feed Forward Neural Network (FFN) is a crucial component that processes the output from the attention layers. Here&#x27;s how the input and output dimensions of the linear transformations in the FFN work: 1. Input Dimension: The input to the FFN comes from the output of the attention layer, which has a dimension of dmodel\\u200b. This dimension is consistent throughout the model, meaning that the output from the attention mechanism (whether it&#x27;s from the encoder or decoder) will have the same dimension as the input to the FFN. 2. Structure of the FFN: The FFN typically consists of two linear transformations with a ReLU activation in between. The structure can be described as follows: - The first linear transformation projects the input from dmodel\\u200b to a higher-dimensional space, often denoted as dff\\u200b (the dimension of the feed-forward layer). This is usually larger than dmodel\\u200b to allow for more complex representations. - The second linear transformation projects the output back from dff\\u200b to dmodel\\u200b. Mathematically, this can be represented as: FFN(x)=ReLU(xW1\\u200b+b1\\u200b)W2\\u200b+b2\\u200b where: - x is the input with dimension dmodel\\u200b. - W1\\u200b is the weight matrix for the first linear transformation with dimensions dmodel\\u200b\\u00d7dff\\u200b. - b1\\u200b is the bias vector for the first layer with dimension dff\\u200b. - W2\\u200b is the weight matrix for the second linear transformation with dimensions dff\\u200b\\u00d7dmodel\\u200b. - b2\\u200b is the bias vector for the second layer with dimension dmodel\\u200b. 3. Output Dimension: The output of the FFN will have the same dimension as the input, which is dmodel\\u200b. This ensures that the output can be seamlessly integrated back into the subsequent layers of the Transformer architecture, maintaining consistency in dimensions throughout the model. In summary, the FFN in a Transformer model takes an input of dimension dmodel\\u200b, processes it through two linear transformations (with an activation function in between), and outputs a tensor of the same dimension dmodel\\u200b. This design allows the model to learn complex representations while maintaining the necessary dimensionality for further processing in the architecture .","In the Transformer architecture, each encoder and decoder consists of a stack of identical layers. Specifically: 1. Encoder: The encoder is composed of N=6 identical layers. Each layer includes a multi-head self-attention mechanism followed by a feed-forward neural network (FFN). 2. Decoder: Similarly, the decoder also consists of N=6 identical layers. Each decoder layer includes a multi-head attention mechanism (which attends to the encoder&#x27;s output), a self-attention mechanism, and a feed-forward neural network. Thus, in total, there are 6 encoder layers and 6 decoder layers in the standard Transformer model, making a total of 12 layers ."]],["reference_answer",["Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.","The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.","The papers listed are related to Natural Language Processing (NLP) and Computational Linguistics.","1997","Recurrent neural networks (RNNs)","The dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients","English constituency parsing","lrate = d\\u22120.5 \\u00b7 model \\u00b7 min(step_num\\u22120.5, step_num \\u00b7 warmup_steps\\u22121.5)","Recent research papers related to neural machine translation include &#x27;Using the output embedding to improve language models&#x27; by Ofir Press and Lior Wolf (2016), and papers related to neural machine translation include &#x27;Outrageously large neural networks: The sparsely-gated mixture-of-experts layer&#x27; by Noam Shazeer et al. (2017).","We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU","512","Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus","increased the maximum output length to input length + 300","O(n2 \\u00b7 d)","dk","The proposed Transformer model is based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.","The dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.","dmodel = 512, and the inner-layer has dimensionality dff = 2048","Most competitive neural sequence transduction models have an encoder-decoder structure and are auto-regressive.","The decoder stack consists of 6 identical layers."]],["id",[41,21,45,42,41,14,35,28,46,32,10,47,37,21,15,0,14,19,9,11]],["content",["[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014. [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016. [7] Junyoung Chung, \\u00c7aglar G\\u00fcl\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeli...","Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention. Layer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 \\u00b7 d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n \\u00b7 d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k \\u00b7 n \\u00b7 d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r...","[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\u2013330, 1993. [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152\\u2013159. ACL, June 2006. [27] Ankur Parikh, Oscar T\\u00e4ckstr\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel...","to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017. [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770\\u2013778, 2016. [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\u00fcrgen Schmidhuber. Gradient flow in\\nrecurrent nets: the dif...","[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014. [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016. [7] Junyoung Chung, \\u00c7aglar G\\u00fcl\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeli...","dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n\\u221adk . 3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe f...","we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to ...","P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days). 5.3\\nOptimizer\\nWe used the Adam optimizer [20] with \\u03b21 = 0.9, \\u03b22 = 0.98 and \\u03f5 = 10\\u22129. We varied the learning\\nrate over the course of training, according ...","of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433\\u2013440. ACL, July 2006. [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016. [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015. [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey ...","set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5. 6.2\\nModel Variations\\nTo ...","Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively. 3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully conn...","15(1):1929\\u20131958, 2014. [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440\\u20132448. Curran Associates,\\nInc., 2015. [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\\u20133112, 2014. [36] Christian S...","Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals &amp; Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang &amp; Harper (2009) [14]\\nsemi-supervis...","Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention. Layer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 \\u00b7 d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n \\u00b7 d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k \\u00b7 n \\u00b7 d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r...","get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q \\u00b7 k = Pdk\\ni=1 qiki, has mean 0 and variance dk. 4","Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works. Attention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez\\u2217\\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n\\u0141ukasz K...","dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n\\u221adk . 3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe f...","is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048. 3.4\\nEmbeddings and Sof...","[17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when...","of dimension dmodel = 512. Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subs..."]],["color",["#0a980a","#ba0e0e","#0a980a","#0a980a","#ba0e0e","#0a980a","#0a980a","#0a980a","#0a980a","#ba0e0e","#ba0e0e","#0a980a","#ba0e0e","#0a980a","#ba0e0e","#ba0e0e","#0a980a","#0a980a","#0a980a","#0a980a"]]]}}},"view":{"type":"object","name":"CDSView","id":"p2090","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p2091"}}},"glyph":{"type":"object","name":"Scatter","id":"p2086","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":6},"line_color":{"type":"field","field":"color"},"line_width":{"type":"value","value":2},"fill_color":{"type":"field","field":"color"},"fill_alpha":{"type":"value","value":0.7},"hatch_color":{"type":"field","field":"color"},"hatch_alpha":{"type":"value","value":0.7}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p2087","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":6},"line_color":{"type":"field","field":"color"},"line_alpha":{"type":"value","value":0.1},"line_width":{"type":"value","value":2},"fill_color":{"type":"field","field":"color"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"field","field":"color"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p2088","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":6},"line_color":{"type":"field","field":"color"},"line_alpha":{"type":"value","value":0.2},"line_width":{"type":"value","value":2},"fill_color":{"type":"field","field":"color"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"field","field":"color"},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p2104","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p2095","attributes":{"selected":{"type":"object","name":"Selection","id":"p2096","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p2097"},"data":{"type":"map","entries":[["x",{"type":"ndarray","array":{"type":"bytes","data":"ZdGEQbjXfUHfuYRBx2qUQTl2gEG+NYtBeqSMQQ3VhEGnCYRBRo2AQeg9YkFiF2lBcViEQZjLgEFAyYZBsJuKQdSye0Gpf3NBwQhtQe+udkET0H1BWhd3QbyvckFCAIRBw0d/QW8piEEwXo5BVvyHQS1ackEPxGBBR7lkQaxwcUGtfm1BaQdfQbo3Y0HVzIRBgwZyQfuRfUHf44VBYU9/Qf3NiUEdq4xBHICPQYV2lEFmY4tB4mySQeWqkUEAh5NBLTiOQfhjaEFwSXBBTsZpQQ=="},"shape":[52],"dtype":"float32","order":"little"}],["y",{"type":"ndarray","array":{"type":"bytes","data":"mgn/QFvDF0EzRhtBK1wEQXXbBUFtvOxA9PXeQA++3kAa5ApBRQz4QAqr1kAs08FAaC2zQByMv0DvvMdAVQy8QCT/q0DmSsFAudLaQHoa90CJyMxACD7lQMMe0UCxCutAw+DkQPC5+UC/wvxAyf4kQcaqDUFntApB4WQSQaHKI0FuURZBXxIaQZppIUGmFSFBzVMaQW3bJUE6zxBBQXITQXb6GUFvPhJB1OkGQUHeFEGrOwtBdpYfQcRbEUGagvNAhIwgQcP88ECWdwNBrlgAQQ=="},"shape":[52],"dtype":"float32","order":"little"}]]}}},"view":{"type":"object","name":"CDSView","id":"p2105","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p2106"}}},"glyph":{"type":"object","name":"Scatter","id":"p2101","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":6},"line_color":{"type":"value","value":"grey"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.2}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p2102","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":6},"line_color":{"type":"value","value":"grey"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p2103","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":6},"line_color":{"type":"value","value":"grey"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.2}}}}}],"toolbar":{"type":"object","name":"Toolbar","id":"p2058","attributes":{"logo":"grey","tools":[{"type":"object","name":"PanTool","id":"p2073"},{"type":"object","name":"WheelZoomTool","id":"p2074","attributes":{"renderers":"auto"}},{"type":"object","name":"BoxZoomTool","id":"p2075","attributes":{"overlay":{"type":"object","name":"BoxAnnotation","id":"p2076","attributes":{"syncable":false,"level":"overlay","visible":false,"left":{"type":"number","value":"nan"},"right":{"type":"number","value":"nan"},"top":{"type":"number","value":"nan"},"bottom":{"type":"number","value":"nan"},"left_units":"canvas","right_units":"canvas","top_units":"canvas","bottom_units":"canvas","line_color":"black","line_alpha":1.0,"line_width":2,"line_dash":[4,4],"fill_color":"lightgrey","fill_alpha":0.5}}}},{"type":"object","name":"ResetTool","id":"p2081"},{"type":"object","name":"SaveTool","id":"p2082"},{"type":"object","name":"HoverTool","id":"p2049","attributes":{"renderers":[{"id":"p2089"}],"tooltips":"\\n    &lt;div style=\\"width:400px;\\"&gt;\\n    &lt;b&gt;Document id:&lt;/b&gt; @id &lt;br&gt;\\n    &lt;b&gt;Topic:&lt;/b&gt; @topic &lt;br&gt;\\n    &lt;b&gt;Question:&lt;/b&gt; @questions &lt;br&gt;\\n    &lt;b&gt;agent Answer:&lt;/b&gt; @agent_answer &lt;br&gt;\\n    &lt;b&gt;Reference Answer:&lt;/b&gt; @reference_answer &lt;br&gt;\\n    &lt;b&gt;Correctness:&lt;/b&gt; @correctness &lt;br&gt;\\n    &lt;b&gt;Content:&lt;/b&gt; @content\\n    &lt;/div&gt;\\n    "}}]}},"left":[{"type":"object","name":"LinearAxis","id":"p2068","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p2069","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p2070"},"axis_label_standoff":10,"axis_label_text_color":"#E0E0E0","axis_label_text_font":"Helvetica","axis_label_text_font_size":"1.25em","axis_label_text_font_style":"normal","major_label_policy":{"type":"object","name":"AllLabels","id":"p2071"},"major_label_text_color":"#E0E0E0","major_label_text_font":"Helvetica","major_label_text_font_size":"1.025em","axis_line_color":"#E0E0E0","axis_line_alpha":0,"major_tick_line_color":"#E0E0E0","major_tick_line_alpha":0,"minor_tick_line_color":"#E0E0E0","minor_tick_line_alpha":0}}],"below":[{"type":"object","name":"LinearAxis","id":"p2063","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p2064","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p2065"},"axis_label_standoff":10,"axis_label_text_color":"#E0E0E0","axis_label_text_font":"Helvetica","axis_label_text_font_size":"1.25em","axis_label_text_font_style":"normal","major_label_policy":{"type":"object","name":"AllLabels","id":"p2066"},"major_label_text_color":"#E0E0E0","major_label_text_font":"Helvetica","major_label_text_font_size":"1.025em","axis_line_color":"#E0E0E0","axis_line_alpha":0,"major_tick_line_color":"#E0E0E0","major_tick_line_alpha":0,"minor_tick_line_color":"#E0E0E0","minor_tick_line_alpha":0}}],"center":[{"type":"object","name":"Grid","id":"p2067","attributes":{"axis":{"id":"p2063"},"grid_line_color":"white","grid_line_alpha":0.25}},{"type":"object","name":"Grid","id":"p2072","attributes":{"dimension":1,"axis":{"id":"p2068"},"grid_line_color":"white","grid_line_alpha":0.25}},{"type":"object","name":"Legend","id":"p2092","attributes":{"title":"Question Correctness","title_text_color":"#B1B1B1","title_text_font_style":"bold","border_line_alpha":0,"background_fill_color":"#111516","background_fill_alpha":0.5,"label_text_color":"#E0E0E0","label_text_font":"Helvetica","label_text_font_size":"1.025em","label_standoff":8,"glyph_width":15,"spacing":8,"items":[{"type":"object","name":"LegendItem","id":"p2093","attributes":{"label":{"type":"value","value":"False"},"renderers":[{"id":"p2089"}],"index":1}},{"type":"object","name":"LegendItem","id":"p2094","attributes":{"label":{"type":"value","value":"True"},"renderers":[{"id":"p2089"}],"index":0}}]}},{"type":"object","name":"LabelSet","id":"p2110","attributes":{"level":"glyph","source":{"type":"object","name":"ColumnDataSource","id":"p2107","attributes":{"selected":{"type":"object","name":"Selection","id":"p2108","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p2109"},"data":{"type":"map","entries":[["x",{"type":"ndarray","array":{"type":"bytes","data":"aGaAQYpRjkE="},"shape":[2],"dtype":"float32","order":"little"}],["y",{"type":"ndarray","array":{"type":"bytes","data":"/ZP5QANtCkE="},"shape":[2],"dtype":"float32","order":"little"}],["topic",["Transformer Models","Deep Learning"]]]}}},"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"text":{"type":"field","field":"topic"},"text_color":{"type":"value","value":"#B1B1B1"},"text_font_size":{"type":"value","value":"12pt"},"text_font_style":{"type":"value","value":"bold"},"text_align":{"type":"value","value":"center"}}}],"background_fill_color":"#14191B","border_fill_color":"#15191C"}}}}],"tabs_location":"below"}}]}}';
        const render_items = [{"docid":"ef57feaf-e4e2-4256-a518-332a18159f87","roots":{"p2115":"c68244c0-c0a2-4cc5-9a3c-1e3466db57e1"},"root_ids":["p2115"]}];
        root.Bokeh.embed.embed_items(docs_json, render_items);
        }
        if (root.Bokeh !== undefined) {
          embed_document(root);
        } else {
          let attempts = 0;
          const timer = setInterval(function(root) {
            if (root.Bokeh !== undefined) {
              clearInterval(timer);
              embed_document(root);
            } else {
              attempts++;
              if (attempts > 100) {
                clearInterval(timer);
                console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing");
              }
            }
          }, 10, root)
        }
      })(window);
    });
  };
  if (document.readyState != "loading") fn();
  else document.addEventListener("DOMContentLoaded", fn);
})();
    </script>

                <div id="c68244c0-c0a2-4cc5-9a3c-1e3466db57e1" data-root-id="p2115" style="display: contents;"></div>
            </div>
        </div>

        <div class="section-container">
            <div class="section-card">

                <div class="section-title">SELECTED METRICS</div>

                

            </div>
        </div>

        
    </div>
</div>



<script type="text/javascript">
    function opentab(evt, name) {
    // Declare all variables
    let i, tabcontent, tablinks;

    // Get all elements with class="tabcontent" and hide them
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }

    // Get all elements with class="tablinks" and remove the class "active"
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }

    // Show the current tab, and add an "active" class to the button that opened the tab
    document.getElementById(name).style.display = "block";
    evt.currentTarget.className += " active";
}
</script>
