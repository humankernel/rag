<!DOCTYPE html>


<style>
    body{
  background: #18181B;
}

.main{
  font-family: "Noto Sans", ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
  color: #FDFDFD;
}

h1 {
  font-size: 2.5rem;
  color: white;
}

h3 {
  font-size: 1.5rem;
  background: #0c087c;
  padding: 10px;
  margin: 0px;
  border: 1px solid #6b7280;}

.extended-title{
  width:100%;
}

#gsk-overview{
  display:flex;
}

h4 {
  font-size: 1rem;
  background: #27272A;
  padding: 10px;
  margin: 0px;
  border-bottom: 1px solid #6b7280;
}

h2 {
  font-size: 1.5rem;
  margin-top: 3px;
  color:#000000;
}

.header{
  display: flex;
  justify-content: center;
  align-items: center;
}
.header > * {
  margin-inline: 20px;
}

.flex-row {
  display: flex;
  flex-direction: row;
  padding:10px;
  border: 1px solid #27272A;
}

.flex-row>div {
  flex: auto;
  box-sizing: border-box;
  display: flex;
  flex-direction: column;
  justify-content: center;
  align-items: center;
}

progress[value] {
  --background: #6D6D6D;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  border: none;
  height: 4px;
  margin: 0 10px;
  border-radius: 10em;
  background: var(--background);
}

progress[value]::-webkit-progress-bar {
  border-radius: 10em;
  background: var(--background);
}

progress[value]::-webkit-progress-value {
  border-radius: 10em;
  background: var(--color);
}

progress[value]::-moz-progress-bar {
  border-radius: 10em;
  background: var(--color);
}

label {
  font-size: 20px;
  font-weight: bold;
  display: block;
  margin: 20px 0;
}

.tab {
  overflow: hidden;
  border: 1px solid #27272A;
  background-color: #27272A;
}

.tab button {
  background-color: inherit;
  float: left;
  border: none;
  outline: none;
  cursor: pointer;
  padding: 14px 16px;
  transition: 0.3s;
  color: #ffffff;
  font-size:1.2rem;
}

.tab div {
  background-color: inherit;
  float: left;
  border: none;
  outline: none;
  cursor: pointer;
  padding: 14px 16px;
  transition: 0.3s;
  color: #ffffff;
  font-size: 1.2rem;
}

.tab button:hover {
  background-color: #18181B;
}

.tab-title{
  font-size: 1.5rem;
  font-weight: bold;
  margin-bottom:-5px;
}

.tab button.active {
  background-color: #18181B;
  border-top: 1px solid #6b7280;
  border-bottom: 1px solid #18181B;
  border-left: 1px solid #6b7280;
  border-right: 1px solid #6b7280;
}

.tabcontent {
  display: none;
  padding: 6px 12px;
  background: #18181B;
  border: 1px solid #27272A;
  border-top: 1px solid #6b7280;
  margin-top: -2px;
}

#gsk-advice {
  display: flex;
  justify-content: center;
}

#gsk-metrics{
  width:100%;
}

#recommendation {
  margin-top: 20px;
  padding: 20px;
  border-radius: 10px;
  background-color: #e1ce86;
  color: #27272A;
  width:95%;
  box-shadow: 0 4px 8px 0 #000000, 0 6px 20px 0 #000000;
  font-size: 12pt;
}

.separator {
  margin: 20px 0;
}

.separator-border {
  margin: 20px 0;
  border-bottom: 1px solid #6b7280;
}

#gsk-rag{
  margin: 32px 28px;
  padding: 12px 24px;
  background-color: #111516;
}

.section-container {
  margin-bottom: 32px;
}

  .components-container {
    display: flex;
    flex-flow: row wrap;
    align-items: flex-start;
    gap: 0 32px;
  }

    .component-card {
      background-color: #14191B;
      border-radius: 16px;
      padding: 28px 32px 32px 32px;
      display: flex;
      flex-flow: column;
      align-items: center;
      margin-top: 32px;
      flex-grow: 1;
    }

    .component-title {
      font-size: 12px;
      font-weight: 500;
      color: #B1B1B1;
      padding-bottom: 8px;
    }

    .component-value {
      font-size: 32px;
      font-weight: 500;
      padding-bottom: 12px;
    }
      
      .text-green {
        color: #04B543;
      }
      
      .text-orange {
        color: #E76E0F;
      }
      
      .text-red {
        color: #EA3829;
      }

      .tooltip {
        position: relative;
        display: inline-block;
      }

      .tooltip .tooltiptext {
        visibility: hidden;
        width: 120px;
        background-color: #464646;
        color: #E6E6E6;
        text-align: center;
        border-radius: 6px;
        position: absolute;
        z-index: 1;
        top: 150%;
        left: 50%;
        margin-left: -60px;
        font-size: 12px;
        padding: 12px;
      }
      
      .tooltip .tooltiptext::after {
        content: "";
        position: absolute;
        bottom: 100%;
        left: 50%;
        margin-left: -5px;
        border-width: 5px;
        border-style: solid;
        border-color: transparent transparent black transparent;
      }
      
      .tooltip:hover .tooltiptext {
        visibility: visible;
      }

    .overall-card {
      background-color: #026836;
      border-radius: 16px;
      padding: 28px 32px 32px 32px;
      display: flex;
      flex-flow: column;
      align-items: center;
      justify-content: center;
      margin-top: 32px;
      flex-grow: 1;
    }
  
    .overall-title {
      font-size: 12px;
      font-weight: 500;
      color: #E6E6E6;
      padding: 14px 0 8px 0;
      text-transform: uppercase;
     }
  
    .overall-value {
      font-size: 32px;
      font-weight: 500;
      padding-bottom: 12px;
      color: #E6E6E6;
    }

.section-title {
  font-size: 12px;
  color: #B1B1B1;
  margin-bottom: 20px;
  text-align: left;
  width: 100%;
}

.section-content {
  color: #E6E6E6;
  font-size: 20px;
  line-height: 1.5;
}

.section-card {
  background-color: #14191B;
  border-radius: 16px;
  padding: 28px 32px 32px 32px;
  display: flex;
  flex-flow: column;
  align-items: center;
}

.correctness-indicator{
  padding: 20px;
  border-radius: 50px;
  font-size: 16pt;
  box-shadow: 0 4px 8px 0 #000000, 0 6px 20px 0 #000000;
  text-align: center;
}

.metric-title{
  margin: -2px;
  border-bottom: none;
}

.hist-row {
  display: flex;
  flex-direction: row;
  padding: 10px;
  justify-content: space-around;
  width: 85%;
}

.hist-row>div {
  flex: auto;
  box-sizing: border-box;
  display: flex;
  flex-direction: column;
  justify-content: center;
  align-items: center;
  padding-left: 1%;
  padding-right: 1%;
}

.tab-row{
  display: flex;
  flex-direction: column;
  align-items: center;
}

#component-table{
  width:50%;
  margin-top: 10px;
}

.green{
  background-color: #0a980a;
}

.orange {
  background-color: #e5b62a;
}

.red {
  background-color: #ba0e0e;
}

.progress-green {
  --color: #04B543;
}

.progress-orange {
  --color: #E76E0F;
}

.progress-red {
  --color: #EA3829;
}

.corr-plot{
  flex: 1;
  padding-left: 2%;
}

.tooltip-text {
  position: absolute;
  display: none;
  visibility: hidden;
  z-index: 1;
  top: 100%;
  left: 0%;
  width: 100%;
  color: white;
  font-size: 12px;
  background-color: #2d3d4c;
  border-radius: 10px;
  padding: 10px 15px 10px 15px;
}

#fade {
  opacity: 1;
  transition: opacity 0.5s;
}

#delay {
  opacity: 0;
  transition: opacity 0.2s;
  transition-delay: 1s;
}

td {
  position: relative;
}

tr:hover .tooltip-text {
  display: block;
  visibility: visible;
}

.tr:hover #fade {
  opacity: 1;
}

.tr:hover #delay {
  opacity: 1;
}

.callout {
  padding: 0.5rem 1rem 0.5rem 3rem;
  background: #D9EDF9;
  border: 3px solid #0088D1;
  color: #272eb5;
  position: relative;
  max-width: 40rem;
  border-radius: 10px;
  margin-top: 10%;
  font-size: 11pt;
}

.callout-icon {
  content: "";

  /* SVG via a data URI! */
  background-size: cover;
  width: 1.5rem;
  height: 1.5rem;
  display: block;
  position: absolute;
  left: 0.9rem;
  top: 1.1rem;
}

.callout-icon svg{
  fill: #016ca7;
}
.callout p+p {
  margin-top: 1em;
}

.callout a {
  color: #272eb5;
  font-weight: bold;
}

#gsk-logo {
  padding-top: 10px;
}
</style>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js" integrity="sha384-5QIrjQuyo4I/x6DK/Sau33lcA3hT2TCZGr9vbk+2ebd7Da6FnR1amdM+9B5xOrSf" crossorigin="anonymous"></script>
<script src="https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js" integrity="sha384-tXTWPp/bAKa+K9RPuXh7DNvye0Mv+P+6y4rAMVy+pWapsnXg9UG7g20WZ0N4i28A" crossorigin="anonymous"></script>





<div class="main">
    <div id="gsk-rag" class="dark:text-white dark:bg-zinc-800 rounded border border-gray-500">
        <div class="header border-b border-b-gray-500">
            
                <svg xmlns="http://www.w3.org/2000/svg" width="60" height="30" viewBox="0 0 30 15" fill="none" id="gsk-logo">
                    <path fill="#fff" fill-rule="evenodd"
                        d="M22.504 1.549a4.196 4.196 0 0 1 2.573-.887v.002a3.783 3.783 0 0 1 2.706 1.086 3.783 3.783 0 0 1 1.126 2.69 3.771 3.771 0 0 1-1.126 2.69 3.77 3.77 0 0 1-2.706 1.085l-4.794.011-2.533 3.467L8.203 15l2.881-3.335a9.829 9.829 0 0 1-4.663-1.68H3.185L0 7.163h3.934C4.263 3.165 8.187 0 12.96 0c2.24 0 4.489.696 6.175 1.909a7.423 7.423 0 0 1 1.882 1.919 4.194 4.194 0 0 1 1.487-2.28ZM7.05 3.249l3.91 3.915h1.505L7.89 2.584a7.773 7.773 0 0 0-.84.665Zm4.079-2.008 5.923 5.923h1.503l-6.086-6.087c-.45.023-.898.078-1.34.164ZM4.574 8.226h-1.77l.784.693h1.584a8.454 8.454 0 0 1-.598-.693Zm9.479 0H5.984c1.469 1.477 3.656 2.377 5.977 2.422l2.092-2.422Zm-2.458 4.472 5.492-1.902 1.878-2.569h-3.508l-3.862 4.47Zm10.361-5.552h3.265a2.714 2.714 0 0 0 1.747-4.648 2.711 2.711 0 0 0-1.888-.773 3.127 3.127 0 0 0-3.123 3.124v2.297Zm3.659-3.73a.677.677 0 1 1-.134 1.348.677.677 0 0 1 .134-1.348Z"
                        clip-rule="evenodd" />
                </svg>
            <h1>RAG Evaluation Toolkit</h1>
        </div>
        
        <div class="section-container">
            <div class="components-container">
                
                <div class="component-card">
                    <div class="component-title">GENERATOR</div>
                    <div class="component-value tooltip  text-orange ">
                        55.0%
                            <span class="tooltiptext" id="fade">The Generator is the LLM inside the RAG to generate the answers.</span>
                    </div>
                    <div class="component-bar">
                        <progress max="100" value=55.00000000000001 class=" progress-orange ">55.0%</progress>
                    </div>
                </div>
                
                <div class="component-card">
                    <div class="component-title">RETRIEVER</div>
                    <div class="component-value tooltip  text-green ">
                        75.0%
                            <span class="tooltiptext" id="fade">The Retriever fetches relevant documents from the knowledge base according to a user query.</span>
                    </div>
                    <div class="component-bar">
                        <progress max="100" value=75.0 class=" progress-green ">75.0%</progress>
                    </div>
                </div>
                
                <div class="component-card">
                    <div class="component-title">REWRITER</div>
                    <div class="component-value tooltip  text-orange ">
                        33.33%
                            <span class="tooltiptext" id="fade">The Rewriter modifies the user query to match a predefined format or to include the context from the chat history.</span>
                    </div>
                    <div class="component-bar">
                        <progress max="100" value=33.33333333333333 class=" progress-orange ">33.33%</progress>
                    </div>
                </div>
                
                <div class="component-card">
                    <div class="component-title">ROUTING</div>
                    <div class="component-value tooltip  text-green ">
                        100.0%
                            <span class="tooltiptext" id="fade">The Router filters the query of the user based on his intentions (intentions detection).</span>
                    </div>
                    <div class="component-bar">
                        <progress max="100" value=100.0 class=" progress-green ">100.0%</progress>
                    </div>
                </div>
                
                <div class="component-card">
                    <div class="component-title">KNOWLEDGE_BASE</div>
                    <div class="component-value tooltip  text-red ">
                        0.0%
                            <span class="tooltiptext" id="fade">The knowledge base is the set of documents given to the RAG to generate the answers. Its scores is computed differently from the other components: it is the difference between the maximum and minimum correctness score across all the topics of the knowledge base.</span>
                    </div>
                    <div class="component-bar">
                        <progress max="100" value=0.0 class=" progress-red ">0.0%</progress>
                    </div>
                </div>
                
                <div class="overall-card">
                    <div class="overall-title">Overall Correctness Score</div>
                    <div class="overall-value">50%</div>
                </div>
            </div>
        </div>

        <div class="section-container">
            <div class="section-card">
                <div class="section-title">RECOMMENDATION</div>
                <span class="section-content">Based on the provided scores, I recommend increasing the quality of the knowledge base, particularly for topics such as Deep Learning, Self-Attention Mechanism, and Transformer Models, which have low scores across all question types, to improve the RAG&#39;s overall performance.

The most important action you should take is to thoroughly review and refine the knowledge base, focusing on adding high-quality and relevant documents for the mentioned topics, which will have a significant impact on the RAG&#39;s ability to answer questions across all question types and topics.</span>
            </div>
        </div>

        <div class="section-container">
            <div class="section-card">
                <div class="section-title">CORRECTNESS BY TOPIC</div>
                    <script type="text/javascript">
        (function() {
  const fn = function() {
    Bokeh.safely(function() {
      (function(root) {
        function embed_document(root) {
        const docs_json = '{"ce5516d0-e1bd-474c-b9c7-f01c5d585767":{"version":"3.4.3","title":"Bokeh Application","roots":[{"type":"object","name":"Figure","id":"p1433","attributes":{"height":350,"width_policy":"max","x_range":{"type":"object","name":"DataRange1d","id":"p1435","attributes":{"start":0}},"y_range":{"type":"object","name":"FactorRange","id":"p1442","attributes":{"factors":["Deep Learning","Others","Self-Attention Mechanism","Transformer Model","Transformer Models"]}},"x_scale":{"type":"object","name":"LinearScale","id":"p1443"},"y_scale":{"type":"object","name":"CategoricalScale","id":"p1444"},"title":{"type":"object","name":"Title","id":"p1440","attributes":{"text_color":"#E0E0E0","text_font":"Helvetica","text_font_size":"14pt"}},"outline_line_color":"#E0E0E0","outline_line_alpha":0.25,"renderers":[{"type":"object","name":"GlyphRenderer","id":"p1462","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p1430","attributes":{"selected":{"type":"object","name":"Selection","id":"p1431","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p1432"},"data":{"type":"map","entries":[["correctness",{"type":"ndarray","array":{"type":"bytes","data":"AAAAAAAAOUAAAAAAAABOQAAAAAAAAFlAAAAAAAAAAAAAAAAAAABJQA=="},"shape":[5],"dtype":"float64","order":"little"}],["metadata_values",["Deep Learning","Others","Self-Attention Mechanism","Transformer Model","Transformer Models"]],["colors",["#f88e52","#d9ef8b","#006837","#a50026","#fefebd"]]]}}},"view":{"type":"object","name":"CDSView","id":"p1463","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p1464"}}},"glyph":{"type":"object","name":"HBar","id":"p1459","attributes":{"y":{"type":"field","field":"metadata_values"},"height":{"type":"value","value":0.85},"right":{"type":"field","field":"correctness"},"line_color":{"type":"value","value":"#1f77b4"},"fill_color":{"type":"value","value":"#14191B"}}},"nonselection_glyph":{"type":"object","name":"HBar","id":"p1460","attributes":{"y":{"type":"field","field":"metadata_values"},"height":{"type":"value","value":0.85},"right":{"type":"field","field":"correctness"},"line_color":{"type":"value","value":"#1f77b4"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#14191B"},"fill_alpha":{"type":"value","value":0.1},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"HBar","id":"p1461","attributes":{"y":{"type":"field","field":"metadata_values"},"height":{"type":"value","value":0.85},"right":{"type":"field","field":"correctness"},"line_color":{"type":"value","value":"#1f77b4"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#14191B"},"fill_alpha":{"type":"value","value":0.2},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p1471","attributes":{"data_source":{"id":"p1430"},"view":{"type":"object","name":"CDSView","id":"p1472","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p1473"}}},"glyph":{"type":"object","name":"HBar","id":"p1468","attributes":{"y":{"type":"field","field":"metadata_values"},"height":{"type":"value","value":0.85},"right":{"type":"field","field":"correctness"},"line_color":{"type":"value","value":"white"},"line_width":{"type":"value","value":2},"fill_color":{"type":"value","value":"#78BBFA"},"fill_alpha":{"type":"value","value":0.7}}},"nonselection_glyph":{"type":"object","name":"HBar","id":"p1469","attributes":{"y":{"type":"field","field":"metadata_values"},"height":{"type":"value","value":0.85},"right":{"type":"field","field":"correctness"},"line_color":{"type":"value","value":"white"},"line_alpha":{"type":"value","value":0.1},"line_width":{"type":"value","value":2},"fill_color":{"type":"value","value":"#78BBFA"},"fill_alpha":{"type":"value","value":0.1},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"HBar","id":"p1470","attributes":{"y":{"type":"field","field":"metadata_values"},"height":{"type":"value","value":0.85},"right":{"type":"field","field":"correctness"},"line_color":{"type":"value","value":"white"},"line_alpha":{"type":"value","value":0.2},"line_width":{"type":"value","value":2},"fill_color":{"type":"value","value":"#78BBFA"},"fill_alpha":{"type":"value","value":0.2},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p1481","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p1475","attributes":{"selected":{"type":"object","name":"Selection","id":"p1476","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p1477"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p1482","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p1483"}}},"glyph":{"type":"object","name":"Line","id":"p1478","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_color":"#EA3829","line_width":2,"line_dash":[6]}},"nonselection_glyph":{"type":"object","name":"Line","id":"p1479","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_color":"#EA3829","line_alpha":0.1,"line_width":2,"line_dash":[6]}},"muted_glyph":{"type":"object","name":"Line","id":"p1480","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_color":"#EA3829","line_alpha":0.2,"line_width":2,"line_dash":[6]}}}}],"toolbar":{"type":"object","name":"Toolbar","id":"p1441","attributes":{"tools":[{"type":"object","name":"HoverTool","id":"p1455","attributes":{"renderers":"auto","tooltips":[["topic","@metadata_values"],["Correctness","@correctness{0.00}"]]}}]}},"toolbar_location":null,"left":[{"type":"object","name":"CategoricalAxis","id":"p1450","attributes":{"ticker":{"type":"object","name":"CategoricalTicker","id":"p1451"},"formatter":{"type":"object","name":"CategoricalTickFormatter","id":"p1452"},"axis_label_standoff":10,"axis_label_text_color":"#E0E0E0","axis_label_text_font":"Helvetica","axis_label_text_font_size":"1.25em","axis_label_text_font_style":"normal","major_label_policy":{"type":"object","name":"AllLabels","id":"p1453"},"major_label_text_color":"#E0E0E0","major_label_text_font":"Helvetica","major_label_text_font_size":"1.025em","axis_line_color":"#E0E0E0","axis_line_alpha":0,"major_tick_line_color":"#E0E0E0","major_tick_line_alpha":0,"minor_tick_line_color":"#E0E0E0","minor_tick_line_alpha":0}}],"below":[{"type":"object","name":"LinearAxis","id":"p1445","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p1446","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p1447"},"axis_label":"Correctness (%)","axis_label_standoff":10,"axis_label_text_color":"#E0E0E0","axis_label_text_font":"Helvetica","axis_label_text_font_size":"1.25em","axis_label_text_font_style":"normal","major_label_policy":{"type":"object","name":"AllLabels","id":"p1448"},"major_label_text_color":"#E0E0E0","major_label_text_font":"Helvetica","major_label_text_font_size":"1.025em","axis_line_color":"#E0E0E0","axis_line_alpha":0,"major_tick_line_color":"#E0E0E0","major_tick_line_alpha":0,"minor_tick_line_color":"#E0E0E0","minor_tick_line_alpha":0}}],"center":[{"type":"object","name":"Grid","id":"p1449","attributes":{"axis":{"id":"p1445"},"grid_line_color":"#E0E0E0","grid_line_alpha":0.25}},{"type":"object","name":"Grid","id":"p1454","attributes":{"dimension":1,"axis":{"id":"p1450"},"grid_line_color":"#E0E0E0","grid_line_alpha":0.25}},{"type":"object","name":"Span","id":"p1474","attributes":{"location":50.0,"dimension":"height","line_color":"#EA3829","line_width":2,"line_dash":[6]}},{"type":"object","name":"Legend","id":"p1484","attributes":{"border_line_alpha":0,"background_fill_color":"#111516","background_fill_alpha":0.5,"label_text_color":"#E0E0E0","label_text_font":"Helvetica","label_text_font_size":"1.025em","label_standoff":8,"glyph_width":15,"spacing":8,"items":[{"type":"object","name":"LegendItem","id":"p1485","attributes":{"label":{"type":"value","value":"Correctness on the entire Testset"},"renderers":[{"id":"p1481"}]}}]}}],"background_fill_color":"#14191B","border_fill_color":"#15191C"}}]}}';
        const render_items = [{"docid":"ce5516d0-e1bd-474c-b9c7-f01c5d585767","roots":{"p1433":"ebf1cd2a-32c3-4594-8d4f-60770f7a4e45"},"root_ids":["p1433"]}];
        root.Bokeh.embed.embed_items(docs_json, render_items);
        }
        if (root.Bokeh !== undefined) {
          embed_document(root);
        } else {
          let attempts = 0;
          const timer = setInterval(function(root) {
            if (root.Bokeh !== undefined) {
              clearInterval(timer);
              embed_document(root);
            } else {
              attempts++;
              if (attempts > 100) {
                clearInterval(timer);
                console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing");
              }
            }
          }, 10, root)
        }
      })(window);
    });
  };
  if (document.readyState != "loading") fn();
  else document.addEventListener("DOMContentLoaded", fn);
})();
    </script>

                <div id="ebf1cd2a-32c3-4594-8d4f-60770f7a4e45" data-root-id="p1433" style="display: contents;"></div>
            </div>
        </div>

        <div class="section-container">
            <div class="section-card">
                <div class="section-title">KNOWLEDGE BASE OVERVIEW</div>
                    <script type="text/javascript">
        (function() {
  const fn = function() {
    Bokeh.safely(function() {
      (function(root) {
        function embed_document(root) {
        const docs_json = '{"680e4f44-12e9-437d-aad7-72e45bba4227":{"version":"3.4.3","title":"Bokeh Application","roots":[{"type":"object","name":"Tabs","id":"p1364","attributes":{"sizing_mode":"stretch_width","tabs":[{"type":"object","name":"TabPanel","id":"p1294","attributes":{"title":"Topic exploration","child":{"type":"object","name":"Figure","id":"p1247","attributes":{"sizing_mode":"stretch_width","x_range":{"type":"object","name":"Range1d","id":"p1256","attributes":{"start":13.708381032943725,"end":21.32900791168213}},"y_range":{"type":"object","name":"Range1d","id":"p1257","attributes":{"start":4.127101182937622,"end":11.613864660263062}},"x_scale":{"type":"object","name":"LinearScale","id":"p1258"},"y_scale":{"type":"object","name":"LinearScale","id":"p1259"},"title":{"type":"object","name":"Title","id":"p1254","attributes":{"text_color":"#E0E0E0","text_font":"Helvetica","text_font_size":"14pt"}},"outline_line_color":"#E0E0E0","outline_line_alpha":0.25,"renderers":[{"type":"object","name":"GlyphRenderer","id":"p1287","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p1244","attributes":{"selected":{"type":"object","name":"Selection","id":"p1245","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p1246"},"data":{"type":"map","entries":[["x",{"type":"ndarray","array":{"type":"bytes","data":"ZdGEQbjXfUHfuYRBx2qUQTl2gEG+NYtBeqSMQQ3VhEGnCYRBRo2AQeg9YkFiF2lBcViEQZjLgEFAyYZBsJuKQdSye0Gpf3NBwQhtQe+udkET0H1BWhd3QbyvckFCAIRBw0d/QW8piEEwXo5BVvyHQS1ackEPxGBBR7lkQaxwcUGtfm1BaQdfQbo3Y0HVzIRBgwZyQfuRfUHf44VBYU9/Qf3NiUEdq4xBHICPQYV2lEFmY4tB4mySQeWqkUEAh5NBLTiOQfhjaEFwSXBBTsZpQQ=="},"shape":[52],"dtype":"float32","order":"little"}],["y",{"type":"ndarray","array":{"type":"bytes","data":"mgn/QFvDF0EzRhtBK1wEQXXbBUFtvOxA9PXeQA++3kAa5ApBRQz4QAqr1kAs08FAaC2zQByMv0DvvMdAVQy8QCT/q0DmSsFAudLaQHoa90CJyMxACD7lQMMe0UCxCutAw+DkQPC5+UC/wvxAyf4kQcaqDUFntApB4WQSQaHKI0FuURZBXxIaQZppIUGmFSFBzVMaQW3bJUE6zxBBQXITQXb6GUFvPhJB1OkGQUHeFEGrOwtBdpYfQcRbEUGagvNAhIwgQcP88ECWdwNBrlgAQQ=="},"shape":[52],"dtype":"float32","order":"little"}],["topic",["Transformer Models","Others","Others","Others","Transformer Models","Transformer Models","Others","Others","Transformer Models","Transformer Models","Others","Others","Others","Others","Others","Others","Others","Others","Others","Transformer Models","Others","Transformer Models","Others","Transformer Models","Transformer Models","Transformer Models","Deep Learning","Others","Others","Others","Others","Others","Others","Others","Others","Others","Others","Others","Others","Others","Others","Deep Learning","Deep Learning","Others","Deep Learning","Others","Deep Learning","Others","Others","Others","Transformer Models","Transformer Models"]],["id",[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51]],["content",["Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works. Attention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez\\u2217\\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n\\u0141ukasz K...","We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014...","applying it successfully to English constituency parsing both with large and limited training data. \\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became...","and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research. \\u2020Work performed while at Google Brain. \\u2021Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS...","1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15]. Recurrent models typically factor computation along the symbol posit...","nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains. Attention mechanisms have become an integral part of com...","model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs. 2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which ...","positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\\n...","a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on sel...","[17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when...","Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively. 3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully conn...","of dimension dmodel = 512. Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subs...","Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel. of the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key. 3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \\"Scaled Dot-Product Attention\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of...","packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n\\u221adk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n\\u221adk . Additive attention computes the compatibility function using a feed-forward network with\\na single hid...","dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n\\u221adk . 3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe f...","get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q \\u00b7 k = Pdk\\ni=1 qiki, has mean 0 and variance dk. 4","output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this. MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n\\u2208Rdmodel\\u00d7dk, W K\\ni\\n\\u2208Rdmodel\\u00d7dk, W V\\n...","the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\u2022 In \\"encoder-decoder attention\\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. ...","in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. \\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \\u2212\\u221e) all values in the input\\nof the softma...","is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048. 3.4\\nEmbeddings and Sof...","model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by \\u221admodel. 5","Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention. Layer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 \\u00b7 d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n \\u00b7 d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k \\u00b7 n \\u00b7 d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r...","decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9]. In this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometr...","learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training. 4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ....","by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn lo...","length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npa...","the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k \\u00b7 n \\u00b7 d + n \\u00b7 d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in o...","to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models. 5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences ...","P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days). 5.3\\nOptimizer\\nWe used the Adam optimizer [20] with \\u03b21 = 0.9, \\u03b22 = 0.98 and \\u03f5 = 10\\u22129. We varied the learning\\nrate over the course of training, according ...","Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost. Model\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 \\u00b7 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 \\u00b7 1019\\n1.4 \\u00b7 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 \\u00b7 1018\\n1.5 \\u00b7 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 \\u00b7 1019\\n1.2 \\u00b7 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 \\u00b7 1020\\nGNMT + ...","encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1. Label Smoothing\\nDuring training, we employed label smoothing of value \\u03f5ls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) b...","surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3. For the base models, we used a singl...","set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5. 6.2\\nModel Variations\\nTo ...","Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities. N\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\n\\u03f5ls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n\\u00d7106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n...","4.33 26.4 213 development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that red...","we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to ...","a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9","Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals &amp; Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang &amp; Harper (2009) [14]\\nsemi-supervis...","300. We used a beam size of 21 and \\u03b1 = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K senten...","multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transfor...","another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor. Acknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration. References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016. [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlea...","[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014. [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016. [7] Junyoung Chung, \\u00c7aglar G\\u00fcl\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeli...","to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017. [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770\\u2013778, 2016. [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\u00fcrgen Schmidhuber. Gradient flow in\\nrecurrent nets: the dif...","Natural Language Processing, pages 832\\u2013841. ACL, August 2009. [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016. [16] \\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016. [17] \\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR),...","Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017. [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017. [23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-tas...","[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\u2013330, 1993. [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152\\u2013159. ACL, June 2006. [27] Ankur Parikh, Oscar T\\u00e4ckstr\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel...","of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433\\u2013440. ACL, July 2006. [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016. [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015. [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey ...","15(1):1929\\u20131958, 2014. [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440\\u20132448. Curran Associates,\\nInc., 2015. [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\\u20133112, 2014. [36] Christian S...","Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\u2019s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016. [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016. [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jing...","Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n&lt;EOS&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n&lt;EOS&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\n&lt;pad&gt;\\nFigure 3: An example of the attention mechanism following long-dista...","Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n&lt;EOS&gt;\\n&lt;pad&gt;\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n&lt;EOS&gt;\\n&lt;pad&gt;\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n&lt;EOS&gt;\\n&lt;pad&gt;\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat...","Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n&lt;EOS&gt;\\n&lt;pad&gt;\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n&lt;EOS&gt;\\n&lt;pad&gt;\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n&lt;EOS&gt;\\n&lt;pad&gt;\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat..."]],["color",{"type":"ndarray","array":["#1f77b4","#999","#999","#999","#1f77b4","#1f77b4","#999","#999","#1f77b4","#1f77b4","#999","#999","#999","#999","#999","#999","#999","#999","#999","#1f77b4","#999","#1f77b4","#999","#1f77b4","#1f77b4","#1f77b4","#aec7e8","#999","#999","#999","#999","#999","#999","#999","#999","#999","#999","#999","#999","#999","#999","#aec7e8","#aec7e8","#999","#aec7e8","#999","#aec7e8","#999","#999","#999","#1f77b4","#1f77b4"],"shape":[52],"dtype":"object","order":"little"}]]}}},"view":{"type":"object","name":"CDSView","id":"p1288","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p1289"}}},"glyph":{"type":"object","name":"Scatter","id":"p1284","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":6},"line_color":{"type":"field","field":"color"},"line_width":{"type":"value","value":2},"fill_color":{"type":"field","field":"color"},"fill_alpha":{"type":"value","value":0.7},"hatch_color":{"type":"field","field":"color"},"hatch_alpha":{"type":"value","value":0.7}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p1285","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":6},"line_color":{"type":"field","field":"color"},"line_alpha":{"type":"value","value":0.1},"line_width":{"type":"value","value":2},"fill_color":{"type":"field","field":"color"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"field","field":"color"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p1286","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":6},"line_color":{"type":"field","field":"color"},"line_alpha":{"type":"value","value":0.2},"line_width":{"type":"value","value":2},"fill_color":{"type":"field","field":"color"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"field","field":"color"},"hatch_alpha":{"type":"value","value":0.2}}}}}],"toolbar":{"type":"object","name":"Toolbar","id":"p1255","attributes":{"logo":"grey","tools":[{"type":"object","name":"HoverTool","id":"p1270","attributes":{"renderers":"auto","tooltips":"\\n    &lt;div style=\\"width:400px;\\"&gt;\\n    &lt;b&gt;Document id:&lt;/b&gt; @id &lt;br&gt;\\n    &lt;b&gt;Topic:&lt;/b&gt; @topic &lt;br&gt;\\n    &lt;b&gt;Document Content:&lt;/b&gt; @content\\n    &lt;/div&gt;\\n    "}},{"type":"object","name":"PanTool","id":"p1271"},{"type":"object","name":"WheelZoomTool","id":"p1272","attributes":{"renderers":"auto"}},{"type":"object","name":"BoxZoomTool","id":"p1273","attributes":{"overlay":{"type":"object","name":"BoxAnnotation","id":"p1274","attributes":{"syncable":false,"level":"overlay","visible":false,"left":{"type":"number","value":"nan"},"right":{"type":"number","value":"nan"},"top":{"type":"number","value":"nan"},"bottom":{"type":"number","value":"nan"},"left_units":"canvas","right_units":"canvas","top_units":"canvas","bottom_units":"canvas","line_color":"black","line_alpha":1.0,"line_width":2,"line_dash":[4,4],"fill_color":"lightgrey","fill_alpha":0.5}}}},{"type":"object","name":"ResetTool","id":"p1279"},{"type":"object","name":"SaveTool","id":"p1280"}]}},"left":[{"type":"object","name":"LinearAxis","id":"p1265","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p1266","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p1267"},"axis_label_standoff":10,"axis_label_text_color":"#E0E0E0","axis_label_text_font":"Helvetica","axis_label_text_font_size":"1.25em","axis_label_text_font_style":"normal","major_label_policy":{"type":"object","name":"AllLabels","id":"p1268"},"major_label_text_color":"#E0E0E0","major_label_text_font":"Helvetica","major_label_text_font_size":"1.025em","axis_line_color":"#E0E0E0","axis_line_alpha":0,"major_tick_line_color":"#E0E0E0","major_tick_line_alpha":0,"minor_tick_line_color":"#E0E0E0","minor_tick_line_alpha":0}}],"below":[{"type":"object","name":"LinearAxis","id":"p1260","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p1261","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p1262"},"axis_label_standoff":10,"axis_label_text_color":"#E0E0E0","axis_label_text_font":"Helvetica","axis_label_text_font_size":"1.25em","axis_label_text_font_style":"normal","major_label_policy":{"type":"object","name":"AllLabels","id":"p1263"},"major_label_text_color":"#E0E0E0","major_label_text_font":"Helvetica","major_label_text_font_size":"1.025em","axis_line_color":"#E0E0E0","axis_line_alpha":0,"major_tick_line_color":"#E0E0E0","major_tick_line_alpha":0,"minor_tick_line_color":"#E0E0E0","minor_tick_line_alpha":0}}],"center":[{"type":"object","name":"Grid","id":"p1264","attributes":{"axis":{"id":"p1260"},"grid_line_color":"white","grid_line_alpha":0.25}},{"type":"object","name":"Grid","id":"p1269","attributes":{"dimension":1,"axis":{"id":"p1265"},"grid_line_color":"white","grid_line_alpha":0.25}},{"type":"object","name":"Legend","id":"p1290","attributes":{"title":"Knowledge Base Tospics","title_text_color":"#B1B1B1","title_text_font_style":"bold","border_line_alpha":0,"background_fill_color":"#111516","background_fill_alpha":0.5,"label_text_color":"#E0E0E0","label_text_font":"Helvetica","label_text_font_size":"1.025em","label_standoff":8,"glyph_width":15,"spacing":8,"items":[{"type":"object","name":"LegendItem","id":"p1291","attributes":{"label":{"type":"value","value":"Deep Learning"},"renderers":[{"id":"p1287"}],"index":26}},{"type":"object","name":"LegendItem","id":"p1292","attributes":{"label":{"type":"value","value":"Others"},"renderers":[{"id":"p1287"}],"index":1}},{"type":"object","name":"LegendItem","id":"p1293","attributes":{"label":{"type":"value","value":"Transformer Models"},"renderers":[{"id":"p1287"}],"index":0}}]}}],"background_fill_color":"#14191B","border_fill_color":"#15191C"}}}},{"type":"object","name":"TabPanel","id":"p1363","attributes":{"title":"Failures","child":{"type":"object","name":"Figure","id":"p1299","attributes":{"sizing_mode":"stretch_width","x_range":{"type":"object","name":"Range1d","id":"p1308","attributes":{"start":13.708381032943725,"end":21.32900791168213}},"y_range":{"type":"object","name":"Range1d","id":"p1309","attributes":{"start":4.127101182937622,"end":11.613864660263062}},"x_scale":{"type":"object","name":"LinearScale","id":"p1310"},"y_scale":{"type":"object","name":"LinearScale","id":"p1311"},"title":{"type":"object","name":"Title","id":"p1306","attributes":{"text_color":"#E0E0E0","text_font":"Helvetica","text_font_size":"14pt"}},"outline_line_color":"#E0E0E0","outline_line_alpha":0.25,"renderers":[{"type":"object","name":"GlyphRenderer","id":"p1338","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p1295","attributes":{"selected":{"type":"object","name":"Selection","id":"p1296","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p1297"},"data":{"type":"map","entries":[["x",{"type":"ndarray","array":{"type":"bytes","data":"HauMQVoXd0HibJJBHICPQR2rjEFAyYZB1cyEQS1ackHlqpFBrX5tQeg9YkEAh5NB+5F9QVoXd0Gwm4pBZdGEQUDJhkHvrnZBRo2AQWIXaUE="},"shape":[20],"dtype":"float32","order":"little"}],["y",{"type":"ndarray","array":{"type":"bytes","data":"bz4SQQg+5UB2lh9B1OkGQW8+EkHvvMdAphUhQcaqDUHEWxFBblEWQQqr1kCagvNAbdslQQg+5UBVDLxAmgn/QO+8x0B6GvdARQz4QCzTwUA="},"shape":[20],"dtype":"float32","order":"little"}],["topic",["Deep Learning","Self-Attention Mechanism","Others","Deep Learning","Deep Learning","Others","Others","Others","Deep Learning","Transformer Model","Others","Others","Others","Transformer Models","Others","Transformer Models","Others","Transformer Models","Transformer Models","Others"]],["correctness",[true,true,false,false,false,true,false,true,false,false,true,true,false,true,true,false,true,false,true,false]],["questions",["Who are the authors of the paper titled &#x27;Learning phrase representations using rnn encoder-decoder for statistical machine translation&#x27;?","What type of operations are required for the Positional Encoding?","Can you provide a brief summary of the context in which the papers listed were published, specifically highlighting any relevant conferences or journal names?","What year was the paper &#x27;Long short-term memory&#x27; first published, and was it initially presented at a conference or published in a journal?","What type of neural networks are mentioned in the context, specifically in relation to the attention mechanism described as &#x27;Scaled Dot-Product Attention&#x27;?","Under what conditions do the dot products in the attention mechanism become excessively large, leading to vanishing gradients in the softmax function?","As a researcher in France interested in evaluating the Transformer&#x27;s ability to generalize to diverse tasks, what task did the researchers perform to assess its performance on constituency parsing?","As a researcher evaluating the efficiency of GPU training for natural language processing models, I&#x27;m trying to understand how to adjust the learning rate during training. What&#x27;s the formula for varying the learning rate during training?","What are some recent research papers related to neural machine translation and what is the concept of the sparsely-gated mixture-of-experts layer?","What is the formula used?","What is the output dimension of each sub-layer in the Transformer model?","Who are the authors of the End-to-end memory networks paper?","What is the maximum output length used in the Transformer model when the input length is increased by 300, as in the experiment reported in Table 4?","What is the computational complexity of the Self-Attention layer, assuming a sequence length of n, a representation dimension of d, and a kernel size of k?","What is the variance of the dot product of two independent random variables q and k, given that q and k are vectors of dimension dk and dv, respectively?","How does the use of attention mechanisms in the proposed Transformer model differ from dot-product attention, especially for large values of sequence length?","As I&#x27;m trying to optimize the performance of my deep neural network with larger input sizes, I&#x27;ve noticed that the dot products in the attention mechanism are getting extremely large for bigger values of d_k. Can you help me understand why this is happening?","As I&#x27;m trying to implement a transformer-based model for natural language processing, I&#x27;m having trouble understanding how the input and output dimensions of the linear transformations in the Feed Forward Neural Network (FFN) work, considering I&#x27;m new to this architecture.","What is the structure of the most competitive neural sequence transduction models, and are they auto-regressive?","How many?"]],["agent_answer",["## Answer\\n### Authors of the Paper\\n\\nThe authors of the paper titled \\"Learning phrase representations using rnn encoder-decoder for statistical machine translation\\" are:\\n\\n* Kyunghyun Cho\\n* Bart van Merrienboer\\n* Caglar Gulcehre\\n* Fethi Bougares\\n* Holger Schwenk\\n* Yoshua Bengio","## Operation for Positional Encoding\\n### Required Operations\\n\\nPositional Encoding requires the following operations:\\n\\n*   **Repeat and Concatenate**: The input sequence is repeated and then concatenated with itself, element-wise.\\n*   **Additive**: The concatenated sequence is then added to the original input sequence.\\n\\nThis process creates a new embedding matrix that includes the original input values and their positional information.\\n\\n### Equation\\n\\nLet&#x27;s represent the input sequence as `x = [x1, x2, ..., xn]`. The positional encoding matrix `PE` is calculated as:\\n\\n`PE = [PE1, PE2, ..., PE2^n]`\\n\\nwhere `PEi = [sin(i * x_i / 10000), cos(i * x_i / 10000)]`\\n\\nThe final embedding matrix `E` is obtained by adding the positional encoding matrix to the original input sequence:\\n\\n`E = [x1 + PE1, x2 + PE2, ..., xn + PE2^n]`\\n\\nThis process effectively adds the positional information to the input values, allowing the model to understand the context and location of each input element.","## Response\\n\\n### Summary of User&#x27;s Query\\n\\nThe user is seeking information about Newton&#x27;s second law of motion, which relates the force applied to an object to its resulting acceleration.\\n\\n### Answer\\n\\n#### Newton&#x27;s Second Law of Motion\\n\\nNewton&#x27;s second law of motion states that:\\n\\n$$ F = ma $$\\n\\nwhere:\\n\\n- **F** is the force applied to an object.\\n- **m** is the mass of the object.\\n- **a** is the acceleration of the object.\\n\\nThis law explains how the velocity of an object changes when it is subjected to an external force.\\n\\n### Additional Information\\n\\nNewton&#x27;s second law is a fundamental principle in physics that has numerous applications in various fields, including mechanics, engineering, and astronomy. It is widely used to describe the relationship between a body and the forces acting upon it.","## Answer\\nThe paper \\"Long short-term memory\\" by Sepp Hochreiter and J\\u00fcrgen Schmidhuber was first published in 1997. Unfortunately, the exact year it was initially presented at a conference is not specified in the given context.","## Answer\\n\\nThe neural network that mentions Scaled Dot-Product Attention is the Transformer model. The Transformer model is based solely on attention mechanisms and does not use traditional recurrent or convolutional neural networks. It uses self-attention layers, which implement Scaled Dot-Product Attention, to perform sequence transduction tasks.","## Answer\\nExcessively large dot products in the attention mechanism can be attributed to the dimensionality of the input (`dk`). When the dimensionality is large, the dot products can become excessively large, pushing the softmax function into regions with extremely small gradients, which can lead to vanishing gradients.\\n\\n## Mitigating Factor\\nScaling the dot products by 1/\\u221adk can help mitigate this issue. This scaling factor reduces the magnitude of the dot products, preventing the softmax function from being pushed into regions with extremely small gradients.\\n\\n## Mathematical Representation\\n$$\\n\\\\text{Scaled Dot Product} = \\\\frac{\\\\mathbf{q} \\\\cdot \\\\mathbf{k}^T}{\\\\sqrt{d_k} \\\\cdot \\\\sqrt{d_k}}\\n$$\\n\\nThis scaled dot product helps stabilize the softmax function, preventing vanishing gradients and ensuring that the attention mechanism can effectively compute weighted sums of the input elements.","## Step 1: Understand the context\\nThe researchers evaluated the Transformer&#x27;s ability to generalize to English constituency parsing by performing two tasks.\\n\\n## Step 2: Identify the tasks\\nThe tasks performed by the researchers were training a 4-layer Transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank, and also in a semi-supervised setting using the larger high-confidence and BerkleyParser corpora.\\n\\n## Step 3: Determine the assessment task\\nThe task performed to assess the Transformer&#x27;s performance on constituency parsing was training and semi-supervised training using specific corpora.\\n\\nThe final answer is: Training and semi-supervised training using specific corpora.","## Step 1: Understanding the Formula\\nThe formula for varying the learning rate during GPU training is `lrate = d\\u22120.5 * min(step_num\\u22120.5, step_num * warmup_steps\\u22121.5)`, where `d` is the model, `step_num` is the current step number, and `warmup_steps` is a hyperparameter set to 4000.\\n\\n## Step 2: Breaking Down the Formula\\n- The formula starts by subtracting 0.5 from the model `d`, then multiplies the result by the minimum of two expressions.\\n- The first expression, `step_num\\u22120.5`, represents the linear decrease in learning rate during the actual training process.\\n- The second expression, `step_num * warmup_steps\\u22121.5`, represents the linear increase in learning rate during the warm-up period.\\n- The `warmup_steps` parameter controls the duration of the warm-up period, which is set to 4000 in this case.\\n\\n## Step 3: Implications for GPU Training\\nThis formula allows the learning rate to vary during GPU training, which can improve the convergence speed and accuracy of natural language processing models. The formula also helps to avoid overfitting by gradually reducing the learning rate during the training process.\\n\\n## Step 4: Conclusion\\nThe formula `lrate = d\\u22120.5 * min(step_num\\u22120.5, step_num * warmup_steps\\u22121.5)` provides a flexible way to vary the learning rate during GPU training for natural language processing models, taking into account the model, step number, and warm-up steps.","## Answer\\nRecent research papers on neural machine translation and the sparsely-gated mixture-of-experts (MoE) layer include:\\n\\n- A review paper by Jozef Gajdo\\u0161, et al. titled \\"Mixture of Experts\\" [1], which discusses the theoretical foundations and applications of the MoE layer.\\n- A paper by Minlin Zhu, et al. titled \\"A Sparsely-Gated Mixture of Experts for Neural Machine Translation\\" [2], which proposes a novel architecture that leverages the MoE layer for neural machine translation tasks.\\n\\n## References\\n[1] Jozef Gajdo\\u0161, et al. Mixture of Experts. arXiv preprint arXiv:2004.10909, 2020.\\n[2] Minlin Zhu, et al. A Sparsely-Gated Mixture of Experts for Neural Machine Translation. arXiv preprint arXiv:2103.03867, 2021.","## Response\\nThe formula used is the learning rate formula for a model, specifically:\\n\\n`lrate = d\\u22120.5 * min(step_num\\u22120.5, step_num * warmup_steps\\u22121.5)`\\n\\nwhere:\\n- `d` is the model size\\n- `step_num` is the current training step number\\n- `warmup_steps` is the number of initial training steps with a linearly increasing learning rate.","## Step 1: Understanding the Transformer Model\\nThe Transformer model is a type of neural network that uses self-attention mechanisms to process sequential data. Each sub-layer in the Transformer model consists of two main components: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\\n\\n## Step 2: Identifying the Output Dimension\\nGiven that the output dimension of each sub-layer in the Transformer model is dmodel = 512, we can determine the output dimension of each sub-layer.\\n\\n## Step 3: Calculating the Output Dimension\\nSince the Transformer model has a stack of N = 6 identical layers, each with two sub-layers, we need to calculate the output dimension of each sub-layer.\\n\\nThe final answer is $\\\\boxed{512}$","**Authors of the End-to-end memory networks paper**\\n==============================================\\n\\nThe authors of the End-to-end memory networks paper are:\\n\\n* Sainbayar Sukhbaatar\\n* Arthur Szlam\\n* Jason Weston\\n* Rob Fergus","## Answer\\nThe maximum output length of the Transformer model when the input length is increased by 300 is `input length + 300`.","## Answer\\n### Computational Complexity of Self-Attention Layer\\n\\nThe computational complexity of the Self-Attention layer is O(n2 \\u00b7 d), where n is the sequence length and d is the representation dimension.\\n\\n### Explanation\\n\\nThe complexity is not affected by the kernel size k. The complexity is O(n2 \\u00b7 d) regardless of the value of k. This formula is calculated using the sequence length n and the representation dimension d, and then multiplied by the number of operations.","## Answer\\n### Variance of the Dot Product of Two Independent Random Vectors\\n\\nThe variance of the dot product of two independent random vectors q and k is given by:\\n\\n$$\\\\text{Var}(q \\\\cdot k) = dk$$\\n\\nwhere dk is the dimension of vector k.\\n\\n### Explanation\\n\\nSince q and k are independent random vectors, the dot product q \\u00b7 k is also a random variable. The variance of this dot product can be calculated using the formula for the variance of a linear combination of independent random variables. In this case, the dimension of the dot product is equal to the dimension of vector k, hence dk.","## Step 1: Understanding the Transformer Model\\nThe Transformer model is a type of neural network architecture designed for natural language processing tasks. It primarily uses self-attention mechanisms to process sequences of data.\\n\\n## Step 2: Dot-Product Attention vs. Transformer Attention\\nDot-product attention requires O(n) sequential operations to compute attention scores, where n is the length of the input sequence. This makes it inefficient for large sequence lengths.\\n\\n## Step 3: Transformer&#x27;s Multi-Head Attention Mechanism\\nThe Transformer model uses multi-head attention in three different ways: encoder-decoder attention, self-attention, and intra-attention. This allows it to connect all positions with a constant number of sequentially executed operations, reducing the maximum path length between any two input and output positions in the network.\\n\\n## Step 4: Impact on Long-Range Dependencies\\nBy using self-attention, the Transformer model can learn long-range dependencies more efficiently than dot-product attention. This is because self-attention connects all positions with a constant number of sequentially executed operations, making it easier to learn relationships between distant elements in the sequence.\\n\\nThe final answer is: The Transformer model differs from dot-product attention for large sequence lengths due to its efficient use of self-attention mechanisms, which reduce the maximum path length between any two input and output positions in the network, enabling the model to learn long-range dependencies more efficiently.","## Step 1: Understanding the Attention Mechanism\\nThe attention mechanism in deep learning models is a way to focus on specific parts of the input data that are most relevant for making predictions.\\n\\n## Step 2: Analyzing the Cause of Increasing Dot Products\\nThe dot product in the attention mechanism is calculated as the dot product of the query and key vectors. As the input size increases, the dimensionality of the query and key vectors also increases.\\n\\n## Step 3: Scaling the Dot Products\\nTo counteract the effect of large dot products, the authors of the original paper scaled the dot products by 1/\\u221adk, where d_k is the dimensionality of the key vector.\\n\\n## Step 4: Conclusion\\nThe dot products in the attention mechanism increase with larger input sizes due to the increased dimensionality of the query and key vectors. Scaling the dot products by 1/\\u221adk helps to mitigate this effect and maintain stable gradients in the softmax function.","## Answer\\nThe input and output dimensions of the linear transformations in the FFN of the Transformer model are both 512, which is the same as the dimensionality of the input and output embeddings. This is because the FFN is applied to each position separately and identically, and the linear transformations use different parameters from layer to layer. The inner-layer has a dimensionality of 2048.","## Answer\\n### Structure of Neural Sequence Transduction Models\\nThe most competitive neural sequence transduction models employ an encoder-decoder structure.\\n\\n### Auto-regressiveness\\nThese models are auto-regressive, which means they consume the previously generated symbols as additional input when generating the next symbol.\\n\\n### Equation\\n$$\\n\\\\hat{y} = f(\\\\mathbf{x}, \\\\mathbf{y}_{&lt;y}) \\n$$\\nwhere:\\n- $\\\\hat{y}$ is the predicted output.\\n- $\\\\mathbf{x}$ is the input sequence.\\n- $\\\\mathbf{y}_{&lt;y}$ is the previously generated symbols.","## Response\\n\\n## User&#x27;s Query:\\n\\"How many are there?\\"\\n\\n## Answer\\nUnfortunately, the given context does not provide sufficient information to determine the exact answer to the user&#x27;s question. Can you provide more context or clarify what you are referring to?"]],["reference_answer",["Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.","The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.","The papers listed are related to Natural Language Processing (NLP) and Computational Linguistics.","1997","Recurrent neural networks (RNNs)","The dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients","English constituency parsing","lrate = d\\u22120.5 \\u00b7 model \\u00b7 min(step_num\\u22120.5, step_num \\u00b7 warmup_steps\\u22121.5)","Recent research papers related to neural machine translation include &#x27;Using the output embedding to improve language models&#x27; by Ofir Press and Lior Wolf (2016), and papers related to neural machine translation include &#x27;Outrageously large neural networks: The sparsely-gated mixture-of-experts layer&#x27; by Noam Shazeer et al. (2017).","We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU","512","Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus","increased the maximum output length to input length + 300","O(n2 \\u00b7 d)","dk","The proposed Transformer model is based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.","The dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.","dmodel = 512, and the inner-layer has dimensionality dff = 2048","Most competitive neural sequence transduction models have an encoder-decoder structure and are auto-regressive.","The decoder stack consists of 6 identical layers."]],["id",[41,21,45,42,41,14,35,28,46,32,10,47,37,21,15,0,14,19,9,11]],["content",["[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014. [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016. [7] Junyoung Chung, \\u00c7aglar G\\u00fcl\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeli...","Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention. Layer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 \\u00b7 d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n \\u00b7 d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k \\u00b7 n \\u00b7 d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r...","[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\u2013330, 1993. [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152\\u2013159. ACL, June 2006. [27] Ankur Parikh, Oscar T\\u00e4ckstr\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel...","to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017. [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770\\u2013778, 2016. [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\u00fcrgen Schmidhuber. Gradient flow in\\nrecurrent nets: the dif...","[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014. [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016. [7] Junyoung Chung, \\u00c7aglar G\\u00fcl\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeli...","dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n\\u221adk . 3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe f...","we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to ...","P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days). 5.3\\nOptimizer\\nWe used the Adam optimizer [20] with \\u03b21 = 0.9, \\u03b22 = 0.98 and \\u03f5 = 10\\u22129. We varied the learning\\nrate over the course of training, according ...","of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433\\u2013440. ACL, July 2006. [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016. [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015. [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey ...","set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5. 6.2\\nModel Variations\\nTo ...","Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively. 3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully conn...","15(1):1929\\u20131958, 2014. [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440\\u20132448. Curran Associates,\\nInc., 2015. [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\\u20133112, 2014. [36] Christian S...","Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals &amp; Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang &amp; Harper (2009) [14]\\nsemi-supervis...","Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention. Layer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 \\u00b7 d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n \\u00b7 d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k \\u00b7 n \\u00b7 d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r...","get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q \\u00b7 k = Pdk\\ni=1 qiki, has mean 0 and variance dk. 4","Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works. Attention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez\\u2217\\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n\\u0141ukasz K...","dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n\\u221adk . 3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe f...","is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048. 3.4\\nEmbeddings and Sof...","[17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when...","of dimension dmodel = 512. Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subs..."]],["color",["#0a980a","#0a980a","#ba0e0e","#ba0e0e","#ba0e0e","#0a980a","#ba0e0e","#0a980a","#ba0e0e","#ba0e0e","#0a980a","#0a980a","#ba0e0e","#0a980a","#0a980a","#ba0e0e","#0a980a","#ba0e0e","#0a980a","#ba0e0e"]]]}}},"view":{"type":"object","name":"CDSView","id":"p1339","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p1340"}}},"glyph":{"type":"object","name":"Scatter","id":"p1335","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":6},"line_color":{"type":"field","field":"color"},"line_width":{"type":"value","value":2},"fill_color":{"type":"field","field":"color"},"fill_alpha":{"type":"value","value":0.7},"hatch_color":{"type":"field","field":"color"},"hatch_alpha":{"type":"value","value":0.7}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p1336","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":6},"line_color":{"type":"field","field":"color"},"line_alpha":{"type":"value","value":0.1},"line_width":{"type":"value","value":2},"fill_color":{"type":"field","field":"color"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"field","field":"color"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p1337","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":6},"line_color":{"type":"field","field":"color"},"line_alpha":{"type":"value","value":0.2},"line_width":{"type":"value","value":2},"fill_color":{"type":"field","field":"color"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"field","field":"color"},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p1353","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p1344","attributes":{"selected":{"type":"object","name":"Selection","id":"p1345","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p1346"},"data":{"type":"map","entries":[["x",{"type":"ndarray","array":{"type":"bytes","data":"ZdGEQbjXfUHfuYRBx2qUQTl2gEG+NYtBeqSMQQ3VhEGnCYRBRo2AQeg9YkFiF2lBcViEQZjLgEFAyYZBsJuKQdSye0Gpf3NBwQhtQe+udkET0H1BWhd3QbyvckFCAIRBw0d/QW8piEEwXo5BVvyHQS1ackEPxGBBR7lkQaxwcUGtfm1BaQdfQbo3Y0HVzIRBgwZyQfuRfUHf44VBYU9/Qf3NiUEdq4xBHICPQYV2lEFmY4tB4mySQeWqkUEAh5NBLTiOQfhjaEFwSXBBTsZpQQ=="},"shape":[52],"dtype":"float32","order":"little"}],["y",{"type":"ndarray","array":{"type":"bytes","data":"mgn/QFvDF0EzRhtBK1wEQXXbBUFtvOxA9PXeQA++3kAa5ApBRQz4QAqr1kAs08FAaC2zQByMv0DvvMdAVQy8QCT/q0DmSsFAudLaQHoa90CJyMxACD7lQMMe0UCxCutAw+DkQPC5+UC/wvxAyf4kQcaqDUFntApB4WQSQaHKI0FuURZBXxIaQZppIUGmFSFBzVMaQW3bJUE6zxBBQXITQXb6GUFvPhJB1OkGQUHeFEGrOwtBdpYfQcRbEUGagvNAhIwgQcP88ECWdwNBrlgAQQ=="},"shape":[52],"dtype":"float32","order":"little"}]]}}},"view":{"type":"object","name":"CDSView","id":"p1354","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p1355"}}},"glyph":{"type":"object","name":"Scatter","id":"p1350","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":6},"line_color":{"type":"value","value":"grey"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.2}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p1351","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":6},"line_color":{"type":"value","value":"grey"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p1352","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":6},"line_color":{"type":"value","value":"grey"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.2}}}}}],"toolbar":{"type":"object","name":"Toolbar","id":"p1307","attributes":{"logo":"grey","tools":[{"type":"object","name":"PanTool","id":"p1322"},{"type":"object","name":"WheelZoomTool","id":"p1323","attributes":{"renderers":"auto"}},{"type":"object","name":"BoxZoomTool","id":"p1324","attributes":{"overlay":{"type":"object","name":"BoxAnnotation","id":"p1325","attributes":{"syncable":false,"level":"overlay","visible":false,"left":{"type":"number","value":"nan"},"right":{"type":"number","value":"nan"},"top":{"type":"number","value":"nan"},"bottom":{"type":"number","value":"nan"},"left_units":"canvas","right_units":"canvas","top_units":"canvas","bottom_units":"canvas","line_color":"black","line_alpha":1.0,"line_width":2,"line_dash":[4,4],"fill_color":"lightgrey","fill_alpha":0.5}}}},{"type":"object","name":"ResetTool","id":"p1330"},{"type":"object","name":"SaveTool","id":"p1331"},{"type":"object","name":"HoverTool","id":"p1298","attributes":{"renderers":[{"id":"p1338"}],"tooltips":"\\n    &lt;div style=\\"width:400px;\\"&gt;\\n    &lt;b&gt;Document id:&lt;/b&gt; @id &lt;br&gt;\\n    &lt;b&gt;Topic:&lt;/b&gt; @topic &lt;br&gt;\\n    &lt;b&gt;Question:&lt;/b&gt; @questions &lt;br&gt;\\n    &lt;b&gt;agent Answer:&lt;/b&gt; @agent_answer &lt;br&gt;\\n    &lt;b&gt;Reference Answer:&lt;/b&gt; @reference_answer &lt;br&gt;\\n    &lt;b&gt;Correctness:&lt;/b&gt; @correctness &lt;br&gt;\\n    &lt;b&gt;Content:&lt;/b&gt; @content\\n    &lt;/div&gt;\\n    "}}]}},"left":[{"type":"object","name":"LinearAxis","id":"p1317","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p1318","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p1319"},"axis_label_standoff":10,"axis_label_text_color":"#E0E0E0","axis_label_text_font":"Helvetica","axis_label_text_font_size":"1.25em","axis_label_text_font_style":"normal","major_label_policy":{"type":"object","name":"AllLabels","id":"p1320"},"major_label_text_color":"#E0E0E0","major_label_text_font":"Helvetica","major_label_text_font_size":"1.025em","axis_line_color":"#E0E0E0","axis_line_alpha":0,"major_tick_line_color":"#E0E0E0","major_tick_line_alpha":0,"minor_tick_line_color":"#E0E0E0","minor_tick_line_alpha":0}}],"below":[{"type":"object","name":"LinearAxis","id":"p1312","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p1313","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p1314"},"axis_label_standoff":10,"axis_label_text_color":"#E0E0E0","axis_label_text_font":"Helvetica","axis_label_text_font_size":"1.25em","axis_label_text_font_style":"normal","major_label_policy":{"type":"object","name":"AllLabels","id":"p1315"},"major_label_text_color":"#E0E0E0","major_label_text_font":"Helvetica","major_label_text_font_size":"1.025em","axis_line_color":"#E0E0E0","axis_line_alpha":0,"major_tick_line_color":"#E0E0E0","major_tick_line_alpha":0,"minor_tick_line_color":"#E0E0E0","minor_tick_line_alpha":0}}],"center":[{"type":"object","name":"Grid","id":"p1316","attributes":{"axis":{"id":"p1312"},"grid_line_color":"white","grid_line_alpha":0.25}},{"type":"object","name":"Grid","id":"p1321","attributes":{"dimension":1,"axis":{"id":"p1317"},"grid_line_color":"white","grid_line_alpha":0.25}},{"type":"object","name":"Legend","id":"p1341","attributes":{"title":"Question Correctness","title_text_color":"#B1B1B1","title_text_font_style":"bold","border_line_alpha":0,"background_fill_color":"#111516","background_fill_alpha":0.5,"label_text_color":"#E0E0E0","label_text_font":"Helvetica","label_text_font_size":"1.025em","label_standoff":8,"glyph_width":15,"spacing":8,"items":[{"type":"object","name":"LegendItem","id":"p1342","attributes":{"label":{"type":"value","value":"False"},"renderers":[{"id":"p1338"}],"index":2}},{"type":"object","name":"LegendItem","id":"p1343","attributes":{"label":{"type":"value","value":"True"},"renderers":[{"id":"p1338"}],"index":0}}]}},{"type":"object","name":"LabelSet","id":"p1359","attributes":{"level":"glyph","source":{"type":"object","name":"ColumnDataSource","id":"p1356","attributes":{"selected":{"type":"object","name":"Selection","id":"p1357","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p1358"},"data":{"type":"map","entries":[["x",{"type":"ndarray","array":{"type":"bytes","data":"aGaAQYpRjkE="},"shape":[2],"dtype":"float32","order":"little"}],["y",{"type":"ndarray","array":{"type":"bytes","data":"/ZP5QANtCkE="},"shape":[2],"dtype":"float32","order":"little"}],["topic",["Transformer Models","Deep Learning"]]]}}},"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"text":{"type":"field","field":"topic"},"text_color":{"type":"value","value":"#B1B1B1"},"text_font_size":{"type":"value","value":"12pt"},"text_font_style":{"type":"value","value":"bold"},"text_align":{"type":"value","value":"center"}}}],"background_fill_color":"#14191B","border_fill_color":"#15191C"}}}}],"tabs_location":"below"}}]}}';
        const render_items = [{"docid":"680e4f44-12e9-437d-aad7-72e45bba4227","roots":{"p1364":"b8fd9d40-407a-456e-aa38-8b70df0cb1fc"},"root_ids":["p1364"]}];
        root.Bokeh.embed.embed_items(docs_json, render_items);
        }
        if (root.Bokeh !== undefined) {
          embed_document(root);
        } else {
          let attempts = 0;
          const timer = setInterval(function(root) {
            if (root.Bokeh !== undefined) {
              clearInterval(timer);
              embed_document(root);
            } else {
              attempts++;
              if (attempts > 100) {
                clearInterval(timer);
                console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing");
              }
            }
          }, 10, root)
        }
      })(window);
    });
  };
  if (document.readyState != "loading") fn();
  else document.addEventListener("DOMContentLoaded", fn);
})();
    </script>

                <div id="b8fd9d40-407a-456e-aa38-8b70df0cb1fc" data-root-id="p1364" style="display: contents;"></div>
            </div>
        </div>

        <div class="section-container">
            <div class="section-card">

                <div class="section-title">SELECTED METRICS</div>

                

            </div>
        </div>

        
    </div>
</div>



<script type="text/javascript">
    function opentab(evt, name) {
    // Declare all variables
    let i, tabcontent, tablinks;

    // Get all elements with class="tabcontent" and hide them
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }

    // Get all elements with class="tablinks" and remove the class "active"
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }

    // Show the current tab, and add an "active" class to the button that opened the tab
    document.getElementById(name).style.display = "block";
    evt.currentTarget.className += " active";
}
</script>
